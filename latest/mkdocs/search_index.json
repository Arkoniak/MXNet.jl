{
    "docs": [
        {
            "location": "/", 
            "text": "MXNet Documentation\n\n\nMXNet.jl\n is the \nJulia\n package of \ndmlc/mxnet\n. \nMXNet.jl\n brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:\n\n\n\n\nEfficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.\n\n\nFlexible symbolic manipulation to composite and construct state-of-the-art deep learning models.\n\n\n\n\nFor more details, see documentation below. Please also checkout the \nexamples\n directory.\n\n\n\n\nTutorials\n\n\n\n\nDigit Recognition on MNIST\n\n\nSimple 3-layer MLP\n\n\nConvolutional Neural Networks\n\n\nPredicting with a trained model\n\n\n\n\n\n\nGenerating Random Sentence with LSTM RNN\n\n\nLSTM Cells\n\n\nUnfolding LSTM\n\n\nData Provider for Text Sequences\n\n\nTraining the LSTM\n\n\nSampling Random Sentences\n\n\nVisualizing the LSTM\n\n\n\n\n\n\n\n\n\n\nUser's Guide\n\n\n\n\nInstallation Guide\n\n\nAutomatic Installation\n\n\nManual Compilation\n\n\n\n\n\n\nOverview\n\n\nMXNet.jl Namespace\n\n\nLow Level Interface\n\n\nIntermediate Level Interface\n\n\nHigh Level Interface\n\n\n\n\n\n\nFAQ\n\n\nRunning MXNet on AWS GPU instances\n\n\n\n\n\n\n\n\n\n\nAPI Documentation\n\n\n\n\nContext\n\n\nModel\n\n\nEvaluation Metrics\n\n\nData Providers\n\n\nNDArray API\n\n\nSymbolic API\n\n\nNeural Network Factora\n\n\nExecutor\n\n\nNetwork Visualization", 
            "title": "Home"
        }, 
        {
            "location": "/#mxnet-documentation", 
            "text": "MXNet.jl  is the  Julia  package of  dmlc/mxnet .  MXNet.jl  brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:   Efficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.  Flexible symbolic manipulation to composite and construct state-of-the-art deep learning models.   For more details, see documentation below. Please also checkout the  examples  directory.", 
            "title": "MXNet Documentation"
        }, 
        {
            "location": "/#tutorials", 
            "text": "Digit Recognition on MNIST  Simple 3-layer MLP  Convolutional Neural Networks  Predicting with a trained model    Generating Random Sentence with LSTM RNN  LSTM Cells  Unfolding LSTM  Data Provider for Text Sequences  Training the LSTM  Sampling Random Sentences  Visualizing the LSTM", 
            "title": "Tutorials"
        }, 
        {
            "location": "/#users-guide", 
            "text": "Installation Guide  Automatic Installation  Manual Compilation    Overview  MXNet.jl Namespace  Low Level Interface  Intermediate Level Interface  High Level Interface    FAQ  Running MXNet on AWS GPU instances", 
            "title": "User's Guide"
        }, 
        {
            "location": "/#api-documentation", 
            "text": "Context  Model  Evaluation Metrics  Data Providers  NDArray API  Symbolic API  Neural Network Factora  Executor  Network Visualization", 
            "title": "API Documentation"
        }, 
        {
            "location": "/tutorial/mnist/", 
            "text": "Digit Recognition on MNIST\n\n\nIn this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the \nMNIST handwritten digit dataset\n. The code for this tutorial could be found in \nexamples/mnist\n.\n\n\n\n\nSimple 3-layer MLP\n\n\nThis is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with\n\n\nusing MXNet\n\n\n\n\nto load the \nMXNet\n module. Then we are ready to define the network architecture via the symbolic API \n/user-guide/overview\n. We start with a placeholder \ndata\n symbol,\n\n\ndata = mx.Variable(:data)\n\n\n\n\nand then cascading fully-connected layers and activation functions:\n\n\n```{.sourceCode .julia}\nfc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)\n\n\n\n\nNote each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like\n\n\n\n\n\n\nInput --\n 128 units (ReLU) --\n 64 units (ReLU) --\n 10 units\n\n\n\n\nwhere the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final `SoftmaxOutput` operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:\n\n\n```julia\nmlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)\n\n\n\n\nAs we can see, the MLP is just a chain of layers. For this case, we can also use the \nmx.chain\n macro. The same architecture above can be defined as\n\n\nmlp = @mx.chain mx.Variable(:data)             =\n\n  mx.FullyConnected(name=:fc1, num_hidden=128) =\n\n  mx.Activation(name=:relu1, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc2, num_hidden=64)  =\n\n  mx.Activation(name=:relu2, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc3, num_hidden=10)  =\n\n  mx.SoftmaxOutput(name=:softmax)\n\n\n\n\nAfter defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into \nPkg.dir(\"MXNet\")/data/mnist\n if necessary. We wrap the code to construct the data provider into \nmnist-data.jl\n so that it could be shared by both the MLP example and the LeNet ConvNets example.\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size)\n\n\n\n\nIf you need to write your own data providers for customized data format, please refer to \nmx.AbstractDataProvider\n.\n\n\nGiven the architecture and data, we can instantiate an \nmodel\n to do the actual training. \nmx.FeedForward\n is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the \ncontext\n on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.\n\n\nmodel = mx.FeedForward(mlp, context=mx.cpu())\n\n\n\n\nYou can use a \nmx.gpu()\n or if a list of devices (e.g. \n[mx.gpu(0), mx.gpu(1)]\n) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.\n\n\nThe last thing we need to specify is the optimization algorithm (a.k.a. \noptimizer\n) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:\n\n\noptimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)\n\n\n\n\nNow we can do the training. Here the \nn_epoch\n parameter specifies that we want to train for 20 epochs. We also supply a \neval_data\n to monitor validation accuracy on the validation set.\n\n\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nHere is a sample output\n\n\nINFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\nIn the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:\n\n\n# input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n\n\n\nWe basically defined two convolution modules. Each convolution module is actually a chain of \nConvolution\n, \ntanh\n activation and then max \nPooling\n operations.\n\n\nEach sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by \nNDArray\n, a batch of 100 samples is a tensor of shape \n(28,28,1,100)\n. The convolution and pooling operates in the spatial axis, so \nkernel=(5,5)\n indicate a square region of 5-width and 5-height. The rest of the architecture follows as:\n\n\n# first fully-connected\nfc1   = @mx.chain mx.Flatten(data=conv2) =\n\n                  mx.FullyConnected(num_hidden=500) =\n\n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(data=fc2, name=:softmax)\n\n\n\n\nNote a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a \nFlatten\n operator to flat the tensor, before connecting it to the \nFullyConnected\n operator.\n\n\nThe rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)\n\n\n\n\nNote we specified \nflat=false\n to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.\n\n\n# fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nAnd here is a sample of running outputs:\n\n\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915\n\n\n\n\n\n\nPredicting with a trained model\n\n\nPredicting with a trained model is very simple. By calling \nmx.predict\n with the model and a data provider, we get the model output as a Julia Array:\n\n\nprobs = mx.predict(model, eval_provider)\n\n\n\n\nThe following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:\n\n\n# collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format(\nAccuracy on eval set: {1:.2f}%\n, 100correct/length(labels)))\n\n\n\n\nAlternatively, when the dataset is huge, one can provide a callback to \nmx.predict\n, then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from \nmx.predict\n. See also predict.", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#digit-recognition-on-mnist", 
            "text": "In this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the  MNIST handwritten digit dataset . The code for this tutorial could be found in  examples/mnist .", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#simple-3-layer-mlp", 
            "text": "This is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with  using MXNet  to load the  MXNet  module. Then we are ready to define the network architecture via the symbolic API  /user-guide/overview . We start with a placeholder  data  symbol,  data = mx.Variable(:data)  and then cascading fully-connected layers and activation functions:  ```{.sourceCode .julia}\nfc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)  \n\nNote each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like  Input --  128 units (ReLU) --  64 units (ReLU) --  10 units  \n\nwhere the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final `SoftmaxOutput` operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:\n\n\n```julia\nmlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)  As we can see, the MLP is just a chain of layers. For this case, we can also use the  mx.chain  macro. The same architecture above can be defined as  mlp = @mx.chain mx.Variable(:data)             = \n  mx.FullyConnected(name=:fc1, num_hidden=128) = \n  mx.Activation(name=:relu1, act_type=:relu)   = \n  mx.FullyConnected(name=:fc2, num_hidden=64)  = \n  mx.Activation(name=:relu2, act_type=:relu)   = \n  mx.FullyConnected(name=:fc3, num_hidden=10)  = \n  mx.SoftmaxOutput(name=:softmax)  After defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into  Pkg.dir(\"MXNet\")/data/mnist  if necessary. We wrap the code to construct the data provider into  mnist-data.jl  so that it could be shared by both the MLP example and the LeNet ConvNets example.  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size)  If you need to write your own data providers for customized data format, please refer to  mx.AbstractDataProvider .  Given the architecture and data, we can instantiate an  model  to do the actual training.  mx.FeedForward  is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the  context  on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.  model = mx.FeedForward(mlp, context=mx.cpu())  You can use a  mx.gpu()  or if a list of devices (e.g.  [mx.gpu(0), mx.gpu(1)] ) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.  The last thing we need to specify is the optimization algorithm (a.k.a.  optimizer ) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:  optimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)  Now we can do the training. Here the  n_epoch  parameter specifies that we want to train for 20 epochs. We also supply a  eval_data  to monitor validation accuracy on the validation set.  mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  Here is a sample output  INFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775", 
            "title": "Simple 3-layer MLP"
        }, 
        {
            "location": "/tutorial/mnist/#convolutional-neural-networks", 
            "text": "In the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:  # input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))  We basically defined two convolution modules. Each convolution module is actually a chain of  Convolution ,  tanh  activation and then max  Pooling  operations.  Each sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by  NDArray , a batch of 100 samples is a tensor of shape  (28,28,1,100) . The convolution and pooling operates in the spatial axis, so  kernel=(5,5)  indicate a square region of 5-width and 5-height. The rest of the architecture follows as:  # first fully-connected\nfc1   = @mx.chain mx.Flatten(data=conv2) = \n                  mx.FullyConnected(num_hidden=500) = \n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(data=fc2, name=:softmax)  Note a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a  Flatten  operator to flat the tensor, before connecting it to the  FullyConnected  operator.  The rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)  Note we specified  flat=false  to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.  # fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  And here is a sample of running outputs:  INFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915", 
            "title": "Convolutional Neural Networks"
        }, 
        {
            "location": "/tutorial/mnist/#predicting-with-a-trained-model", 
            "text": "Predicting with a trained model is very simple. By calling  mx.predict  with the model and a data provider, we get the model output as a Julia Array:  probs = mx.predict(model, eval_provider)  The following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:  # collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format( Accuracy on eval set: {1:.2f}% , 100correct/length(labels)))  Alternatively, when the dataset is huge, one can provide a callback to  mx.predict , then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from  mx.predict . See also predict.", 
            "title": "Predicting with a trained model"
        }, 
        {
            "location": "/tutorial/char-lstm/", 
            "text": "Generating Random Sentence with LSTM RNN\n\n\nThis tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called \nchar-rnn\n is described in \nAndrej Karpathy's blog\n, with a reference implementation in Torch available \nhere\n.\n\n\nBecause MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the \nchar-rnn example for MXNet's Python binding\n, which demonstrates how to use low-level \nSymbolic API\n to build customized neural network models directly.\n\n\nThe most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the \nexamples/char-lstm\n directory. You will need to install \nIterators.jl\n and \nStatsBase.jl\n to run this example.\n\n\n\n\nLSTM Cells\n\n\nChristopher Olah has a \ngreat blog post about LSTM\n with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input \nx\n, as well as previous states (including \nc\n and \nh\n), and produce the next states. We define a helper type to bundle the two state variables together:\n\n\nBecause LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.\n\n\nNote all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.\n\n\nThe following figure is stolen (permission requested) from \nChristopher Olah's blog\n, which illustrate exactly what the code snippet above is doing.\n\n\n\n\nIn particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.\n\n\n\n\nUnfolding LSTM\n\n\nUsing the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.\n\n\nThe \nembed_W\n is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The \npred_W\n and \npred_b\n are weights and bias for the final prediction at each time step.\n\n\nThen we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however, \nnot\n shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.\n\n\nUnrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as \n:ptb\n, the data and label at step \nt\n will be named \n:ptb_data_$t\n and \n:ptb_label_$t\n. Late on when we prepare the data, we will define the data provider to match those names.\n\n\nNote at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.\n\n\nIn the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.\n\n\n\n\nData Provider for Text Sequences\n\n\nNow we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.\n\n\nNote the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.\n\n\nThe text sequence data provider implements the \nData Providers\n api. We define the \nCharSeqProvider\n as below:\n\n\nThe provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from \n$name_data_$t\n and \n$name_label_$t\n, we also provides the initial \nc\n and \nh\n states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.\n\n\nNext we implement the \neachbatch\n method from the \nmx.AbstractDataProvider\n interface for the provider. We start by defining the data and label arrays, and the \nDataBatch\n object we will provide in each iteration.\n\n\nThe actual data providing iteration is implemented as a Julia \ncoroutine\n. In this way, we can write the data loading logic as a simple coherent \nfor\n loop, and do not need to implement the interface functions like Base.start, Base.next, etc.\n\n\nBasically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.\n\n\n\n\nTraining the LSTM\n\n\nNow we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:\n\n\nNote all the parameters are defined in \nexamples/char-lstm/config.jl\n. Now we load the text file and define the data provider. The data \ninput.txt\n we used in this example is \na tiny Shakespeare dataset\n. But you can try with other text files.\n\n\nThe last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.\n\n\nNote we are also using a customized \nNLL\n evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.\n\n\n...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'\n\n\n\n\n\n\nSampling Random Sentences\n\n\nAfter training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:\n\n\n\n\nStarting from some fixed character, take \na\n for example, and feed   it as input to the LSTM.\n\n\nThe LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.\n\n\nIn the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).\n\n\nContinue running until we sampled enough characters.\n\n\n\n\nNote we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.\n\n\n## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?\n\n\n\n\nSee \nAndrej Karpathy's blog post\n on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in \nexamples/char-lstm/sampler.jl\n.\n\n\n\n\nVisualizing the LSTM\n\n\nFinally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than \nChristopher Olah's illustrations\n, but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in \nexamples/char-lstm/visualize.jl\n.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#generating-random-sentence-with-lstm-rnn", 
            "text": "This tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called  char-rnn  is described in  Andrej Karpathy's blog , with a reference implementation in Torch available  here .  Because MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the  char-rnn example for MXNet's Python binding , which demonstrates how to use low-level  Symbolic API  to build customized neural network models directly.  The most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the  examples/char-lstm  directory. You will need to install  Iterators.jl  and  StatsBase.jl  to run this example.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#lstm-cells", 
            "text": "Christopher Olah has a  great blog post about LSTM  with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input  x , as well as previous states (including  c  and  h ), and produce the next states. We define a helper type to bundle the two state variables together:  Because LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.  Note all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.  The following figure is stolen (permission requested) from  Christopher Olah's blog , which illustrate exactly what the code snippet above is doing.   In particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.", 
            "title": "LSTM Cells"
        }, 
        {
            "location": "/tutorial/char-lstm/#unfolding-lstm", 
            "text": "Using the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.  The  embed_W  is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The  pred_W  and  pred_b  are weights and bias for the final prediction at each time step.  Then we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however,  not  shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.  Unrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as  :ptb , the data and label at step  t  will be named  :ptb_data_$t  and  :ptb_label_$t . Late on when we prepare the data, we will define the data provider to match those names.  Note at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.  In the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.", 
            "title": "Unfolding LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#data-provider-for-text-sequences", 
            "text": "Now we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.  Note the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.  The text sequence data provider implements the  Data Providers  api. We define the  CharSeqProvider  as below:  The provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from  $name_data_$t  and  $name_label_$t , we also provides the initial  c  and  h  states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.  Next we implement the  eachbatch  method from the  mx.AbstractDataProvider  interface for the provider. We start by defining the data and label arrays, and the  DataBatch  object we will provide in each iteration.  The actual data providing iteration is implemented as a Julia  coroutine . In this way, we can write the data loading logic as a simple coherent  for  loop, and do not need to implement the interface functions like Base.start, Base.next, etc.  Basically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.", 
            "title": "Data Provider for Text Sequences"
        }, 
        {
            "location": "/tutorial/char-lstm/#training-the-lstm", 
            "text": "Now we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:  Note all the parameters are defined in  examples/char-lstm/config.jl . Now we load the text file and define the data provider. The data  input.txt  we used in this example is  a tiny Shakespeare dataset . But you can try with other text files.  The last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.  Note we are also using a customized  NLL  evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.  ...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'", 
            "title": "Training the LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#sampling-random-sentences", 
            "text": "After training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:   Starting from some fixed character, take  a  for example, and feed   it as input to the LSTM.  The LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.  In the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).  Continue running until we sampled enough characters.   Note we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.  ## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?  See  Andrej Karpathy's blog post  on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in  examples/char-lstm/sampler.jl .", 
            "title": "Sampling Random Sentences"
        }, 
        {
            "location": "/tutorial/char-lstm/#visualizing-the-lstm", 
            "text": "Finally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than  Christopher Olah's illustrations , but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in  examples/char-lstm/visualize.jl .", 
            "title": "Visualizing the LSTM"
        }, 
        {
            "location": "/user-guide/install/", 
            "text": "Installation Guide\n\n\n\n\nAutomatic Installation\n\n\nTo install MXNet.jl, simply type\n\n\nPkg.add(\nMXNet\n)\n\n\n\n\nin the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead\n\n\nPkg.checkout(\nMXNet\n)\n\n\n\n\nMXNet.jl is built on top of \nlibmxnet\n. Upon installation, Julia will try to automatically download and build libmxnet.\n\n\nThe libmxnet source is downloaded to \nPkg.dir(\"MXNet\")/deps/src/mxnet\n. The automatic build is using default configurations, with OpenCV, CUDA disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, it is recommended to compile and install libmxnet manually. Please see below \nmanual-compilation\n for more details.\n\n\n\n\nManual Compilation\n\n\nIt is possible to compile libmxnet separately and point MXNet.jl to a the existing library in case automatic compilation fails due to unresolved dependencies in an un-standard environment; Or when one want to work with a seperate, maybe customized libmxnet.\n\n\nTo build libmxnet, please refer to \nthe installation guide of libmxnet\n. After successfully installing libmxnet, set the \nMXNET_HOME\n environment variable to the location of libmxnet. In other words, the compiled \nlibmxnet.so\n should be found in \n$MXNET_HOME/lib\n.\n\n\n\n\nnote\n\n\nThe constant \nMXNET_HOME\n is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by \nBase.compilecache(\"MXNet\")\n.\n\n\n\n\nWhen the \nMXNET_HOME\n environment variable is detected and the corresponding \nlibmxnet.so\n could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.\n\n\nBasically, MXNet.jl will search \nlibmxnet.so\n or \nlibmxnet.dll\n in the following paths (and in that order):\n\n\n\n\n$MXNET_HOME/lib\n: customized libmxnet builds\n\n\nPkg.dir(\"MXNet\")/deps/usr/lib\n: automatic builds\n\n\nAny system wide library search path\n\n\n\n\nNote that MXNet.jl will not find \nlibmxnet.so\n even if it is on one of the paths above if a library it depends upon is missing from the \nLD_LIBRARY_PATH\n. Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to \nLD_LIBRARY_PATH\n.", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#installation-guide", 
            "text": "", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#automatic-installation", 
            "text": "To install MXNet.jl, simply type  Pkg.add( MXNet )  in the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead  Pkg.checkout( MXNet )  MXNet.jl is built on top of  libmxnet . Upon installation, Julia will try to automatically download and build libmxnet.  The libmxnet source is downloaded to  Pkg.dir(\"MXNet\")/deps/src/mxnet . The automatic build is using default configurations, with OpenCV, CUDA disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, it is recommended to compile and install libmxnet manually. Please see below  manual-compilation  for more details.", 
            "title": "Automatic Installation"
        }, 
        {
            "location": "/user-guide/install/#manual-compilation", 
            "text": "It is possible to compile libmxnet separately and point MXNet.jl to a the existing library in case automatic compilation fails due to unresolved dependencies in an un-standard environment; Or when one want to work with a seperate, maybe customized libmxnet.  To build libmxnet, please refer to  the installation guide of libmxnet . After successfully installing libmxnet, set the  MXNET_HOME  environment variable to the location of libmxnet. In other words, the compiled  libmxnet.so  should be found in  $MXNET_HOME/lib .   note  The constant  MXNET_HOME  is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by  Base.compilecache(\"MXNet\") .   When the  MXNET_HOME  environment variable is detected and the corresponding  libmxnet.so  could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.  Basically, MXNet.jl will search  libmxnet.so  or  libmxnet.dll  in the following paths (and in that order):   $MXNET_HOME/lib : customized libmxnet builds  Pkg.dir(\"MXNet\")/deps/usr/lib : automatic builds  Any system wide library search path   Note that MXNet.jl will not find  libmxnet.so  even if it is on one of the paths above if a library it depends upon is missing from the  LD_LIBRARY_PATH . Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to  LD_LIBRARY_PATH .", 
            "title": "Manual Compilation"
        }, 
        {
            "location": "/user-guide/overview/", 
            "text": "Overview\n\n\n\n\nMXNet.jl Namespace\n\n\nMost the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a \nmx\n module. The convention of accessing the MXNet.jl interface is the to use the \nmx.\n prefix explicitly:\n\n\nusing MXNet\n\nx = mx.zeros(2,3)              # MXNet NDArray\ny = zeros(eltype(x), size(x))  # Julia Array\ncopy!(y, x)                    # Overloaded function in Julia Base\nz = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\nmx.copy!(z, y)                 # Same as copy!(z, y)\n\n\n\n\nNote functions like \nsize\n, \ncopy!\n that is extensively overloaded for various types works out of the box. But functions like \nzeros\n and \nones\n will be ambiguous, so we always use the \nmx.\n prefix. If you prefer, the \nmx.\n prefix can be used explicitly for all MXNet.jl functions, including \nsize\n and \ncopy!\n as shown in the last line.\n\n\n\n\nLow Level Interface\n\n\n\n\nNDArrays\n\n\nNDArray is the basic building blocks of the actual computations in MXNet. It is like a Julia \nArray\n object, with some important differences listed here:\n\n\n\n\nThe actual data could live on different \nContext\n (e.g. GPUs). For   some contexts, iterating into the elements one by one is very slow,   thus indexing into NDArray is not supported in general. The easiest   way to inspect the contents of an NDArray is to use the \ncopy\n   function to copy the contents as a Julia \nArray\n.\n\n\nOperations on NDArray (including basic arithmetics and neural   network related operators) are executed in parallel with automatic   dependency tracking to ensure correctness.\n\n\nThere is no generics in NDArray, the \neltype\n is always   \nmx.MX_float\n. Because for applications in machine learning, single   precision floating point numbers are typical a best choice balancing   between precision, speed and portability. Also since libmxnet is   designed to support multiple languages as front-ends, it is much   simpler to implement with a fixed data type.\n\n\n\n\nWhile most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the NDArray API is useful for implementing \nOptimizer\n or customized operators in Julia directly.\n\n\nThe followings are common ways to create NDArray objects:\n\n\n\n\nmx.empty(shape[, context])\n: create on uninitialized array of a   given shape on a specific device. For example,   $mx.empty(2,3)\n,\nmx.((2,3), mx.gpu(2))$.\n\n\nmx.zeros(shape[, context])\n and \nmx.ones(shape[, context])\n:   similar to the Julia's built-in \nzeros\n and \nones\n.\n\n\nmx.copy(jl_arr, context)\n: copy the contents of a Julia \nArray\n to   a specific device.\n\n\n\n\nMost of the convenient functions like \nsize\n, \nlength\n, \nndims\n, \neltype\n on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take \nslices\n:\n\n\na = mx.ones(2,3)\nb = mx.slice(a, 1:2)\nb[:] = 2\nprintln(copy(a))\n# =\n\n# Float32[2.0 2.0 1.0\n#         2.0 2.0 1.0]\n\n\n\n\nA slice is a sub-region sharing the same memory with the original NDArray object. A slice is always a contiguous piece of memory, so only slicing on the \nlast\n dimension is supported. The example above also shows a way to set the contents of an NDArray.\n\n\na = mx.empty(2,3)\na[:] = 0.5              # set all elements to a scalar\na[:] = rand(size(a))    # set contents with a Julia Array\ncopy!(a, rand(size(a))) # set value by copying a Julia Array\nb = mx.empty(size(a))\nb[:] = a                # copying and assignment between NDArrays\n\n\n\n\nNote due to the intrinsic design of the Julia language, a normal assignment\n\n\na = b\n\n\n\n\ndoes \nnot\n mean copying the contents of \nb\n to \na\n. Instead, it just make the variable \na\n pointing to a new object, which is \nb\n. Similarly, inplace arithmetics does not work as expected:\n\n\na = mx.ones(2)\nr = a           # keep a reference to a\nb = mx.ones(2)\na += b          # translates to a = a + b\nprintln(copy(a))\n# =\n Float32[2.0f0,2.0f0]\nprintln(copy(r))\n# =\n Float32[1.0f0,1.0f0]\n\n\n\n\nAs we can see, \na\n has expected value, but instead of inplace updating, a new NDArray is created and \na\n is set to point to this new object. If we look at \nr\n, which still reference to the old \na\n, its content has not changed. There is currently no way in Julia to overload the operators like \n+=\n to get customized behavior.\n\n\nInstead, you will need to write \na[:] = a+b\n, or if you want \nreal\n inplace \n+=\n operation, MXNet.jl provides a simple macro \n@mx.inplace\n:\n\n\n@mx.inplace a += b\nmacroexpand(:(@mx.inplace a += b))\n# =\n :(MXNet.mx.add_to!(a,b))\n\n\n\n\nAs we can see, it translate the \n+=\n operator to an explicit \nadd_to!\n function call, which invokes into libmxnet to add the contents of \nb\n into \na\n directly. For example, the following is the update rule in the SGD \nOptimizer\n (both \ngrad\n and \nweight\n are NDArray objects):\n\n\n@inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)\n\n\n\n\nNote there is no much magic in \nmx.inplace\n: it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by \ngrad_scale\n and adding the weight decay all create temporary NDArray objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp NDArray vs. pre-allocating:\n\n\nusing Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op()))) \n 1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))\n\n\n\n\nThe comparison on my laptop shows that \nnormal_op\n while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing \nN_REP\n), is only about twice slower than the pre-allocated one.\n\n\nRow    Function        Average      Relative    Replications   \u2013\u2013\u2013 \u2013\u2013\u2013\u2013\u2013\u2013\u2013- \u2013\u2013\u2013\u2013\u2013\u2013 \u2013\u2013\u2013\u2013\u2013- \u2013\u2013\u2013\u2013\u2013\u2013\u2013-   1      \"inplace_op\"   0.0074854    1.0         100   2      \"normal_op\"    0.0174202    2.32723     100\n\n\nSo it will usually not be a big problem unless you are at the bottleneck of the computation.\n\n\n\n\nDistributed Key-value Store\n\n\nThe type \nKVStore\n and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.\n\n\nThe following example shows how to create a local \nKVStore\n, initialize a value and then pull it back.\n\n\nkv    = mx.KVStore(:local)\nshape = (2,3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape)*2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\nprintln(copy(a))\n# =\n\n# Float32[2.0 2.0 2.0\n#        2.0 2.0 2.0]\n\n\n\n\n\n\nIntermediate Level Interface\n\n\n\n\nSymbols and Composition\n\n\nThe way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like \nTheano\n, except that we avoided long expression compiliation time by providing \nlarger\n neural network related building blocks to guarantee computation performance. See also \nthis note\n for the design and trade-off of the MXNet symbolic composition system.\n\n\nThe basic type is \nmx.Symbol\n. The following is a trivial example of composing two symbols with the \n+\n operation.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B\n\n\n\n\nWe get a new \nsymbol\n by composing existing \nsymbols\n by some \noperations\n. A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a ReLU activation function.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet = mx.Activation(data=net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(data=net, name=:fc2, num_hidden=64)\nnet = mx.Softmax(data=net, name=:out)\n\n\n\n\nEach time we take the previous symbol, and compose with an operation. Unlike the simple \n+\n example above, the \noperations\n here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.\n\n\nEach of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g. \nnum_hidden\n, \nact_type\n) to further customize the composition results.\n\n\nWhen applying those operations, we can also specify a \nname\n for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.\n\n\nEach symbol takes some arguments. For example, in the \n+\n case above, to compute the value of \nC\n, we will need to know the values of the two inputs \nA\n and \nB\n. For neural networks, the arguments are primarily two categories: \ninputs\n and \nparameters\n. \ninputs\n are data and labels for the networks, while \nparameters\n are typically trainable \nweights\n, \nbias\n, \nfilters\n.\n\n\nWhen composing symbols, their arguments accumulates. We can list all the arguments by\n\n\njulia\n mx.list_arguments(net)\n6-element Array{Symbol,1}:\n :data         # Input data, name from the first data variable\n :fc1_weight   # Weights of the fully connected layer named :fc1\n :fc1_bias     # Bias of the layer :fc1\n :fc2_weight   # Weights of the layer :fc2\n :fc2_bias     # Bias of the layer :fc2\n :out_label    # Input label, required by the softmax layer named :out\n\n\n\n\nNote the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:\n\n\nnet = mx.Variable(:data)\nw   = mx.Variable(:myweight)\nnet = mx.FullyConnected(data=data, weight=w, name=:fc1, num_hidden=128)\nmx.list_arguments(net)\n# =\n\n# 3-element Array{Symbol,1}:\n#  :data\n#  :myweight\n#  :fc1_bias\n\n\n\n\nThe simple fact is that a \nVariable\n is just a placeholder \nmx.Symbol\n. In composition, we can use arbitrary symbols for arguments. For example:\n\n\nnet  = mx.Variable(:data)\nnet  = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet2 = mx.Variable(:data2)\nnet2 = mx.FullyConnected(data=net2, name=:net2, num_hidden=128)\nmx.list_arguments(net2)\n# =\n\n# 3-element Array{Symbol,1}:\n#  :data2\n#  :net2_weight\n#  :net2_bias\ncomposed_net = net2(data2=net, name=:composed)\nmx.list_arguments(composed_net)\n# =\n\n# 5-element Array{Symbol,1}:\n#  :data\n#  :fc1_weight\n#  :fc1_bias\n#  :net2_weight\n#  :net2_bias\n\n\n\n\nNote we use a composed symbol, \nnet\n as the argument \ndata2\n for \nnet2\n to get a new symbol, which we named \n:composed\n. It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.\n\n\n\n\nShape Inference\n\n\nGiven enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like \nnum_hidden\n, the shapes for the weights and bias in a neural network could be inferred.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=10)\narg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))\n\n\n\n\nThe returned shapes corresponds to arguments with the same order as returned by \nmx.list_arguments\n. The \nout_shapes\n are shapes for outputs, and \naux_shapes\n can be safely ignored for now.\n\n\nfor (n,s) in zip(mx.list_arguments(net), arg_shapes)\n  println(\n$n =\n $s\n)\nend\n# =\n\n# data =\n (10,64)\n# fc1_weight =\n (10,10)\n# fc1_bias =\n (10,)\nfor (n,s) in zip(mx.list_outputs(net), out_shapes)\n  println(\n$n =\n $s\n)\nend\n# =\n\n# fc1_output =\n (10,64)\n\n\n\n\n\n\nBinding and Executing\n\n\nIn order to execute the computation graph specified a composed symbol, we will \nbind\n the free variables to concrete values, specified as \nmx.NDArray\n. This will create an \nmx.Executor\n on a given \nmx.Context\n. A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A .* B\na = mx.ones(3) * 4\nb = mx.ones(3) * 2\nc_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =\n a, :B =\n b))\n\nmx.forward(c_exec)\ncopy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n# =\n\n# 3-element Array{Float32,1}:\n#  8.0\n#  8.0\n#  8.0\n\n\n\n\nFor neural networks, it is easier to use \nsimple_bind\n. By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the \nModel\n interface.\n\n\nTODO\n Provide pointers to model tutorial and further details about binding and symbolic API.\n\n\n\n\nHigh Level Interface\n\n\nThe high level interface include model training and prediction API, etc.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#mxnetjl-namespace", 
            "text": "Most the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a  mx  module. The convention of accessing the MXNet.jl interface is the to use the  mx.  prefix explicitly:  using MXNet\n\nx = mx.zeros(2,3)              # MXNet NDArray\ny = zeros(eltype(x), size(x))  # Julia Array\ncopy!(y, x)                    # Overloaded function in Julia Base\nz = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\nmx.copy!(z, y)                 # Same as copy!(z, y)  Note functions like  size ,  copy!  that is extensively overloaded for various types works out of the box. But functions like  zeros  and  ones  will be ambiguous, so we always use the  mx.  prefix. If you prefer, the  mx.  prefix can be used explicitly for all MXNet.jl functions, including  size  and  copy!  as shown in the last line.", 
            "title": "MXNet.jl Namespace"
        }, 
        {
            "location": "/user-guide/overview/#low-level-interface", 
            "text": "", 
            "title": "Low Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#ndarrays", 
            "text": "NDArray is the basic building blocks of the actual computations in MXNet. It is like a Julia  Array  object, with some important differences listed here:   The actual data could live on different  Context  (e.g. GPUs). For   some contexts, iterating into the elements one by one is very slow,   thus indexing into NDArray is not supported in general. The easiest   way to inspect the contents of an NDArray is to use the  copy    function to copy the contents as a Julia  Array .  Operations on NDArray (including basic arithmetics and neural   network related operators) are executed in parallel with automatic   dependency tracking to ensure correctness.  There is no generics in NDArray, the  eltype  is always    mx.MX_float . Because for applications in machine learning, single   precision floating point numbers are typical a best choice balancing   between precision, speed and portability. Also since libmxnet is   designed to support multiple languages as front-ends, it is much   simpler to implement with a fixed data type.   While most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the NDArray API is useful for implementing  Optimizer  or customized operators in Julia directly.  The followings are common ways to create NDArray objects:   mx.empty(shape[, context]) : create on uninitialized array of a   given shape on a specific device. For example,   $mx.empty(2,3) , mx.((2,3), mx.gpu(2))$.  mx.zeros(shape[, context])  and  mx.ones(shape[, context]) :   similar to the Julia's built-in  zeros  and  ones .  mx.copy(jl_arr, context) : copy the contents of a Julia  Array  to   a specific device.   Most of the convenient functions like  size ,  length ,  ndims ,  eltype  on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take  slices :  a = mx.ones(2,3)\nb = mx.slice(a, 1:2)\nb[:] = 2\nprintln(copy(a))\n# = \n# Float32[2.0 2.0 1.0\n#         2.0 2.0 1.0]  A slice is a sub-region sharing the same memory with the original NDArray object. A slice is always a contiguous piece of memory, so only slicing on the  last  dimension is supported. The example above also shows a way to set the contents of an NDArray.  a = mx.empty(2,3)\na[:] = 0.5              # set all elements to a scalar\na[:] = rand(size(a))    # set contents with a Julia Array\ncopy!(a, rand(size(a))) # set value by copying a Julia Array\nb = mx.empty(size(a))\nb[:] = a                # copying and assignment between NDArrays  Note due to the intrinsic design of the Julia language, a normal assignment  a = b  does  not  mean copying the contents of  b  to  a . Instead, it just make the variable  a  pointing to a new object, which is  b . Similarly, inplace arithmetics does not work as expected:  a = mx.ones(2)\nr = a           # keep a reference to a\nb = mx.ones(2)\na += b          # translates to a = a + b\nprintln(copy(a))\n# =  Float32[2.0f0,2.0f0]\nprintln(copy(r))\n# =  Float32[1.0f0,1.0f0]  As we can see,  a  has expected value, but instead of inplace updating, a new NDArray is created and  a  is set to point to this new object. If we look at  r , which still reference to the old  a , its content has not changed. There is currently no way in Julia to overload the operators like  +=  to get customized behavior.  Instead, you will need to write  a[:] = a+b , or if you want  real  inplace  +=  operation, MXNet.jl provides a simple macro  @mx.inplace :  @mx.inplace a += b\nmacroexpand(:(@mx.inplace a += b))\n# =  :(MXNet.mx.add_to!(a,b))  As we can see, it translate the  +=  operator to an explicit  add_to!  function call, which invokes into libmxnet to add the contents of  b  into  a  directly. For example, the following is the update rule in the SGD  Optimizer  (both  grad  and  weight  are NDArray objects):  @inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)  Note there is no much magic in  mx.inplace : it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by  grad_scale  and adding the weight decay all create temporary NDArray objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp NDArray vs. pre-allocating:  using Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op())))   1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))  The comparison on my laptop shows that  normal_op  while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing  N_REP ), is only about twice slower than the pre-allocated one.  Row    Function        Average      Relative    Replications   \u2013\u2013\u2013 \u2013\u2013\u2013\u2013\u2013\u2013\u2013- \u2013\u2013\u2013\u2013\u2013\u2013 \u2013\u2013\u2013\u2013\u2013- \u2013\u2013\u2013\u2013\u2013\u2013\u2013-   1      \"inplace_op\"   0.0074854    1.0         100   2      \"normal_op\"    0.0174202    2.32723     100  So it will usually not be a big problem unless you are at the bottleneck of the computation.", 
            "title": "NDArrays"
        }, 
        {
            "location": "/user-guide/overview/#distributed-key-value-store", 
            "text": "The type  KVStore  and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.  The following example shows how to create a local  KVStore , initialize a value and then pull it back.  kv    = mx.KVStore(:local)\nshape = (2,3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape)*2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\nprintln(copy(a))\n# = \n# Float32[2.0 2.0 2.0\n#        2.0 2.0 2.0]", 
            "title": "Distributed Key-value Store"
        }, 
        {
            "location": "/user-guide/overview/#intermediate-level-interface", 
            "text": "", 
            "title": "Intermediate Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#symbols-and-composition", 
            "text": "The way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like  Theano , except that we avoided long expression compiliation time by providing  larger  neural network related building blocks to guarantee computation performance. See also  this note  for the design and trade-off of the MXNet symbolic composition system.  The basic type is  mx.Symbol . The following is a trivial example of composing two symbols with the  +  operation.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B  We get a new  symbol  by composing existing  symbols  by some  operations . A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a ReLU activation function.  net = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet = mx.Activation(data=net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(data=net, name=:fc2, num_hidden=64)\nnet = mx.Softmax(data=net, name=:out)  Each time we take the previous symbol, and compose with an operation. Unlike the simple  +  example above, the  operations  here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.  Each of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g.  num_hidden ,  act_type ) to further customize the composition results.  When applying those operations, we can also specify a  name  for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.  Each symbol takes some arguments. For example, in the  +  case above, to compute the value of  C , we will need to know the values of the two inputs  A  and  B . For neural networks, the arguments are primarily two categories:  inputs  and  parameters .  inputs  are data and labels for the networks, while  parameters  are typically trainable  weights ,  bias ,  filters .  When composing symbols, their arguments accumulates. We can list all the arguments by  julia  mx.list_arguments(net)\n6-element Array{Symbol,1}:\n :data         # Input data, name from the first data variable\n :fc1_weight   # Weights of the fully connected layer named :fc1\n :fc1_bias     # Bias of the layer :fc1\n :fc2_weight   # Weights of the layer :fc2\n :fc2_bias     # Bias of the layer :fc2\n :out_label    # Input label, required by the softmax layer named :out  Note the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:  net = mx.Variable(:data)\nw   = mx.Variable(:myweight)\nnet = mx.FullyConnected(data=data, weight=w, name=:fc1, num_hidden=128)\nmx.list_arguments(net)\n# = \n# 3-element Array{Symbol,1}:\n#  :data\n#  :myweight\n#  :fc1_bias  The simple fact is that a  Variable  is just a placeholder  mx.Symbol . In composition, we can use arbitrary symbols for arguments. For example:  net  = mx.Variable(:data)\nnet  = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet2 = mx.Variable(:data2)\nnet2 = mx.FullyConnected(data=net2, name=:net2, num_hidden=128)\nmx.list_arguments(net2)\n# = \n# 3-element Array{Symbol,1}:\n#  :data2\n#  :net2_weight\n#  :net2_bias\ncomposed_net = net2(data2=net, name=:composed)\nmx.list_arguments(composed_net)\n# = \n# 5-element Array{Symbol,1}:\n#  :data\n#  :fc1_weight\n#  :fc1_bias\n#  :net2_weight\n#  :net2_bias  Note we use a composed symbol,  net  as the argument  data2  for  net2  to get a new symbol, which we named  :composed . It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.", 
            "title": "Symbols and Composition"
        }, 
        {
            "location": "/user-guide/overview/#shape-inference", 
            "text": "Given enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like  num_hidden , the shapes for the weights and bias in a neural network could be inferred.  net = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=10)\narg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))  The returned shapes corresponds to arguments with the same order as returned by  mx.list_arguments . The  out_shapes  are shapes for outputs, and  aux_shapes  can be safely ignored for now.  for (n,s) in zip(mx.list_arguments(net), arg_shapes)\n  println( $n =  $s )\nend\n# = \n# data =  (10,64)\n# fc1_weight =  (10,10)\n# fc1_bias =  (10,)\nfor (n,s) in zip(mx.list_outputs(net), out_shapes)\n  println( $n =  $s )\nend\n# = \n# fc1_output =  (10,64)", 
            "title": "Shape Inference"
        }, 
        {
            "location": "/user-guide/overview/#binding-and-executing", 
            "text": "In order to execute the computation graph specified a composed symbol, we will  bind  the free variables to concrete values, specified as  mx.NDArray . This will create an  mx.Executor  on a given  mx.Context . A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A .* B\na = mx.ones(3) * 4\nb = mx.ones(3) * 2\nc_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =  a, :B =  b))\n\nmx.forward(c_exec)\ncopy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n# = \n# 3-element Array{Float32,1}:\n#  8.0\n#  8.0\n#  8.0  For neural networks, it is easier to use  simple_bind . By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the  Model  interface.  TODO  Provide pointers to model tutorial and further details about binding and symbolic API.", 
            "title": "Binding and Executing"
        }, 
        {
            "location": "/user-guide/overview/#high-level-interface", 
            "text": "The high level interface include model training and prediction API, etc.", 
            "title": "High Level Interface"
        }, 
        {
            "location": "/user-guide/faq/", 
            "text": "FAQ\n\n\n\n\nRunning MXNet on AWS GPU instances\n\n\nSee the discussions and notes \nhere\n.", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#faq", 
            "text": "", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#running-mxnet-on-aws-gpu-instances", 
            "text": "See the discussions and notes  here .", 
            "title": "Running MXNet on AWS GPU instances"
        }, 
        {
            "location": "/api/context/", 
            "text": "Context\n\n\n#\n\n\nMXNet.mx.Context\n \n \nType\n.\n\n\nContext(dev_type, dev_id)\n\n\n\n\nA context describes the device type and id on which computation should be carried on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cpu\n \n \nFunction\n.\n\n\ncpu(dev_id)\n\n\n\n\nGet a CPU context with a specific id. $cpu()$ is usually the default context for many operations when no context is specified.\n\n\nArguments\n\n\n\n\ndev_id::Int = 0\n: the CPU id.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gpu\n \n \nFunction\n.\n\n\ngpu(dev_id)\n\n\n\n\nGet a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.\n\n\nArguments\n\n\n\n\ndev_id :: Int = 0\n the GPU device id.\n\n\n\n\nsource", 
            "title": "Context"
        }, 
        {
            "location": "/api/context/#context", 
            "text": "#  MXNet.mx.Context     Type .  Context(dev_type, dev_id)  A context describes the device type and id on which computation should be carried on.  source  #  MXNet.mx.cpu     Function .  cpu(dev_id)  Get a CPU context with a specific id. $cpu()$ is usually the default context for many operations when no context is specified.  Arguments   dev_id::Int = 0 : the CPU id.   source  #  MXNet.mx.gpu     Function .  gpu(dev_id)  Get a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.  Arguments   dev_id :: Int = 0  the GPU device id.   source", 
            "title": "Context"
        }, 
        {
            "location": "/api/model/", 
            "text": "Model\n\n\nThe model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.\n\n\n#\n\n\nMXNet.mx.AbstractModel\n \n \nType\n.\n\n\nAbstractModel\n\n\n\n\nThe abstract super type of all models in MXNet.jl.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nType\n.\n\n\nFeedForward\n\n\n\n\nThe feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of \ntime index\n, but it is relatively easy to implement unrolled RNN / LSTM under this framework (\nTODO\n: add example). For models that handles sequential data explicitly, please use \nTODO\n...\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nMethod\n.\n\n\nFeedForward(arch :: SymbolicNode, ctx)\n\n\n\n\n\n\narch: the architecture of the network constructed using the symbolic API.\n\n\nctx: the devices on which this model should do computation. It could be a single :class:\nContext\n              or a list of :class:\nContext\n objects. In the latter case, data parallelization will be used              for training. If no context is provided, the default context $cpu()$ will be used.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._split_inputs\n \n \nMethod\n.\n\n\nGet a split of \nbatch_size\n into \nn_split\n pieces for data parallelization. Returns a vector of length \nn_split\n, with each entry a \nUnitRange{Int}\n indicating the slice index for that piece.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fit\n \n \nMethod\n.\n\n\nfit(model :: FeedForward, optimizer, data; kwargs...)\n\n\n\n\nTrain the $model$ on $data$ with the $optimizer$.\n\n\n\n\nFeedForward model: the model to be trained.\n\n\nAbstractOptimizer optimizer: the optimization algorithm to use.\n\n\nAbstractDataProvider data: the training data provider.\n\n\nInt n_epoch: default 10, the number of full data-passes to run.\n\n\nAbstractDataProvider eval_data: keyword argument, default $nothing$. The data provider for         the validation set.\n\n\nAbstractEvalMetric eval_metric: keyword argument, default $Accuracy()$. The metric used         to evaluate the training performance. If $eval_data$ is provided, the same metric is also         calculated on the validation set.\n\n\nkvstore: keyword argument, default $:local$. The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore: :class:\nKVStore\n or $Base.Symbol$\n\n\nAbstractInitializer initializer: keyword argument, default $UniformInitializer(0.01)$.\n\n\nBool force_init: keyword argument, default false. By default, the random initialization using the         provided $initializer$ will be skipped if the model weights already exists, maybe from a previous         call to :func:\ntrain\n or an explicit call to :func:\ninit_model\n or :func:\nload_checkpoint\n. When         this option is set, it will always do random initialization at the begining of training.\n\n\ncallbacks: keyword argument, default $[]$. Callbacks to be invoked at each epoch or mini-batch,         see :class:\nAbstractCallback\n.  :type callbacks: $Vector{AbstractCallback}$\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.init_model\n \n \nMethod\n.\n\n\ninit_model(self, initializer; overwrite=false, input_shapes...)\n\n\n\n\nInitialize the weights in the model.\n\n\nThis method will be called automatically when training a model. So there is usually no    need to call this method unless one needs to inspect a model with only randomly initialized    weights.\n\n\n\n\nFeedForward self: the model to be initialized.\n\n\nAbstractInitializer initializer: an initializer describing how the weights should be initialized.\n\n\nBool overwrite: keyword argument, force initialization even when weights already exists.\n\n\ninput_shapes: the shape of all data and label inputs to this model, given as keyword arguments.                       For example, $data=(28,28,1,100), label=(100,)$.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.predict\n \n \nMethod\n.\n\n\n.. function::    predict(self, data; overwrite=false, callback=nothing)\n\n\nPredict using an existing model. The model should be already initialized, or trained or loaded from    a checkpoint. There is an overloaded function that allows to pass the callback as the first argument,    so it is possible to do\n\n\n.. code-block:: julia\n\n\n  predict(model, data) do batch_output\n    # consume or write batch_output to file\n  end\n\n\n\n\n\n\nFeedForward self: the model.\n\n\nAbstractDataProvider data: the data to perform prediction on.\n\n\n\n\nBool overwrite: an :class:\nExecutor\n is initialized the first time predict is called. The memory                         allocation of the :class:\nExecutor\n depends on the mini-batch size of the test                         data provider. If you call predict twice with data provider of the same batch-size,                         then the executor can be potentially be re-used. So, if $overwrite$ is false,                         we will try to re-use, and raise an error if batch-size changed. If $overwrite$                         is true (the default), a new :class:\nExecutor\n will be created to replace the old one.\n\n\n.. note::\n\n\n```\nPrediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO\nfor copying mini-batches of data. Since there is no concern about convergence in prediction, it is better\nto set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a\nconcern.\n\n\nFor the same reason, currently prediction will only use the first device even if multiple devices are\nprovided to construct the model.\n```\n\n\n.. note::\n\n\nIf you perform further after prediction. The weights are not automatically synchronized if ``overwrite``\nis set to false and the old predictor is re-used. In this case\nsetting ``overwrite`` to true (the default) will re-initialize the predictor the next time you call\npredict and synchronize the weights again.\n\n\n:seealso: :func:\ntrain\n, :func:\nfit\n, :func:\ninit_model\n, :func:\nload_checkpoint\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.train\n \n \nMethod\n.\n\n\ntrain(model :: FeedForward, ...)\n\n\n\n\nAlias to :func:\nfit\n.\n\n\nsource", 
            "title": "Models"
        }, 
        {
            "location": "/api/model/#model", 
            "text": "The model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.  #  MXNet.mx.AbstractModel     Type .  AbstractModel  The abstract super type of all models in MXNet.jl.  source  #  MXNet.mx.FeedForward     Type .  FeedForward  The feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of  time index , but it is relatively easy to implement unrolled RNN / LSTM under this framework ( TODO : add example). For models that handles sequential data explicitly, please use  TODO ...  source  #  MXNet.mx.FeedForward     Method .  FeedForward(arch :: SymbolicNode, ctx)   arch: the architecture of the network constructed using the symbolic API.  ctx: the devices on which this model should do computation. It could be a single :class: Context               or a list of :class: Context  objects. In the latter case, data parallelization will be used              for training. If no context is provided, the default context $cpu()$ will be used.   source  #  MXNet.mx._split_inputs     Method .  Get a split of  batch_size  into  n_split  pieces for data parallelization. Returns a vector of length  n_split , with each entry a  UnitRange{Int}  indicating the slice index for that piece.  source  #  MXNet.mx.fit     Method .  fit(model :: FeedForward, optimizer, data; kwargs...)  Train the $model$ on $data$ with the $optimizer$.   FeedForward model: the model to be trained.  AbstractOptimizer optimizer: the optimization algorithm to use.  AbstractDataProvider data: the training data provider.  Int n_epoch: default 10, the number of full data-passes to run.  AbstractDataProvider eval_data: keyword argument, default $nothing$. The data provider for         the validation set.  AbstractEvalMetric eval_metric: keyword argument, default $Accuracy()$. The metric used         to evaluate the training performance. If $eval_data$ is provided, the same metric is also         calculated on the validation set.  kvstore: keyword argument, default $:local$. The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore: :class: KVStore  or $Base.Symbol$  AbstractInitializer initializer: keyword argument, default $UniformInitializer(0.01)$.  Bool force_init: keyword argument, default false. By default, the random initialization using the         provided $initializer$ will be skipped if the model weights already exists, maybe from a previous         call to :func: train  or an explicit call to :func: init_model  or :func: load_checkpoint . When         this option is set, it will always do random initialization at the begining of training.  callbacks: keyword argument, default $[]$. Callbacks to be invoked at each epoch or mini-batch,         see :class: AbstractCallback .  :type callbacks: $Vector{AbstractCallback}$   source  #  MXNet.mx.init_model     Method .  init_model(self, initializer; overwrite=false, input_shapes...)  Initialize the weights in the model.  This method will be called automatically when training a model. So there is usually no    need to call this method unless one needs to inspect a model with only randomly initialized    weights.   FeedForward self: the model to be initialized.  AbstractInitializer initializer: an initializer describing how the weights should be initialized.  Bool overwrite: keyword argument, force initialization even when weights already exists.  input_shapes: the shape of all data and label inputs to this model, given as keyword arguments.                       For example, $data=(28,28,1,100), label=(100,)$.   source  #  MXNet.mx.predict     Method .  .. function::    predict(self, data; overwrite=false, callback=nothing)  Predict using an existing model. The model should be already initialized, or trained or loaded from    a checkpoint. There is an overloaded function that allows to pass the callback as the first argument,    so it is possible to do  .. code-block:: julia    predict(model, data) do batch_output\n    # consume or write batch_output to file\n  end   FeedForward self: the model.  AbstractDataProvider data: the data to perform prediction on.   Bool overwrite: an :class: Executor  is initialized the first time predict is called. The memory                         allocation of the :class: Executor  depends on the mini-batch size of the test                         data provider. If you call predict twice with data provider of the same batch-size,                         then the executor can be potentially be re-used. So, if $overwrite$ is false,                         we will try to re-use, and raise an error if batch-size changed. If $overwrite$                         is true (the default), a new :class: Executor  will be created to replace the old one.  .. note::  ```\nPrediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO\nfor copying mini-batches of data. Since there is no concern about convergence in prediction, it is better\nto set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a\nconcern.  For the same reason, currently prediction will only use the first device even if multiple devices are\nprovided to construct the model.\n```  .. note::  If you perform further after prediction. The weights are not automatically synchronized if ``overwrite``\nis set to false and the old predictor is re-used. In this case\nsetting ``overwrite`` to true (the default) will re-initialize the predictor the next time you call\npredict and synchronize the weights again.  :seealso: :func: train , :func: fit , :func: init_model , :func: load_checkpoint    source  #  MXNet.mx.train     Method .  train(model :: FeedForward, ...)  Alias to :func: fit .  source", 
            "title": "Model"
        }, 
        {
            "location": "/api/initializer/", 
            "text": "Initializer\n\n\n#\n\n\nMXNet.mx.AbstractInitializer\n \n \nType\n.\n\n\nAbstractInitializer\n\n\n\n\nThe abstract base class for all initializers.\n\n\nTo define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:\n\n\n_init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nOr, if full behavior customization is needed, override the following function\n\n\ninit(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nType\n.\n\n\nNormalInitializer\n\n\n\n\nInitialize weights according to a univariate Gaussian distribution.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nMethod\n.\n\n\nNormalIninitializer(; mu=0, sigma=0.01)\n\n\n\n\nConstruct a :class:\nNormalInitializer\n with mean $mu$ and variance $sigma$.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nType\n.\n\n\nUniformInitializer\n\n\n\n\nInitialize weights according to a uniform distribution within the provided scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nMethod\n.\n\n\nUniformInitializer(scale=0.07)\n\n\n\n\nConstruct a :class:\nUniformInitializer\n with the specified scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.XavierDistribution\n \n \nType\n.\n\n\nXavierInitializer\n\n\n\n\nThe initializer documented in the paper [Bengio and Glorot 2010]: \nUnderstanding the difficulty of training deep feedforward neuralnetworks\n.\n\n\nThere are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.\n\n\nSeveral different ways of calculating the variance are given in the literature or are used by various libraries.\n\n\n\n\n[Bengio and Glorot 2010]: $mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)$\n\n\n[K. He, X. Zhang, S. Ren, and J. Sun 2015]: $mx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)$\n\n\ncaffe_avg: $mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)$\n\n\n\n\nsource", 
            "title": "Initializers"
        }, 
        {
            "location": "/api/initializer/#initializer", 
            "text": "#  MXNet.mx.AbstractInitializer     Type .  AbstractInitializer  The abstract base class for all initializers.  To define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:  _init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  Or, if full behavior customization is needed, override the following function  init(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  source  #  MXNet.mx.NormalInitializer     Type .  NormalInitializer  Initialize weights according to a univariate Gaussian distribution.  source  #  MXNet.mx.NormalInitializer     Method .  NormalIninitializer(; mu=0, sigma=0.01)  Construct a :class: NormalInitializer  with mean $mu$ and variance $sigma$.  source  #  MXNet.mx.UniformInitializer     Type .  UniformInitializer  Initialize weights according to a uniform distribution within the provided scale.  source  #  MXNet.mx.UniformInitializer     Method .  UniformInitializer(scale=0.07)  Construct a :class: UniformInitializer  with the specified scale.  source  #  MXNet.mx.XavierDistribution     Type .  XavierInitializer  The initializer documented in the paper [Bengio and Glorot 2010]:  Understanding the difficulty of training deep feedforward neuralnetworks .  There are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.  Several different ways of calculating the variance are given in the literature or are used by various libraries.   [Bengio and Glorot 2010]: $mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)$  [K. He, X. Zhang, S. Ren, and J. Sun 2015]: $mx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)$  caffe_avg: $mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)$   source", 
            "title": "Initializer"
        }, 
        {
            "location": "/api/optimizer/", 
            "text": "Optimizers\n\n\n#\n\n\nMXNet.mx.AbstractLearningRateScheduler\n \n \nType\n.\n\n\nAbstractLearningRateScheduler\n\n\n\n\nBase type for all learning rate scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractMomentumScheduler\n \n \nType\n.\n\n\nAbstractMomentumScheduler\n\n\n\n\nBase type for all momentum scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizer\n \n \nType\n.\n\n\nAbstractOptimizer\n\n\n\n\nBase type for all optimizers.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizerOptions\n \n \nType\n.\n\n\nAbstractOptimizerOptions\n\n\n\n\nBase class for all optimizer options.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.OptimizationState\n \n \nType\n.\n\n\nOptimizationState\n\n\n\n\n.. attribute:: batch_size\n\n\n  The size of the mini-batch used in stochastic training.\n\n\n\n\n.. attribute:: curr_epoch\n\n\n  The current epoch count. Epoch 0 means no training yet, during the first\n  pass through the data, the epoch will be 1; during the second pass, the\n  epoch count will be 1, and so on.\n\n\n\n\n.. attribute:: curr_batch\n\n\n  The current mini-batch count. The batch count is reset during every epoch.\n  The batch count 0 means the beginning of each epoch, with no mini-batch\n  seen yet. During the first mini-batch, the mini-batch count will be 1.\n\n\n\n\n.. attribute:: curr_iter\n\n\n  The current iteration count. One iteration corresponds to one mini-batch,\n  but unlike the mini-batch count, the iteration count does **not** reset\n  in each epoch. So it track the *total* number of mini-batches seen so far.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_learning_rate\n \n \nFunction\n.\n\n\nget_learning_rate(scheduler, state)\n\n\n\n\nArguments\n\n\n\n\nAbstractLearningRateScheduler scheduler: a learning rate scheduler.\n\n\nOptimizationState state: the current state about epoch, mini-batch and iteration count.  :return: the current learning rate.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_momentum\n \n \nFunction\n.\n\n\nget_momentum(scheduler, state)\n\n\n\n\n\n\nAbstractMomentumScheduler scheduler: the momentum scheduler.\n\n\nOptimizationState state: the state about current epoch, mini-batch and iteration count.  :return: the current momentum.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_updater\n \n \nMethod\n.\n\n\nget_updater(optimizer)\n\n\n\n\n\n\n\n\nAbstractOptimizer optimizer: the underlying optimizer.\n\n\nA utility function to create an updater function, that uses its closure to  store all the states needed for each weights.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normalized_gradient\n \n \nMethod\n.\n\n\nnormalized_gradient(opts, state, grad)\n\n\n\n\n\n\nAbstractOptimizerOptions opts: options for the optimizer, should contain the field         $grad_scale$, $grad_clip$ and $weight_decay$.\n\n\nOptimizationState state: the current optimization state.\n\n\nNDArray weight: the trainable weights.\n\n\n\n\nNDArray grad: the original gradient of the weights.\n\n\nGet the properly normalized gradient (re-scaled and clipped if necessary).\n\n\n\n\n\n\nsource\n\n\n\n\nBuilt-in optimizers\n\n\n\n\nADAM\n\n\nStochastic Gradient Descent", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#optimizers", 
            "text": "#  MXNet.mx.AbstractLearningRateScheduler     Type .  AbstractLearningRateScheduler  Base type for all learning rate scheduler.  source  #  MXNet.mx.AbstractMomentumScheduler     Type .  AbstractMomentumScheduler  Base type for all momentum scheduler.  source  #  MXNet.mx.AbstractOptimizer     Type .  AbstractOptimizer  Base type for all optimizers.  source  #  MXNet.mx.AbstractOptimizerOptions     Type .  AbstractOptimizerOptions  Base class for all optimizer options.  source  #  MXNet.mx.OptimizationState     Type .  OptimizationState  .. attribute:: batch_size    The size of the mini-batch used in stochastic training.  .. attribute:: curr_epoch    The current epoch count. Epoch 0 means no training yet, during the first\n  pass through the data, the epoch will be 1; during the second pass, the\n  epoch count will be 1, and so on.  .. attribute:: curr_batch    The current mini-batch count. The batch count is reset during every epoch.\n  The batch count 0 means the beginning of each epoch, with no mini-batch\n  seen yet. During the first mini-batch, the mini-batch count will be 1.  .. attribute:: curr_iter    The current iteration count. One iteration corresponds to one mini-batch,\n  but unlike the mini-batch count, the iteration count does **not** reset\n  in each epoch. So it track the *total* number of mini-batches seen so far.  source  #  MXNet.mx.get_learning_rate     Function .  get_learning_rate(scheduler, state)  Arguments   AbstractLearningRateScheduler scheduler: a learning rate scheduler.  OptimizationState state: the current state about epoch, mini-batch and iteration count.  :return: the current learning rate.   source  #  MXNet.mx.get_momentum     Function .  get_momentum(scheduler, state)   AbstractMomentumScheduler scheduler: the momentum scheduler.  OptimizationState state: the state about current epoch, mini-batch and iteration count.  :return: the current momentum.   source  #  MXNet.mx.get_updater     Method .  get_updater(optimizer)    AbstractOptimizer optimizer: the underlying optimizer.  A utility function to create an updater function, that uses its closure to  store all the states needed for each weights.    source  #  MXNet.mx.normalized_gradient     Method .  normalized_gradient(opts, state, grad)   AbstractOptimizerOptions opts: options for the optimizer, should contain the field         $grad_scale$, $grad_clip$ and $weight_decay$.  OptimizationState state: the current optimization state.  NDArray weight: the trainable weights.   NDArray grad: the original gradient of the weights.  Get the properly normalized gradient (re-scaled and clipped if necessary).    source", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#built-in-optimizers", 
            "text": "ADAM  Stochastic Gradient Descent", 
            "title": "Built-in optimizers"
        }, 
        {
            "location": "/api/callback/", 
            "text": "Callback in training\n\n\n#\n\n\nMXNet.mx.AbstractBatchCallback\n \n \nType\n.\n\n\nAbstractBatchCallback\n\n\n\n\nAbstract type of callbacks to be called every mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractCallback\n \n \nType\n.\n\n\nAbstractCallback\n\n\n\n\nAbstract type of callback functions used in training.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEpochCallback\n \n \nType\n.\n\n\nAbstractEpochCallback\n\n\n\n\nAbstract type of callbacks to be called every epoch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.do_checkpoint\n \n \nMethod\n.\n\n\ndo_checkpoint(prefix; frequency=1, save_epoch_0=false)\n\n\n\n\nCreate an :class:\nAbstractEpochCallback\n that save checkpoints of the model to disk. The checkpoints can be loaded back later on.\n\n\nArguments\n\n\n\n\nprefix::AbstractString\n: the prefix of the filenames to save the model. The model         architecture will be saved to prefix-symbol.json, while the weights will be saved         to prefix-0012.params, for example, for the 12-th epoch.\n\n\nInt frequency: keyword argument, default 1. The frequency (measured in epochs) to         save checkpoints.\n\n\nBool save_epoch_0: keyword argument, default false. Whether we should save a         checkpoint for epoch 0 (model initialized but not seen any data yet).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_batch\n \n \nMethod\n.\n\n\nevery_n_batch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every $n$ mini-batches.\n\n\nArguments\n\n\n\n\ncall_on_0::Bool\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on batch 0.\n\n\n\n\nFor example, the :func:\nspeedometer\n callback is defined as\n\n\n.. code-block:: julia\n\n\n  every_n_iter(frequency, call_on_0=true) do state :: OptimizationState\n    if state.curr_batch == 0\n      # reset timer\n    else\n      # compute and print speed\n    end\n  end\n\n\n\n\n:seealso: :func:\nevery_n_epoch\n, :func:\nspeedometer\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_epoch\n \n \nMethod\n.\n\n\nevery_n_epoch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every $n$ full data-passes.\n\n\n\n\n\n\nInt call_on_0: keyword argument, default false. Unless set, the callback         will \nnot\n be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.\n\n\n:seealso: :func:\nevery_n_iter\n.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.speedometer\n \n \nMethod\n.\n\n\nspeedometer(; frequency=50)\n\n\n\n\nCreate an :class:\nAbstractBatchCallback\n that measure the training speed    (number of samples processed per second) every k mini-batches.\n\n\nArguments\n\n\n\n\nInt frequency: keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.\n\n\n\n\nsource", 
            "title": "Callbacks in training"
        }, 
        {
            "location": "/api/callback/#callback-in-training", 
            "text": "#  MXNet.mx.AbstractBatchCallback     Type .  AbstractBatchCallback  Abstract type of callbacks to be called every mini-batch.  source  #  MXNet.mx.AbstractCallback     Type .  AbstractCallback  Abstract type of callback functions used in training.  source  #  MXNet.mx.AbstractEpochCallback     Type .  AbstractEpochCallback  Abstract type of callbacks to be called every epoch.  source  #  MXNet.mx.do_checkpoint     Method .  do_checkpoint(prefix; frequency=1, save_epoch_0=false)  Create an :class: AbstractEpochCallback  that save checkpoints of the model to disk. The checkpoints can be loaded back later on.  Arguments   prefix::AbstractString : the prefix of the filenames to save the model. The model         architecture will be saved to prefix-symbol.json, while the weights will be saved         to prefix-0012.params, for example, for the 12-th epoch.  Int frequency: keyword argument, default 1. The frequency (measured in epochs) to         save checkpoints.  Bool save_epoch_0: keyword argument, default false. Whether we should save a         checkpoint for epoch 0 (model initialized but not seen any data yet).   source  #  MXNet.mx.every_n_batch     Method .  every_n_batch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every $n$ mini-batches.  Arguments   call_on_0::Bool : keyword argument, default false. Unless set, the callback         will  not  be run on batch 0.   For example, the :func: speedometer  callback is defined as  .. code-block:: julia    every_n_iter(frequency, call_on_0=true) do state :: OptimizationState\n    if state.curr_batch == 0\n      # reset timer\n    else\n      # compute and print speed\n    end\n  end  :seealso: :func: every_n_epoch , :func: speedometer .  source  #  MXNet.mx.every_n_epoch     Method .  every_n_epoch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every $n$ full data-passes.    Int call_on_0: keyword argument, default false. Unless set, the callback         will  not  be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.  :seealso: :func: every_n_iter .    source  #  MXNet.mx.speedometer     Method .  speedometer(; frequency=50)  Create an :class: AbstractBatchCallback  that measure the training speed    (number of samples processed per second) every k mini-batches.  Arguments   Int frequency: keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.   source", 
            "title": "Callback in training"
        }, 
        {
            "location": "/api/metric/", 
            "text": "Evaluation Metrics\n\n\nEvaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.\n\n\n#\n\n\nMXNet.mx.ACE\n \n \nType\n.\n\n\nACE\n\n\n\n\nAveraged cross-entropy for classification. This also know als logloss.\n\n\nCalculated the averaged cross entropy for multi-dimentions output.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEvalMetric\n \n \nType\n.\n\n\nAbstractEvalMetric\n\n\n\n\nThe base class for all evaluation metrics. The sub-types should implement the following interfaces.\n\n\n.. function:: update!(metric, labels, preds)\n\n\n  Update and accumulate metrics.\n\n  :param AbstractEvalMetric metric: the metric object.\n  :param labels: the labels from the data provider.\n  :type labels: Vector{NDArray}\n  :param preds: the outputs (predictions) of the network.\n  :type preds: Vector{NDArray}\n\n\n\n\n.. function:: reset!(metric)\n\n\n  Reset the accumulation counter.\n\n\n\n\n.. function:: get(metric)\n\n\n  Get the accumulated metrics.\n\n  :return: ``Vector{Tuple{Base.Symbol, Real}}``, a list of name-value pairs. For\n           example, ``[(:accuracy, 0.9)]``.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Accuracy\n \n \nType\n.\n\n\nAccuracy\n\n\n\n\nMulticlass classification accuracy.\n\n\nCalculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MSE\n \n \nType\n.\n\n\nMSE\n\n\n\n\nMean Squared Error. TODO: add support for multi-dimensional outputs.\n\n\nCalculates the mean squared error regression loss in one dimension.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiACE\n \n \nType\n.\n\n\nMultiACE\n\n\n\n\nAveraged cross-entropy for classification. This also know als logloss. This variant keeps track of the different losses per class.\n\n\nCalculated the averaged cross entropy for multi-dimentions output.\n\n\nsource", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/metric/#evaluation-metrics", 
            "text": "Evaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.  #  MXNet.mx.ACE     Type .  ACE  Averaged cross-entropy for classification. This also know als logloss.  Calculated the averaged cross entropy for multi-dimentions output.  source  #  MXNet.mx.AbstractEvalMetric     Type .  AbstractEvalMetric  The base class for all evaluation metrics. The sub-types should implement the following interfaces.  .. function:: update!(metric, labels, preds)    Update and accumulate metrics.\n\n  :param AbstractEvalMetric metric: the metric object.\n  :param labels: the labels from the data provider.\n  :type labels: Vector{NDArray}\n  :param preds: the outputs (predictions) of the network.\n  :type preds: Vector{NDArray}  .. function:: reset!(metric)    Reset the accumulation counter.  .. function:: get(metric)    Get the accumulated metrics.\n\n  :return: ``Vector{Tuple{Base.Symbol, Real}}``, a list of name-value pairs. For\n           example, ``[(:accuracy, 0.9)]``.  source  #  MXNet.mx.Accuracy     Type .  Accuracy  Multiclass classification accuracy.  Calculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.  source  #  MXNet.mx.MSE     Type .  MSE  Mean Squared Error. TODO: add support for multi-dimensional outputs.  Calculates the mean squared error regression loss in one dimension.  source  #  MXNet.mx.MultiACE     Type .  MultiACE  Averaged cross-entropy for classification. This also know als logloss. This variant keeps track of the different losses per class.  Calculated the averaged cross entropy for multi-dimentions output.  source", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/io/", 
            "text": "Data Providers\n\n\nData providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.\n\n\n#\n\n\nMXNet.mx.AbstractDataBatch\n \n \nType\n.\n\n\nAbstractDataBatch\n\n\n\n\nBase type for a data mini-batch. It should implement the following interfaces:\n\n\n   count_samples(provider, batch) -\n Int\n\n  :param AbstractDataBatch batch: the data batch object.\n  :return: the number of samples in this batch. This number should be greater than 0, but\n           less than or equal to the batch size. This is used to indicate at the end of\n           the data set, there might not be enough samples for a whole mini-batch.\n\n   get_data(provider, batch) -\n Vector{NDArray}\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :return: a vector of data in this batch, should be in the same order as declared in\n           :func:`provide_data() \nAbstractDataProvider.provide_data\n`.\n\n           The last dimension of each :class:`NDArray` should always match the batch_size, even when\n           :func:`count_samples` returns a value less than the batch size. In this case,\n           the data provider is free to pad the remaining contents with any value.\n\n   get_label(provider, batch) -\n Vector{NDArray}\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :return: a vector of labels in this batch. Similar to :func:`get_data`.\n\n\n\n\nThe following utility functions will be automatically defined.\n\n\n   get(provider, batch, name) -\n NDArray\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :param Base.Symbol name: the name of the data to get, should be one of the names\n         provided in either :func:`provide_data() \nAbstractDataProvider.provide_data\n`\n         or :func:`provide_label() \nAbstractDataProvider.provide_label\n`.\n  :return: the corresponding data array corresponding to that name.\n\n   load_data!(provider, batch, targets)\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :param targets: the targets to load data into.\n  :type targets: Vector{Vector{SlicedNDArray}}\n\n  The targets is a list of the same length as number of data provided by this provider.\n  Each element in the list is a list of :class:`SlicedNDArray`. This list described a\n  spliting scheme of this data batch into different slices, each slice is specified by\n  a slice-ndarray pair, where *slice* specify the range of samples in the mini-batch\n  that should be loaded into the corresponding *ndarray*.\n\n  This utility function is used in data parallelization, where a mini-batch is splited\n  and computed on several different devices.\n\n   load_label!(provider, batch, targets)\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :param targets: the targets to load label into.\n  :type targets: Vector{Vector{SlicedNDArray}}\n\n  The same as :func:`load_data!`, except that this is for loading labels.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractDataProvider\n \n \nType\n.\n\n\nAbstractDataProvider\n\n\n\n\nThe root type for all data provider. A data provider should implement the following interfaces:\n\n\n   get_batch_size(provider) -\n Int\n\n  :param AbstractDataProvider provider: the data provider.\n  :return: the mini-batch size of the provided data. All the provided data should have the\n           same mini-batch size (i.e. the last dimension).\n\n   provide_data(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n  :param AbstractDataProvider provider: the data provider.\n  :return: a vector of (name, shape) pairs describing the names of the data it provides, and\n           the corresponding shapes.\n\n   provide_label(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n  :param AbstractDataProvider provider: the data provider.\n  :return: a vector of (name, shape) pairs describing the names of the labels it provides, and\n           the corresponding shapes.\n\n\n\n\nThe difference between \ndata\n and \nlabel\n is that during    training stage, both \ndata\n and \nlabel\n will be feeded into the model, while during    prediction stage, only \ndata\n is loaded. Otherwise, they could be anything, with any names, and    of any shapes. The provided data and label names here should match the input names in a target    :class:\nSymbolicNode\n.\n\n\nA data provider should also implement the Julia iteration interface, in order to allow iterating    through the data set. The provider will be called in the following way:\n\n\n.. code-block:: julia\n\n\n  for batch in eachbatch(provider)\n    data = get_data(provider, batch)\n  end\n\n\n\n\nwhich will be translated by Julia compiler into\n\n\n.. code-block:: julia\n\n\n  state = Base.start(eachbatch(provider))\n  while !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\n  end\n\n\n\n\nBy default, :func:\neachbatch\n simply returns the provider itself, so the iterator interface    is implemented on the provider type itself. But the extra layer of abstraction allows us to    implement a data provider easily via a Julia $Task$ coroutine. See the    data provider defined in :doc:\nthe char-lstm example    \n/tutorial/char-lstm\n for an example of using coroutine to define data    providers.\n\n\nThe detailed interface functions for the iterator API is listed below:\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\n\n\n:param AbstractDataProvider provider: the data provider.    :return: the specific subtype representing a data batch. See :class:\nAbstractDataBatch\n.\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\n\n\n:param AbstractDataProvider provider: the data provider.\n\n\nThis function is always called before iterating into the dataset. It should initialize    the iterator, reset the index, and do data shuffling if needed.\n\n\nBase.done(provider, state) -\n Bool\n\n\n\n\n:param AbstractDataProvider provider: the data provider.    :param AbstractDataProviderState state: the state returned by :func:\nBase.start\n :func:\nBase.next\n.    :return: true if there is no more data to iterate in this dataset.\n\n\nBase.next(provider) -\n (AbstractDataBatch, AbstractDataProviderState)\n\n\n\n\n:param AbstractDataProvider provider: the data provider.    :return: the current data batch, and the state for the next iteration.\n\n\nNote sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that\n\n\n\n\n:func:\nBase.start\n will always be called, and called only once before the iteration starts.\n\n\n:func:\nBase.done\n will always be called at the beginning of every iteration and always be called once.\n\n\nIf :func:\nBase.done\n return true, the iteration will stop, until the next round, again, starting with a call to :func:\nBase.start\n.\n\n\n:func:\nBase.next\n will always be called only once in each iteration. It will always be called after one and only one call to :func:\nBase.done\n; but if :func:\nBase.done\n returns true, :func:\nBase.next\n will not be called.\n\n\n\n\nWith those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in :class:\nMXDataProvider\n for example.\n\n\n.. caution::\n\n\nPlease do not use the one data provider simultaneously in two different places, either in parallel,    or in a nested loop. For example, the behavior for the following code is undefined\n\n\n.. code-block:: julia\n\n\n  for batch in data\n    # updating the parameters\n\n    # now let's test the performance on the training set\n    for b2 in data\n      # ...\n    end\n  end\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractDataProviderState\n \n \nType\n.\n\n\nAbstractDataProviderState\n\n\n\n\nBase type for data provider states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ArrayDataProvider\n \n \nType\n.\n\n\nArrayDataProvider\n\n\n\n\nA convenient tool to iterate :class:\nNDArray\n or Julia $Array$.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ArrayDataProvider\n \n \nMethod\n.\n\n\nArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)\n\n\n\n\nConstruct a data provider from :class:\nNDArray\n or Julia Arrays.\n\n\n:param data: the data, could be\n\n\n      - a :class:`NDArray`, or a Julia Array. This is equivalent to ``:data =\n data``.\n      - a name-data pair, like ``:mydata =\n array``, where ``:mydata`` is the name of the data\n        and ``array`` is an :class:`NDArray` or a Julia Array.\n      - a list of name-data pairs.\n\n\n\n\n:param label: the same as the $data$ parameter. When this argument is omitted, the constructed           provider will provide no labels.    :param Int batch_size: the batch size, default is 0, which means treating the whole array as a           single mini-batch.    :param Bool shuffle: turn on if the data should be shuffled at every epoch.    :param Real data_padding: when the mini-batch goes beyond the dataset boundary, there might           be less samples to include than a mini-batch. This value specify a scalar to pad the           contents of all the missing data points.    :param Real label_padding: the same as $data_padding$, except for the labels.\n\n\nTODO: remove $data_padding$ and $label_padding$, and implement rollover that copies    the last or first several training samples to feed the padding.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.DataBatch\n \n \nType\n.\n\n\nDataBatch\n\n\n\n\nA basic subclass of :class:\nAbstractDataBatch\n, that implement the interface by    accessing member fields.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MXDataProvider\n \n \nType\n.\n\n\nMXDataProvider\n\n\n\n\nA data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SlicedNDArray\n \n \nType\n.\n\n\nSlicedNDArray\n\n\n\n\nA alias type of $Tuple{UnitRange{Int},NDArray}$.\n\n\nsource", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#data-providers", 
            "text": "Data providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.  #  MXNet.mx.AbstractDataBatch     Type .  AbstractDataBatch  Base type for a data mini-batch. It should implement the following interfaces:     count_samples(provider, batch) -  Int\n\n  :param AbstractDataBatch batch: the data batch object.\n  :return: the number of samples in this batch. This number should be greater than 0, but\n           less than or equal to the batch size. This is used to indicate at the end of\n           the data set, there might not be enough samples for a whole mini-batch.\n\n   get_data(provider, batch) -  Vector{NDArray}\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :return: a vector of data in this batch, should be in the same order as declared in\n           :func:`provide_data()  AbstractDataProvider.provide_data `.\n\n           The last dimension of each :class:`NDArray` should always match the batch_size, even when\n           :func:`count_samples` returns a value less than the batch size. In this case,\n           the data provider is free to pad the remaining contents with any value.\n\n   get_label(provider, batch) -  Vector{NDArray}\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :return: a vector of labels in this batch. Similar to :func:`get_data`.  The following utility functions will be automatically defined.     get(provider, batch, name) -  NDArray\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :param Base.Symbol name: the name of the data to get, should be one of the names\n         provided in either :func:`provide_data()  AbstractDataProvider.provide_data `\n         or :func:`provide_label()  AbstractDataProvider.provide_label `.\n  :return: the corresponding data array corresponding to that name.\n\n   load_data!(provider, batch, targets)\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :param targets: the targets to load data into.\n  :type targets: Vector{Vector{SlicedNDArray}}\n\n  The targets is a list of the same length as number of data provided by this provider.\n  Each element in the list is a list of :class:`SlicedNDArray`. This list described a\n  spliting scheme of this data batch into different slices, each slice is specified by\n  a slice-ndarray pair, where *slice* specify the range of samples in the mini-batch\n  that should be loaded into the corresponding *ndarray*.\n\n  This utility function is used in data parallelization, where a mini-batch is splited\n  and computed on several different devices.\n\n   load_label!(provider, batch, targets)\n\n  :param AbstractDataProvider provider: the data provider.\n  :param AbstractDataBatch batch: the data batch object.\n  :param targets: the targets to load label into.\n  :type targets: Vector{Vector{SlicedNDArray}}\n\n  The same as :func:`load_data!`, except that this is for loading labels.  source  #  MXNet.mx.AbstractDataProvider     Type .  AbstractDataProvider  The root type for all data provider. A data provider should implement the following interfaces:     get_batch_size(provider) -  Int\n\n  :param AbstractDataProvider provider: the data provider.\n  :return: the mini-batch size of the provided data. All the provided data should have the\n           same mini-batch size (i.e. the last dimension).\n\n   provide_data(provider) -  Vector{Tuple{Base.Symbol, Tuple}}\n\n  :param AbstractDataProvider provider: the data provider.\n  :return: a vector of (name, shape) pairs describing the names of the data it provides, and\n           the corresponding shapes.\n\n   provide_label(provider) -  Vector{Tuple{Base.Symbol, Tuple}}\n\n  :param AbstractDataProvider provider: the data provider.\n  :return: a vector of (name, shape) pairs describing the names of the labels it provides, and\n           the corresponding shapes.  The difference between  data  and  label  is that during    training stage, both  data  and  label  will be feeded into the model, while during    prediction stage, only  data  is loaded. Otherwise, they could be anything, with any names, and    of any shapes. The provided data and label names here should match the input names in a target    :class: SymbolicNode .  A data provider should also implement the Julia iteration interface, in order to allow iterating    through the data set. The provider will be called in the following way:  .. code-block:: julia    for batch in eachbatch(provider)\n    data = get_data(provider, batch)\n  end  which will be translated by Julia compiler into  .. code-block:: julia    state = Base.start(eachbatch(provider))\n  while !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\n  end  By default, :func: eachbatch  simply returns the provider itself, so the iterator interface    is implemented on the provider type itself. But the extra layer of abstraction allows us to    implement a data provider easily via a Julia $Task$ coroutine. See the    data provider defined in :doc: the char-lstm example     /tutorial/char-lstm  for an example of using coroutine to define data    providers.  The detailed interface functions for the iterator API is listed below:  Base.eltype(provider) -  AbstractDataBatch  :param AbstractDataProvider provider: the data provider.    :return: the specific subtype representing a data batch. See :class: AbstractDataBatch .  Base.start(provider) -  AbstractDataProviderState  :param AbstractDataProvider provider: the data provider.  This function is always called before iterating into the dataset. It should initialize    the iterator, reset the index, and do data shuffling if needed.  Base.done(provider, state) -  Bool  :param AbstractDataProvider provider: the data provider.    :param AbstractDataProviderState state: the state returned by :func: Base.start  :func: Base.next .    :return: true if there is no more data to iterate in this dataset.  Base.next(provider) -  (AbstractDataBatch, AbstractDataProviderState)  :param AbstractDataProvider provider: the data provider.    :return: the current data batch, and the state for the next iteration.  Note sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that   :func: Base.start  will always be called, and called only once before the iteration starts.  :func: Base.done  will always be called at the beginning of every iteration and always be called once.  If :func: Base.done  return true, the iteration will stop, until the next round, again, starting with a call to :func: Base.start .  :func: Base.next  will always be called only once in each iteration. It will always be called after one and only one call to :func: Base.done ; but if :func: Base.done  returns true, :func: Base.next  will not be called.   With those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in :class: MXDataProvider  for example.  .. caution::  Please do not use the one data provider simultaneously in two different places, either in parallel,    or in a nested loop. For example, the behavior for the following code is undefined  .. code-block:: julia    for batch in data\n    # updating the parameters\n\n    # now let's test the performance on the training set\n    for b2 in data\n      # ...\n    end\n  end  source  #  MXNet.mx.AbstractDataProviderState     Type .  AbstractDataProviderState  Base type for data provider states.  source  #  MXNet.mx.ArrayDataProvider     Type .  ArrayDataProvider  A convenient tool to iterate :class: NDArray  or Julia $Array$.  source  #  MXNet.mx.ArrayDataProvider     Method .  ArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)  Construct a data provider from :class: NDArray  or Julia Arrays.  :param data: the data, could be        - a :class:`NDArray`, or a Julia Array. This is equivalent to ``:data =  data``.\n      - a name-data pair, like ``:mydata =  array``, where ``:mydata`` is the name of the data\n        and ``array`` is an :class:`NDArray` or a Julia Array.\n      - a list of name-data pairs.  :param label: the same as the $data$ parameter. When this argument is omitted, the constructed           provider will provide no labels.    :param Int batch_size: the batch size, default is 0, which means treating the whole array as a           single mini-batch.    :param Bool shuffle: turn on if the data should be shuffled at every epoch.    :param Real data_padding: when the mini-batch goes beyond the dataset boundary, there might           be less samples to include than a mini-batch. This value specify a scalar to pad the           contents of all the missing data points.    :param Real label_padding: the same as $data_padding$, except for the labels.  TODO: remove $data_padding$ and $label_padding$, and implement rollover that copies    the last or first several training samples to feed the padding.  source  #  MXNet.mx.DataBatch     Type .  DataBatch  A basic subclass of :class: AbstractDataBatch , that implement the interface by    accessing member fields.  source  #  MXNet.mx.MXDataProvider     Type .  MXDataProvider  A data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.  source  #  MXNet.mx.SlicedNDArray     Type .  SlicedNDArray  A alias type of $Tuple{UnitRange{Int},NDArray}$.  source", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/ndarray/", 
            "text": "NDArray API\n\n\n#\n\n\nMXNet.mx.NDArray\n \n \nType\n.\n\n\nNDArray\n\n\n\n\nWrapper of the $NDArray$ type in $libmxnet$. This is the basic building block    of tensor-based computation.\n\n\n.. _ndarray-shape-note:\n\n\n.. note::\n\n\n  since C/C++ use row-major ordering for arrays while Julia follows a\n  column-major ordering. To keep things consistent, we keep the underlying data\n  in their original layout, but use *language-native* convention when we talk\n  about shapes. For example, a mini-batch of 100 MNIST images is a tensor of\n  C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory\n  have shape (28,28,1,100).\n\n\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nMethod\n.\n\n\n*(arg0, arg1)\n\n\n\n\nCurrently only multiplication a scalar with an :class:\nNDArray\n is implemented. Matrix multiplication is to be added soon.\n\n\nsource\n\n\n#\n\n\nBase.:+\n \n \nMethod\n.\n\n\n+(args...)\n.+(args...)\n\n\n\n\nSummation. Multiple arguments of either scalar or :class:\nNDArray\n could be added together. Note at least the first or second argument needs to be an :class:\nNDArray\n to avoid ambiguity of built-in summation.\n\n\nsource\n\n\n#\n\n\nBase.:-\n \n \nMethod\n.\n\n\n-(arg0, arg1)\n-(arg0)\n.-(arg0, arg1)\n\n\n\n\nSubtraction $arg0 - arg1$, of scalar types or :class:\nNDArray\n. Or create the negative of $arg0$.\n\n\nsource\n\n\n#\n\n\nBase.:.*\n \n \nMethod\n.\n\n\n.*(arg0, arg1)\n\n\n\n\nElementwise multiplication of $arg0$ and $arg$, could be either scalar or :class:\nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.:./\n \n \nMethod\n.\n\n\n./(arg0 :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise dividing an :class:\nNDArray\n by a scalar or another :class:\nNDArray\n of the same shape.\n\n\nsource\n\n\n#\n\n\nBase.:/\n \n \nMethod\n.\n\n\n/(arg0 :: NDArray, arg :: Real)\n\n\n\n\nDivide an :class:\nNDArray\n by a scalar. Matrix division (solving linear systems) is not implemented yet.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.norm\n \n \nFunction\n.\n\n\nTake L2 norm of the src.The result will be ndarray of shape (1,) on the same device.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.convert\n \n \nMethod\n.\n\n\nconvert(::Type{Array{T}}, arr :: NDArray)\n\n\n\n\nConvert an :class:\nNDArray\n into a Julia $Array$ of specific type. Data will be copied.\n\n\nsource\n\n\n#\n\n\nBase.copy!\n \n \nMethod\n.\n\n\n.. function::    copy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})\n\n\nCopy contents of $src$ into $dst$.\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\n.. function::    copy(arr :: NDArray)    copy(arr :: NDArray, ctx :: Context)    copy(arr :: Array, ctx :: Context)\n\n\nCreate a copy of an array. When no :class:\nContext\n is given, create a Julia $Array$.    Otherwise, create an :class:\nNDArray\n on the specified context.\n\n\nsource\n\n\n#\n\n\nBase.eltype\n \n \nMethod\n.\n\n\neltype(arr :: NDArray)\n\n\n\n\nGet the element type of an :class:\nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(arr :: NDArray, idx)\n\n\n\n\nShortcut for :func:\nslice\n. A typical use is to write\n\n\n.. code-block:: julia\n\n\n  arr[:] += 5\n\n\n\n\nwhich translates into\n\n\n.. code-block:: julia\n\n\n  arr[:] = arr[:] + 5\n\n\n\n\nwhich furthur translates into\n\n\n.. code-block:: julia\n\n\n  setindex!(getindex(arr, Colon()), 5, Colon())\n\n\n\n\n.. note::\n\n\n  The behavior is quite different from indexing into Julia's ``Array``. For example, ``arr[2:5]``\n  create a **copy** of the sub-array for Julia ``Array``, while for :class:`NDArray`, this is\n  a *slice* that shares the memory.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\nShortcut for \nslice\n. \nNOTE\n the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call \nslice\n, which shares the underlying memory.\n\n\nsource\n\n\n#\n\n\nBase.length\n \n \nMethod\n.\n\n\nlength(arr :: NDArray)\n\n\n\n\nGet the number of elements in an :class:\nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.ndims\n \n \nMethod\n.\n\n\nndims(arr :: NDArray)\n\n\n\n\nGet the number of dimensions of an :class:\nNDArray\n. Is equivalent to $length(size(arr))$.\n\n\nsource\n\n\n#\n\n\nBase.setindex!\n \n \nMethod\n.\n\n\nsetindex!(arr :: NDArray, val, idx)\n\n\n\n\nAssign values to an :class:\nNDArray\n. Elementwise assignment is not implemented, only the following    scenarios are supported\n\n\n\n\n$arr[:] = val$: whole array assignment, $val$ could be a scalar or an array (Julia $Array$ or :class:\nNDArray\n) of the same shape.\n\n\n$arr[start:stop] = val$: assignment to a \nslice\n, $val$ could be a scalar or an array of the same shape to the slice. See also :func:\nslice\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.size\n \n \nMethod\n.\n\n\n.. function::    size(arr :: NDArray)    size(arr :: NDArray, dim :: Int)\n\n\nGet the shape of an :class:\nNDArray\n. The shape is in Julia's column-major convention. See    also the :ref:\nnotes on NDArray shapes \nndarray-shape-note\n.\n\n\nsource\n\n\n#\n\n\nBase.slice\n \n \nMethod\n.\n\n\nslice(arr :: NDArray, start:stop)\n\n\n\n\nCreate a view into a sub-slice of an :class:\nNDArray\n. Note only slicing at the slowest    changing dimension is supported. In Julia's column-major perspective, this is the last    dimension. For example, given an :class:\nNDArray\n of shape (2,3,4), $slice(array, 2:3)$ will create    a :class:\nNDArray\n of shape (2,3,2), sharing the data with the original array. This operation is    used in data parallelization to split mini-batch into sub-batches for different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast\n \n \nFunction\n.\n\n\nBroadcast array in the given axis to the given size\n\n\n\n\nsrc::NDArray\n: source ndarray\n\n\naxis::int\n: axis to broadcast\n\n\nsize::int\n: size of broadcast\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copyto\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div\n \n \nFunction\n.\n\n\nMultiply lhs by rhs\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._get_ndarray_functions\n \n \nMethod\n.\n\n\nThe libxmnet APIs are automatically imported from $libmxnet.so$. The functions listed here operate on :class:\nNDArray\n objects. The arguments to the functions are typically ordered as\n\n\n.. code-block:: julia\n\n\nfunc_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)\n\n\nunless $NDARRAY_ARG_BEFORE_SCALAR$ is not set. In this case, the scalars are put before the input arguments:\n\n\n.. code-block:: julia\n\n\nfunc_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)\n\n\nIf $ACCEPT_EMPTY_MUTATE_TARGET$ is set. An overloaded function without the output arguments will also be defined:\n\n\n.. code-block:: julia\n\n\nfunc_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)\n\n\nUpon calling, the output arguments will be automatically initialized with empty NDArrays.\n\n\nThose functions always return the output arguments. If there is only one output (the typical situation), that object (:class:\nNDArray\n) is returned. Otherwise, a tuple containing all the outputs will be returned.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._imdecode\n \n \nFunction\n.\n\n\nDecode an image, clip to (x0, y0, x1, y1), substract mean, and write to buffer\n\n\n\n\nmean::NDArray\n: image mean\n\n\nindex::int\n: buffer position for output\n\n\nx0::int\n: x0\n\n\ny0::int\n: y0\n\n\nx1::int\n: x1\n\n\ny1::int\n: y1\n\n\nc::int\n: channel\n\n\nsize::int\n: length of str_img\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum\n \n \nFunction\n.\n\n\nElementwise max of lhs by rhs\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum\n \n \nFunction\n.\n\n\nElementwise min of lhs by rhs\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus\n \n \nFunction\n.\n\n\nMinus lhs and rhs\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul\n \n \nFunction\n.\n\n\nMultiply lhs and rhs\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._onehot_encode\n \n \nFunction\n.\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus\n \n \nFunction\n.\n\n\nAdd lhs and rhs\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power\n \n \nFunction\n.\n\n\nElementwise power(lhs, rhs)\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_gaussian\n \n \nFunction\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_uniform\n \n \nFunction\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rdiv_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rminus_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rpower_scalar\n \n \nFunction\n.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_normal\n \n \nFunction\n.\n\n\nSample a normal distribution\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_uniform\n \n \nFunction\n.\n\n\nSample a uniform distribution\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._set_value\n \n \nFunction\n.\n\n\n\n\nsrc::real_t\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.add_to!\n \n \nMethod\n.\n\n\nadd_to!(dst :: NDArray, args :: Union{Real, NDArray}...)\n\n\n\n\nAdd a bunch of arguments into $dst$. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax_channel\n \n \nFunction\n.\n\n\nTake argmax indices of each channel of the src.The result will be ndarray of shape (num_channel,) on the same device.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.choose_element_0index\n \n \nFunction\n.\n\n\nChoose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip\n \n \nFunction\n.\n\n\nClip ndarray elements to range (a_min, a_max)\n\n\n\n\nsrc::NDArray\n: Source input\n\n\na_min::real_t\n: Minimum value\n\n\na_max::real_t\n: Maximum value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.context\n \n \nMethod\n.\n\n\ncontext(arr :: NDArray)\n\n\n\n\nGet the context that this :class:\nNDArray\n lives on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.crop\n \n \nFunction\n.\n\n\nCrop the input matrix and return a new one\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.div_from!\n \n \nMethod\n.\n\n\ndiv_from!(dst :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise divide a scalar or an :class:\nNDArray\n of the same shape from $dst$. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\n.. function::    empty(shape :: Tuple, ctx :: Context)    empty(shape :: Tuple)    empty(dim1, dim2, ...)\n\n\nAllocate memory for an uninitialized :class:\nNDArray\n with specific shape of type Float32.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\n.. function::    empty(DType, shape :: Tuple, ctx :: Context)    empty(DType, shape :: Tuple)    empty(DType, dim1, dim2, ...)\n\n\nAllocate memory for an uninitialized :class:\nNDArray\n with a specified type.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill_element_0index\n \n \nFunction\n.\n\n\nFill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nmhs::NDArray\n: Middle operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flip\n \n \nFunction\n.\n\n\nFlip the input matrix along axis and return a new one\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::int, required\n: The dimension to flip\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.is_shared\n \n \nMethod\n.\n\n\nis_shared(j_arr, arr)\n\n\n\n\nTest whether $j_arr$ is sharing data with $arr$.\n\n\n\n\nArray j_arr: the Julia Array.\n\n\nNDArray arr: the :class:\nNDArray\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename, ::Type{NDArray})\n\n\n\n\nLoad NDArrays from binary file.\n\n\n\n\nAbstractString filename: the path of the file to load. It could be S3 or HDFS address.  :return: Either $Dict{Base.Symbol, NDArray}$ or $Vector{NDArray}$.\n\n\n\n\nIf the $libmxnet$ is built with the corresponding component enabled. Examples * $s3://my-bucket/path/my-s3-ndarray$ * $hdfs://my-bucket/path/my-hdfs-ndarray$ * $/path-to/my-local-ndarray$\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max\n \n \nFunction\n.\n\n\nTake max of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max_axis\n \n \nFunction\n.\n\n\n(Depreciated! Use max instead!) Take max of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min\n \n \nFunction\n.\n\n\nTake min of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min_axis\n \n \nFunction\n.\n\n\n(Depreciated! Use min instead!) Take min of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mul_to!\n \n \nMethod\n.\n\n\nmul_to!(dst :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise multiplication into $dst$ of either a scalar or an :class:\nNDArray\n of the same shape.    Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones(shape :: Tuple, ctx :: Context)\nones(shape :: Tuple)\nones(dim1, dim2, ...)\n\n\n\n\nCreate an :class:\nNDArray\n with specific shape and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones(DType, shape :: Tuple, ctx :: Context)\nones(DType, shape :: Tuple)\nones(DType, dim1, dim2, ...)\n\n\n\n\nCreate an :class:\nNDArray\n with specific shape \n type, and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, data)\n\n\n\n\nSave NDarrays to binary file. Filename could be S3 or HDFS address, if $libmxnet$ is built with corresponding support.\n\n\n\n\nAbstractString filename: path to the binary file to write to.\n\n\ndata: data to save to file.  :type data: :class:\nNDArray\n, or a $Vector{NDArray}$ or a $Dict{Base.Symbol, NDArray}$.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sub_from!\n \n \nMethod\n.\n\n\nsub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)\n\n\n\n\nSubtract a bunch of arguments from $dst$. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.try_get_shared\n \n \nMethod\n.\n\n\ntry_get_shared(arr)\n\n\n\n\nTry to create a Julia array by sharing the data with the underlying :class:\nNDArray\n.\n\n\n\n\n\n\nNDArray arr: the array to be shared.\n\n\n.. warning::\n\n\nThe returned array does not guarantee to share data with the underlying :class:`NDArray`.\nIn particular, data sharing is possible only when the :class:`NDArray` lives on CPU.\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros(shape :: Tuple, ctx :: Context)\nzeros(shape :: Tuple)\nzeros(dim1, dim2, ...)\n\n\n\n\nCreate zero-ed :class:\nNDArray\n with specific shape.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros(DType, shape :: Tuple, ctx :: Context)\nzeros(DType, shape :: Tuple)\nzeros(DType, dim1, dim2, ...)\n\n\n\n\nCreate zero-ed :class:\nNDArray\n with specific shape and type\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@inplace\n \n \nMacro\n.\n\n\n@inplace\n\n\n\n\nJulia does not support re-definiton of $+=$ operator (like $\niadd\n$ in python), When one write $a += b$, it gets translated to $a = a+b$. $a+b$ will allocate new memory for the results, and the newly allocated :class:\nNDArray\n object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.\n\n\nThis macro is a simple utility to implement this behavior. Write\n\n\n.. code-block:: julia\n\n\n  @mx.inplace a += b\n\n\n\n\nwill translate into\n\n\n.. code-block:: julia\n\n\n  mx.add_to!(a, b)\n\n\n\n\nwhich will do inplace adding of the contents of $b$ into $a$.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@nd_as_jl\n \n \nMacro\n.\n\n\nManipulating as Julia Arrays\n\n\n@nd_as_jl(captures..., statement)\n\n\n\n\nA convenient macro that allows to operate :class:\nNDArray\n as Julia Arrays. For example,\n\n\n.. code-block:: julia\n\n\n  x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end\n\n\n\n\nUnder the hood, the macro convert all the declared captures from :class:\nNDArray\n into Julia    Arrays, by using :func:\ntry_get_shared\n. And automatically commit the modifications back into    the :class:\nNDArray\n that is declared as $rw$. This is useful for fast prototyping and when    implement non-critical computations, such as :class:\nAbstractEvalMetric\n.\n\n\n.. note::\n\n\n  - Multiple ``rw`` and / or ``ro`` capture declaration could be made.\n  - The macro does **not** check to make sure that ``ro`` captures are not modified. If the\n    original :class:`NDArray` lives in CPU memory, then it is very likely the corresponding\n    Julia Array shares data with the :class:`NDArray`, so modifying the Julia Array will also\n    modify the underlying :class:`NDArray`.\n  - More importantly, since the :class:`NDArray` is\n    asynchronized, we will wait for *writing* for ``rw`` variables but wait only for *reading*\n    in ``ro`` variables. If we write into those ``ro`` variables, **and** if the memory is\n    shared, racing condition might happen, and the behavior is undefined.\n  - When an :class:`NDArray` is declared to be captured as ``rw``, its contents is always sync\n    back in the end.\n  - The execution results of the expanded macro is always ``nothing``.\n  - The statements are wrapped in a ``let``, thus locally introduced new variables will not be\n    available after the statements. So you will need to declare the variables before calling the\n    macro if needed.\n\n\n\n\nsource", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/ndarray/#ndarray-api", 
            "text": "#  MXNet.mx.NDArray     Type .  NDArray  Wrapper of the $NDArray$ type in $libmxnet$. This is the basic building block    of tensor-based computation.  .. _ndarray-shape-note:  .. note::    since C/C++ use row-major ordering for arrays while Julia follows a\n  column-major ordering. To keep things consistent, we keep the underlying data\n  in their original layout, but use *language-native* convention when we talk\n  about shapes. For example, a mini-batch of 100 MNIST images is a tensor of\n  C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory\n  have shape (28,28,1,100).  source  #  Base.:*     Method .  *(arg0, arg1)  Currently only multiplication a scalar with an :class: NDArray  is implemented. Matrix multiplication is to be added soon.  source  #  Base.:+     Method .  +(args...)\n.+(args...)  Summation. Multiple arguments of either scalar or :class: NDArray  could be added together. Note at least the first or second argument needs to be an :class: NDArray  to avoid ambiguity of built-in summation.  source  #  Base.:-     Method .  -(arg0, arg1)\n-(arg0)\n.-(arg0, arg1)  Subtraction $arg0 - arg1$, of scalar types or :class: NDArray . Or create the negative of $arg0$.  source  #  Base.:.*     Method .  .*(arg0, arg1)  Elementwise multiplication of $arg0$ and $arg$, could be either scalar or :class: NDArray .  source  #  Base.:./     Method .  ./(arg0 :: NDArray, arg :: Union{Real, NDArray})  Elementwise dividing an :class: NDArray  by a scalar or another :class: NDArray  of the same shape.  source  #  Base.:/     Method .  /(arg0 :: NDArray, arg :: Real)  Divide an :class: NDArray  by a scalar. Matrix division (solving linear systems) is not implemented yet.  source  #  Base.LinAlg.norm     Function .  Take L2 norm of the src.The result will be ndarray of shape (1,) on the same device.   src::NDArray : Source input to the function   source  #  Base.convert     Method .  convert(::Type{Array{T}}, arr :: NDArray)  Convert an :class: NDArray  into a Julia $Array$ of specific type. Data will be copied.  source  #  Base.copy!     Method .  .. function::    copy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})  Copy contents of $src$ into $dst$.  source  #  Base.copy     Method .  .. function::    copy(arr :: NDArray)    copy(arr :: NDArray, ctx :: Context)    copy(arr :: Array, ctx :: Context)  Create a copy of an array. When no :class: Context  is given, create a Julia $Array$.    Otherwise, create an :class: NDArray  on the specified context.  source  #  Base.eltype     Method .  eltype(arr :: NDArray)  Get the element type of an :class: NDArray .  source  #  Base.getindex     Method .  getindex(arr :: NDArray, idx)  Shortcut for :func: slice . A typical use is to write  .. code-block:: julia    arr[:] += 5  which translates into  .. code-block:: julia    arr[:] = arr[:] + 5  which furthur translates into  .. code-block:: julia    setindex!(getindex(arr, Colon()), 5, Colon())  .. note::    The behavior is quite different from indexing into Julia's ``Array``. For example, ``arr[2:5]``\n  create a **copy** of the sub-array for Julia ``Array``, while for :class:`NDArray`, this is\n  a *slice* that shares the memory.  source  #  Base.getindex     Method .  Shortcut for  slice .  NOTE  the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call  slice , which shares the underlying memory.  source  #  Base.length     Method .  length(arr :: NDArray)  Get the number of elements in an :class: NDArray .  source  #  Base.ndims     Method .  ndims(arr :: NDArray)  Get the number of dimensions of an :class: NDArray . Is equivalent to $length(size(arr))$.  source  #  Base.setindex!     Method .  setindex!(arr :: NDArray, val, idx)  Assign values to an :class: NDArray . Elementwise assignment is not implemented, only the following    scenarios are supported   $arr[:] = val$: whole array assignment, $val$ could be a scalar or an array (Julia $Array$ or :class: NDArray ) of the same shape.  $arr[start:stop] = val$: assignment to a  slice , $val$ could be a scalar or an array of the same shape to the slice. See also :func: slice .   source  #  Base.size     Method .  .. function::    size(arr :: NDArray)    size(arr :: NDArray, dim :: Int)  Get the shape of an :class: NDArray . The shape is in Julia's column-major convention. See    also the :ref: notes on NDArray shapes  ndarray-shape-note .  source  #  Base.slice     Method .  slice(arr :: NDArray, start:stop)  Create a view into a sub-slice of an :class: NDArray . Note only slicing at the slowest    changing dimension is supported. In Julia's column-major perspective, this is the last    dimension. For example, given an :class: NDArray  of shape (2,3,4), $slice(array, 2:3)$ will create    a :class: NDArray  of shape (2,3,2), sharing the data with the original array. This operation is    used in data parallelization to split mini-batch into sub-batches for different devices.  source  #  MXNet.mx._broadcast     Function .  Broadcast array in the given axis to the given size   src::NDArray : source ndarray  axis::int : axis to broadcast  size::int : size of broadcast   source  #  MXNet.mx._copyto     Function .   src::NDArray : Source input to the function.   source  #  MXNet.mx._div     Function .  Multiply lhs by rhs   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._div_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._get_ndarray_functions     Method .  The libxmnet APIs are automatically imported from $libmxnet.so$. The functions listed here operate on :class: NDArray  objects. The arguments to the functions are typically ordered as  .. code-block:: julia  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)  unless $NDARRAY_ARG_BEFORE_SCALAR$ is not set. In this case, the scalars are put before the input arguments:  .. code-block:: julia  func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)  If $ACCEPT_EMPTY_MUTATE_TARGET$ is set. An overloaded function without the output arguments will also be defined:  .. code-block:: julia  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)  Upon calling, the output arguments will be automatically initialized with empty NDArrays.  Those functions always return the output arguments. If there is only one output (the typical situation), that object (:class: NDArray ) is returned. Otherwise, a tuple containing all the outputs will be returned.  source  #  MXNet.mx._imdecode     Function .  Decode an image, clip to (x0, y0, x1, y1), substract mean, and write to buffer   mean::NDArray : image mean  index::int : buffer position for output  x0::int : x0  y0::int : y0  x1::int : x1  y1::int : y1  c::int : channel  size::int : length of str_img   source  #  MXNet.mx._maximum     Function .  Elementwise max of lhs by rhs   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._maximum_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._minimum     Function .  Elementwise min of lhs by rhs   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._minimum_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._minus     Function .  Minus lhs and rhs   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._minus_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._mul     Function .  Multiply lhs and rhs   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._mul_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._onehot_encode     Function .   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx._plus     Function .  Add lhs and rhs   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._plus_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._power     Function .  Elementwise power(lhs, rhs)   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._power_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._random_gaussian     Function .  source  #  MXNet.mx._random_uniform     Function .  source  #  MXNet.mx._rdiv_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._rminus_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._rpower_scalar     Function .   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._sample_normal     Function .  Sample a normal distribution   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), required : The shape of the output   source  #  MXNet.mx._sample_uniform     Function .  Sample a uniform distribution   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), required : The shape of the output   source  #  MXNet.mx._set_value     Function .   src::real_t : Source input to the function.   source  #  MXNet.mx.add_to!     Method .  add_to!(dst :: NDArray, args :: Union{Real, NDArray}...)  Add a bunch of arguments into $dst$. Inplace updating.  source  #  MXNet.mx.argmax_channel     Function .  Take argmax indices of each channel of the src.The result will be ndarray of shape (num_channel,) on the same device.   src::NDArray : Source input to the function   source  #  MXNet.mx.choose_element_0index     Function .  Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.clip     Function .  Clip ndarray elements to range (a_min, a_max)   src::NDArray : Source input  a_min::real_t : Minimum value  a_max::real_t : Maximum value   source  #  MXNet.mx.context     Method .  context(arr :: NDArray)  Get the context that this :class: NDArray  lives on.  source  #  MXNet.mx.crop     Function .  Crop the input matrix and return a new one   src::NDArray : Source input to the function  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates   source  #  MXNet.mx.div_from!     Method .  div_from!(dst :: NDArray, arg :: Union{Real, NDArray})  Elementwise divide a scalar or an :class: NDArray  of the same shape from $dst$. Inplace updating.  source  #  MXNet.mx.empty     Method .  .. function::    empty(shape :: Tuple, ctx :: Context)    empty(shape :: Tuple)    empty(dim1, dim2, ...)  Allocate memory for an uninitialized :class: NDArray  with specific shape of type Float32.  source  #  MXNet.mx.empty     Method .  .. function::    empty(DType, shape :: Tuple, ctx :: Context)    empty(DType, shape :: Tuple)    empty(DType, dim1, dim2, ...)  Allocate memory for an uninitialized :class: NDArray  with a specified type.  source  #  MXNet.mx.fill_element_0index     Function .  Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.   lhs::NDArray : Left operand to the function.  mhs::NDArray : Middle operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.flip     Function .  Flip the input matrix along axis and return a new one   src::NDArray : Source input to the function  axis::int, required : The dimension to flip   source  #  MXNet.mx.is_shared     Method .  is_shared(j_arr, arr)  Test whether $j_arr$ is sharing data with $arr$.   Array j_arr: the Julia Array.  NDArray arr: the :class: NDArray .   source  #  MXNet.mx.load     Method .  load(filename, ::Type{NDArray})  Load NDArrays from binary file.   AbstractString filename: the path of the file to load. It could be S3 or HDFS address.  :return: Either $Dict{Base.Symbol, NDArray}$ or $Vector{NDArray}$.   If the $libmxnet$ is built with the corresponding component enabled. Examples * $s3://my-bucket/path/my-s3-ndarray$ * $hdfs://my-bucket/path/my-hdfs-ndarray$ * $/path-to/my-local-ndarray$  source  #  MXNet.mx.max     Function .  Take max of the src in the given axis and returns a NDArray. Follows numpy semantics.   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.max_axis     Function .  (Depreciated! Use max instead!) Take max of the src in the given axis and returns a NDArray. Follows numpy semantics.   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.min     Function .  Take min of the src in the given axis and returns a NDArray. Follows numpy semantics.   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.min_axis     Function .  (Depreciated! Use min instead!) Take min of the src in the given axis and returns a NDArray. Follows numpy semantics.   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.mul_to!     Method .  mul_to!(dst :: NDArray, arg :: Union{Real, NDArray})  Elementwise multiplication into $dst$ of either a scalar or an :class: NDArray  of the same shape.    Inplace updating.  source  #  MXNet.mx.ones     Method .  ones(shape :: Tuple, ctx :: Context)\nones(shape :: Tuple)\nones(dim1, dim2, ...)  Create an :class: NDArray  with specific shape and initialize with 1.  source  #  MXNet.mx.ones     Method .  ones(DType, shape :: Tuple, ctx :: Context)\nones(DType, shape :: Tuple)\nones(DType, dim1, dim2, ...)  Create an :class: NDArray  with specific shape   type, and initialize with 1.  source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, data)  Save NDarrays to binary file. Filename could be S3 or HDFS address, if $libmxnet$ is built with corresponding support.   AbstractString filename: path to the binary file to write to.  data: data to save to file.  :type data: :class: NDArray , or a $Vector{NDArray}$ or a $Dict{Base.Symbol, NDArray}$.   source  #  MXNet.mx.sub_from!     Method .  sub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)  Subtract a bunch of arguments from $dst$. Inplace updating.  source  #  MXNet.mx.try_get_shared     Method .  try_get_shared(arr)  Try to create a Julia array by sharing the data with the underlying :class: NDArray .    NDArray arr: the array to be shared.  .. warning::  The returned array does not guarantee to share data with the underlying :class:`NDArray`.\nIn particular, data sharing is possible only when the :class:`NDArray` lives on CPU.    source  #  MXNet.mx.zeros     Method .  zeros(shape :: Tuple, ctx :: Context)\nzeros(shape :: Tuple)\nzeros(dim1, dim2, ...)  Create zero-ed :class: NDArray  with specific shape.  source  #  MXNet.mx.zeros     Method .  zeros(DType, shape :: Tuple, ctx :: Context)\nzeros(DType, shape :: Tuple)\nzeros(DType, dim1, dim2, ...)  Create zero-ed :class: NDArray  with specific shape and type  source  #  MXNet.mx.@inplace     Macro .  @inplace  Julia does not support re-definiton of $+=$ operator (like $ iadd $ in python), When one write $a += b$, it gets translated to $a = a+b$. $a+b$ will allocate new memory for the results, and the newly allocated :class: NDArray  object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.  This macro is a simple utility to implement this behavior. Write  .. code-block:: julia    @mx.inplace a += b  will translate into  .. code-block:: julia    mx.add_to!(a, b)  which will do inplace adding of the contents of $b$ into $a$.  source  #  MXNet.mx.@nd_as_jl     Macro .  Manipulating as Julia Arrays  @nd_as_jl(captures..., statement)  A convenient macro that allows to operate :class: NDArray  as Julia Arrays. For example,  .. code-block:: julia    x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end  Under the hood, the macro convert all the declared captures from :class: NDArray  into Julia    Arrays, by using :func: try_get_shared . And automatically commit the modifications back into    the :class: NDArray  that is declared as $rw$. This is useful for fast prototyping and when    implement non-critical computations, such as :class: AbstractEvalMetric .  .. note::    - Multiple ``rw`` and / or ``ro`` capture declaration could be made.\n  - The macro does **not** check to make sure that ``ro`` captures are not modified. If the\n    original :class:`NDArray` lives in CPU memory, then it is very likely the corresponding\n    Julia Array shares data with the :class:`NDArray`, so modifying the Julia Array will also\n    modify the underlying :class:`NDArray`.\n  - More importantly, since the :class:`NDArray` is\n    asynchronized, we will wait for *writing* for ``rw`` variables but wait only for *reading*\n    in ``ro`` variables. If we write into those ``ro`` variables, **and** if the memory is\n    shared, racing condition might happen, and the behavior is undefined.\n  - When an :class:`NDArray` is declared to be captured as ``rw``, its contents is always sync\n    back in the end.\n  - The execution results of the expanded macro is always ``nothing``.\n  - The statements are wrapped in a ``let``, thus locally introduced new variables will not be\n    available after the statements. So you will need to declare the variables before calling the\n    macro if needed.  source", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/symbolic-node/", 
            "text": "Symbolic API\n\n\n#\n\n\nMXNet.mx.SymbolicNode\n \n \nType\n.\n\n\nSymbolicNode\n\n\n\n\nSymbolicNode is the basic building block of the symbolic graph in MXNet.jl.\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nFunction\n.\n\n\nTake absolute value of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.call\n \n \nMethod\n.\n\n\ncall(self :: SymbolicNode, args :: SymbolicNode...)\ncall(self :: SymbolicNode; kwargs...)\n\n\n\n\nMake a new node by composing $self$ with $args$. Or the arguments can be specified using keyword arguments.\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nFunction\n.\n\n\nTake ceil value of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(self :: SymbolicNode)\n\n\n\n\nMake a copy of a SymbolicNode. The same as making a deep copy.\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nFunction\n.\n\n\nTake cos of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.deepcopy\n \n \nMethod\n.\n\n\ndeepcopy(self :: SymbolicNode)\n\n\n\n\nMake a deep copy of a SymbolicNode.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nFunction\n.\n\n\nTake exp of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nFunction\n.\n\n\nTake floor value of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})\n\n\n\n\nGet a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of :func:\nlist_outputs\n.\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nFunction\n.\n\n\nTake log of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nFunction\n.\n\n\nTake round value of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nFunction\n.\n\n\nTake sign value of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nFunction\n.\n\n\nTake sin of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nFunction\n.\n\n\nTake sqrt of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nFunction\n.\n\n\nTranspose the input matrix and return a new one\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxes::Shape(tuple), optional, default=()\n: Target axis order. By default the axes will be inverted.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Activation\n \n \nFunction\n.\n\n\nApply activation function to input.Softmax Activation is only available with CUDNN on GPUand will be computed at each location across channel if input is 4D.\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required\n: Activation function to be applied.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm\n \n \nFunction\n.\n\n\nApply batch normalization to input.\n\n\n\n\ndata::SymbolicNode\n: Input data to batch normalization\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent div 0\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=True\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=False\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BlockGrad\n \n \nFunction\n.\n\n\nGet output from a symbol and pass 0 gradient back\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Cast\n \n \nFunction\n.\n\n\nCast array to a different data type.\n\n\n\n\ndata::SymbolicNode\n: Input data to cast function.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Target data type.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Concat\n \n \nFunction\n.\n\n\nPerform an feature concat on channel dim (defaut is 1) over all\n\n\nThis function support variable length positional :class:\nSymbolicNode\n inputs.\n\n\n\n\ndata::SymbolicNode[]\n: List of tensors to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution\n \n \nFunction\n.\n\n\nApply convolution to input then add a bias.\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: convolution kernel size: (y, x) or (d, y, x)\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: convolution stride: (y, x) or (d, y, x)\n\n\ndilate::Shape(tuple), optional, default=(1,1)\n: convolution dilate: (y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for convolution: (y, x) or (d, y, x)\n\n\nnum_filter::int (non-negative), required\n: convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of groups partition. This option is not supported by CuDNN, you can use SliceChannel to num_group,apply convolution and concat instead to achieve the same need.\n\n\nworkspace::long (non-negative), optional, default=1024\n: Tmp workspace for convolution (MB).\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{'fastest', 'limited_workspace', 'off'},optional, default='off'\n: Whether to find convolution algo by running performance test.Leads to higher startup time but may give better speed.auto tune is turned off by default.Set environment varialbe MXNET_CUDNN_AUTOTUNE_DEFAULT=1 to turn on by default.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Correlation\n \n \nFunction\n.\n\n\nApply correlation to inputs\n\n\n\n\ndata1::SymbolicNode\n: Input data1 to the correlation.\n\n\ndata2::SymbolicNode\n: Input data2 to the correlation.\n\n\nkernel_size::int (non-negative), optional, default=1\n: kernel size for Correlation must be an odd number\n\n\nmax_displacement::int (non-negative), optional, default=1\n: Max displacement of Correlation\n\n\nstride1::int (non-negative), optional, default=1\n: stride1 quantize data1 globally\n\n\nstride2::int (non-negative), optional, default=1\n: stride2 quantize data2 within the neighborhood centered around data1\n\n\npad_size::int (non-negative), optional, default=0\n: pad for Correlation\n\n\nis_multiply::boolean, optional, default=True\n: operation type is either multiplication or subduction\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Crop\n \n \nFunction\n.\n\n\nCrop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used\n\n\nThis function support variable length positional :class:\nSymbolicNode\n inputs.\n\n\n\n\ndata::SymbolicNode or SymbolicNode[]\n: Tensor or List of Tensors, the second input will be used as crop_like shape reference\n\n\nnum_args::int, required\n: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here\n\n\noffset::Shape(tuple), optional, default=(0,0)\n: crop offset coordinate: (y, x)\n\n\nh_w::Shape(tuple), optional, default=(0,0)\n: crop height and weight: (h, w)\n\n\ncenter_crop::boolean, optional, default=False\n: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Custom\n \n \nFunction\n.\n\n\nCustom operator implemented in frontend.\n\n\n\n\nop_type::string\n: Type of custom operator. Must be registered first.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Deconvolution\n \n \nFunction\n.\n\n\nApply deconvolution to input then add a bias.\n\n\n\n\ndata::SymbolicNode\n: Input data to the DeconvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: deconvolution kernel size: (y, x)\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: deconvolution stride: (y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically\n\n\nadj::Shape(tuple), optional, default=(0,0)\n: adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape with targe shape : (y, x)\n\n\nnum_filter::int (non-negative), required\n: deconvolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: number of groups partition\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nno_bias::boolean, optional, default=True\n: Whether to disable bias parameter.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Dropout\n \n \nFunction\n.\n\n\nApply dropout to input\n\n\n\n\ndata::SymbolicNode\n: Input data to dropout.\n\n\np::float, optional, default=0.5\n: Fraction of the input that gets dropped out at training time\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ElementWiseSum\n \n \nFunction\n.\n\n\nPerform an elementwise sum over all the inputs.\n\n\nThis function support variable length positional :class:\nSymbolicNode\n inputs.\n\n\n\n\nnum_args::int, required\n: Number of inputs to be summed.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Embedding\n \n \nFunction\n.\n\n\nGet embedding for one-hot input. A n-dimensional input tensor will be trainsformed into a (n+1)-dimensional tensor, where a new dimension is added for the embedding results.\n\n\n\n\ndata::SymbolicNode\n: Input data to the EmbeddingOp.\n\n\nweight::SymbolicNode\n: Enbedding weight matrix.\n\n\ninput_dim::int, required\n: input dim of one-hot encoding\n\n\noutput_dim::int, required\n: output dim of embedding\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Flatten\n \n \nFunction\n.\n\n\nFlatten input\n\n\n\n\ndata::SymbolicNode\n: Input data to flatten.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FullyConnected\n \n \nFunction\n.\n\n\nApply matrix multiplication to input then add a bias.\n\n\n\n\ndata::SymbolicNode\n: Input data to the FullyConnectedOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nnum_hidden::int, required\n: Number of hidden nodes of the output.\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Group\n \n \nMethod\n.\n\n\nGroup(nodes :: SymbolicNode...)\n\n\n\n\nCreate a :class:\nSymbolicNode\n by grouping nodes together.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.IdentityAttachKLSparseReg\n \n \nFunction\n.\n\n\nApply a sparse regularization to the output a sigmoid activation function.\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\nsparseness_target::float, optional, default=0.1\n: The sparseness target\n\n\npenalty::float, optional, default=0.001\n: The tradeoff parameter for the sparseness penalty\n\n\nmomentum::float, optional, default=0.9\n: The momentum for running average\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.L2Normalization\n \n \nFunction\n.\n\n\nSet the l2 norm of each instance to a constant.\n\n\n\n\ndata::SymbolicNode\n: Input data to the L2NormalizationOp.\n\n\neps::float, optional, default=1e-10\n: Epsilon to prevent div 0\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LRN\n \n \nFunction\n.\n\n\nApply convolution to input then add a bias.\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nalpha::float, optional, default=0.0001\n: value of the alpha variance scaling parameter in the normalization formula\n\n\nbeta::float, optional, default=0.75\n: value of the beta power parameter in the normalization formula\n\n\nknorm::float, optional, default=2\n: value of the k parameter in normalization formula\n\n\nnsize::int (non-negative), required\n: normalization window width in elements.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LeakyReLU\n \n \nFunction\n.\n\n\nApply activation function to input.\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'\n: Activation function to be applied.\n\n\nslope::float, optional, default=0.25\n: Init slope for the activation. (For leaky and elu only)\n\n\nlower_bound::float, optional, default=0.125\n: Lower bound of random slope. (For rrelu only)\n\n\nupper_bound::float, optional, default=0.334\n: Upper bound of random slope. (For rrelu only)\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LinearRegressionOutput\n \n \nFunction\n.\n\n\nUse linear regression for final output, this is used on final output of a net.\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LogisticRegressionOutput\n \n \nFunction\n.\n\n\nUse Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MAERegressionOutput\n \n \nFunction\n.\n\n\nUse mean absolute error regression for final output, this is used on final output of a net.\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MakeLoss\n \n \nFunction\n.\n\n\nGet output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\ngrad_scale::float, optional, default=1\n: gradient scale as a supplement to unary and binary operators\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling\n \n \nFunction\n.\n\n\nPerform spatial pooling on inputs.\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=False\n: Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape\n\n\nkernel::Shape(tuple), required\n: pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: stride: for pooling (y, x) or (d, y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for pooling: (y, x) or (d, y, x)\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.RNN\n \n \nFunction\n.\n\n\nApply a recurrent layer to input.\n\n\n\n\ndata::SymbolicNode\n: Input data to RNN\n\n\nparameters::SymbolicNode\n: Vector of all RNN trainable parameters\n\n\nstate::SymbolicNode\n: initial hidden state of the RNN\n\n\nstate_cell::SymbolicNode\n: initial cell state for LSTM networks (only for LSTM)\n\n\nstate_size::int (non-negative), required\n: size of the state for each layer\n\n\nnum_layers::int (non-negative), required\n: number of stacked layers\n\n\nbidirectional::boolean, optional, default=False\n: whether to use bidirectional recurrent layers\n\n\nmode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required\n: the type of RNN to compute\n\n\np::float, optional, default=0\n: Fraction of the input that gets dropped out at training time\n\n\nstate_outputs::boolean, optional, default=False\n: Whether to have the states as symbol outputs.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ROIPooling\n \n \nFunction\n.\n\n\nPerforms region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\npooled_size::Shape(tuple), required\n: fix pooled size: (h, w)\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Reshape\n \n \nFunction\n.\n\n\nReshape input to target shape\n\n\n\n\ndata::SymbolicNode\n: Input data to reshape.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims\n\n\nkeep_highest::boolean, optional, default=False\n: (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to yes, than the first dim in target_shape is ignored,and always fixed as input\n\n\nshape::, optional, default=()\n: Target new shape. If the dim is same, set it to 0. If the dim is set to be -1, it will be inferred from the rest of dims. One and only one dim can be -1\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SVMOutput\n \n \nFunction\n.\n\n\nSupport Vector Machine based transformation on input, backprop L2-SVM\n\n\n\n\ndata::SymbolicNode\n: Input data to svm.\n\n\nlabel::SymbolicNode\n: Label data.\n\n\nmargin::float, optional, default=1\n: Scale the DType(param_.margin) for activation size\n\n\nregularization_coefficient::float, optional, default=1\n: Scale the coefficient responsible for balacing coefficient size and error tradeoff\n\n\nuse_linear::boolean, optional, default=False\n: If set true, uses L1-SVM objective function. Default uses L2-SVM objective\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SliceChannel\n \n \nFunction\n.\n\n\nSlice input equally along specified axis\n\n\n\n\nnum_outputs::int, required\n: Number of outputs to be sliced.\n\n\naxis::int, optional, default='1'\n: Dimension along which to slice.\n\n\nsqueeze_axis::boolean, optional, default=False\n: If true AND the sliced dimension becomes 1, squeeze that dimension.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode[].\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Softmax\n \n \nFunction\n.\n\n\nDEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxActivation\n \n \nFunction\n.\n\n\nApply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nmode::{'channel', 'instance'},optional, default='instance'\n: Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxOutput\n \n \nFunction\n.\n\n\nPerform a softmax transformation on input, backprop with logloss.\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\nlabel::SymbolicNode\n: Label data, can also be probability value with same shape as data\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SpatialTransformer\n \n \nFunction\n.\n\n\nApply spatial transformer to input feature map.\n\n\n\n\ndata::SymbolicNode\n: Input data to the SpatialTransformerOp.\n\n\nloc::SymbolicNode\n: localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape(h, w) of spatial transformer: (y, x)\n\n\ntransform_type::{'affine'}, required\n: transformation type\n\n\nsampler_type::{'bilinear'}, required\n: sampling type\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SwapAxis\n \n \nFunction\n.\n\n\nApply swapaxis to input.\n\n\n\n\ndata::SymbolicNode\n: Input data to the SwapAxisOp.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UpSampling\n \n \nFunction\n.\n\n\nPerform nearest neighboor/bilinear up sampling to inputs\n\n\nThis function support variable length positional :class:\nSymbolicNode\n inputs.\n\n\n\n\ndata::SymbolicNode[]\n: Array of tensors to upsample\n\n\nscale::int (non-negative), required\n: Up sampling scale\n\n\nnum_filter::int (non-negative), optional, default=0\n: Input filter. Only used by nearest sample_type.\n\n\nsample_type::{'bilinear', 'nearest'}, required\n: upsampling method\n\n\nmulti_input_mode::{'concat', 'sum'},optional, default='concat'\n: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n\n\nnum_args::int, required\n: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale\nh_0,scale\nw_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Variable\n \n \nMethod\n.\n\n\nVariable(name :: Union{Symbol, AbstractString})\n\n\n\n\nCreate a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.\n\n\nArguments\n\n\n\n\nDict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nVariable\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CrossDeviceCopy\n \n \nFunction\n.\n\n\nSpecial op to copy data cross device\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Div\n \n \nFunction\n.\n\n\nMultiply lhs by rhs\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._DivScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Maximum\n \n \nFunction\n.\n\n\nElementwise max of lhs by rhs\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MaximumScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minimum\n \n \nFunction\n.\n\n\nElementwise min of lhs by rhs\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinimumScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minus\n \n \nFunction\n.\n\n\nMinus lhs and rhs\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinusScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mul\n \n \nFunction\n.\n\n\nMultiply lhs and rhs\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MulScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NDArray\n \n \nFunction\n.\n\n\nStub for implementing an operator implemented in native frontend language with ndarray.\n\n\n\n\ninfo::, required\n:\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Native\n \n \nFunction\n.\n\n\nStub for implementing an operator implemented in native frontend language.\n\n\n\n\ninfo::, required\n:\n\n\nneed_top_grad::boolean, optional, default=True\n: Whether this layer needs out grad for backward. Should be false for loss layers.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: SymbolicNode.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Plus\n \n \nFunction\n.\n\n\nAdd lhs and rhs\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PlusScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Power\n \n \nFunction\n.\n\n\nElementwise power(lhs, rhs)\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PowerScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RDivScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RMinusScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RPowerScalar\n \n \nFunction\n.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nFunction\n.\n\n\nCalculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) \u2013\n (batch, M, N)\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nFunction\n.\n\n\nBroadcast data in the given axis to the given size. The original size of the broadcasting axis must be 1.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=()\n: Target sizes of the broadcasting axes.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nFunction\n.\n\n\nlhs divide rhs with broadcast\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nFunction\n.\n\n\nlhs minus rhs with broadcast\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nFunction\n.\n\n\nlhs multiple rhs with broadcast\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nFunction\n.\n\n\nlhs add rhs with broadcast\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nFunction\n.\n\n\nlhs power rhs with broadcast\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nFunction\n.\n\n\nBroadcast data to the target shape. The original size of the broadcasting axis must be 1.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.dot\n \n \nFunction\n.\n\n\nCalculate dot product of two matrices or two vectors\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nFunction\n.\n\n\nExpand the shape of array by inserting a new axis.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::int (non-negative), required\n: Position (amongst axes) where new axis is to be inserted.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.from_json\n \n \nMethod\n.\n\n\nfrom_json(repr :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a :class:\nSymbolicNode\n from a JSON string representation.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_attr\n \n \nMethod\n.\n\n\nget_attr(self :: SymbolicNode, key :: Symbol)\n\n\n\n\nGet attribute attached to this :class:\nSymbolicNode\n belonging to key. :return: The value belonging to key as a :class:\nNullable\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_internals\n \n \nMethod\n.\n\n\nget_internals(self :: SymbolicNode)\n\n\n\n\nGet a new grouped :class:\nSymbolicNode\n whose output contains all the internal outputs of this :class:\nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_shape\n \n \nMethod\n.\n\n\ninfer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)\n\n\n\n\nDo shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by :func:\nlist_arguments\n. Alternatively, the shape information could be specified via keyword arguments.\n\n\n:return: A 3-tuple containing shapes of all the arguments, shapes of all the outputs and          shapes of all the auxiliary variables. If shape inference failed due to incomplete          or incompatible inputs, the return value will be $(nothing, nothing, nothing)$.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_type\n \n \nMethod\n.\n\n\ninfer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)\n\n\n\n\nDo type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by :func:\nlist_arguments\n. Alternatively, the type information could be specified via keyword arguments.\n\n\n:return: A 3-tuple containing types of all the arguments, types of all the outputs and          types of all the auxiliary variables. If type inference failed due to incomplete          or incompatible inputs, the return value will be $(nothing, nothing, nothing)$.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_all_attr\n \n \nMethod\n.\n\n\nlist_all_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from the symbol graph. :return: Dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_arguments\n \n \nMethod\n.\n\n\nlist_arguments(self :: SymbolicNode)\n\n\n\n\nList all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a :class:\nFullyConnected\n node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.\n\n\n:return: A list of symbols indicating the names of the arguments.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_attr\n \n \nMethod\n.\n\n\nlist_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from a symbol. :return: Dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_auxiliary_states\n \n \nMethod\n.\n\n\nlist_auxiliary_states(self :: SymbolicNode)\n\n\n\n\nList all auxiliary states in the symbool.\n\n\nAuxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.\n\n\n:return: A list of symbols indicating the names of the auxiliary states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_outputs\n \n \nMethod\n.\n\n\nlist_outputs(self :: SymbolicNode)\n\n\n\n\nList all the outputs of this node.\n\n\n:return: A list of symbols indicating the names of the outputs.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a :class:\nSymbolicNode\n from a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normal\n \n \nFunction\n.\n\n\nSample a normal distribution\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nFunction\n.\n\n\nTake rsqrt of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, node :: SymbolicNode)\n\n\n\n\nSave a :class:\nSymbolicNode\n to a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.set_attr\n \n \nMethod\n.\n\n\nset_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)\n\n\n\n\nSet the attribute key to value for this :class:\nSymbolicNode\n.\n\n\nWarning\n\n\nIt is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the :class:\nSymbolicNode\n. Changing the attributes of a :class:\nSymbolicNode\n that is already been used somewhere else might cause unexpected behavior and inconsistency.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nFunction\n.\n\n\nSlice the input along certain axis and return a sliced array.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::int, required\n: The axis to be sliced\n\n\nbegin::int, required\n: The beginning index to be sliced\n\n\nend::int, required\n: The end index to be sliced\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nFunction\n.\n\n\nCalculate Smooth L1 Loss(lhs, scalar)\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nFunction\n.\n\n\nCalculate cross_entropy(lhs, one_hot(rhs))\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nFunction\n.\n\n\nTake square of the src\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum\n \n \nFunction\n.\n\n\nTake sum of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nFunction\n.\n\n\n(Depreciated! Use sum instead!) Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource\n\n\n#\n\n\nMXNet.mx.to_json\n \n \nMethod\n.\n\n\nto_json(self :: SymbolicNode)\n\n\n\n\nConvert a :class:\nSymbolicNode\n into a JSON string.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.uniform\n \n \nFunction\n.\n\n\nSample a uniform distribution\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\n\n\n:param Symbol name: The name of the :class:\nSymbolicNode\n. (e.g. \n:my_symbol\n), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class:\nSymbolicNode\n.\n\n\n:return: .\n\n\nsource", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/symbolic-node/#symbolic-api", 
            "text": "#  MXNet.mx.SymbolicNode     Type .  SymbolicNode  SymbolicNode is the basic building block of the symbolic graph in MXNet.jl.  source  #  Base.abs     Function .  Take absolute value of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.call     Method .  call(self :: SymbolicNode, args :: SymbolicNode...)\ncall(self :: SymbolicNode; kwargs...)  Make a new node by composing $self$ with $args$. Or the arguments can be specified using keyword arguments.  source  #  Base.ceil     Function .  Take ceil value of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.copy     Method .  copy(self :: SymbolicNode)  Make a copy of a SymbolicNode. The same as making a deep copy.  source  #  Base.cos     Function .  Take cos of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.deepcopy     Method .  deepcopy(self :: SymbolicNode)  Make a deep copy of a SymbolicNode.  source  #  Base.exp     Function .  Take exp of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.floor     Function .  Take floor value of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.getindex     Method .  getindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})  Get a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of :func: list_outputs .  source  #  Base.log     Function .  Take log of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.round     Function .  Take round value of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.sign     Function .  Take sign value of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.sin     Function .  Take sin of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.sqrt     Function .  Take sqrt of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  Base.transpose     Function .  Transpose the input matrix and return a new one   src::SymbolicNode : Left symbolic input to the function  axes::Shape(tuple), optional, default=() : Target axis order. By default the axes will be inverted.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.Activation     Function .  Apply activation function to input.Softmax Activation is only available with CUDNN on GPUand will be computed at each location across channel if input is 4D.   data::SymbolicNode : Input data to activation function.  act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required : Activation function to be applied.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.BatchNorm     Function .  Apply batch normalization to input.   data::SymbolicNode : Input data to batch normalization  eps::float, optional, default=0.001 : Epsilon to prevent div 0  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=True : Fix gamma while training  use_global_stats::boolean, optional, default=False : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.BlockGrad     Function .  Get output from a symbol and pass 0 gradient back   data::SymbolicNode : Input data.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Cast     Function .  Cast array to a different data type.   data::SymbolicNode : Input data to cast function.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Target data type.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Concat     Function .  Perform an feature concat on channel dim (defaut is 1) over all  This function support variable length positional :class: SymbolicNode  inputs.   data::SymbolicNode[] : List of tensors to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Convolution     Function .  Apply convolution to input then add a bias.   data::SymbolicNode : Input data to the ConvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : convolution kernel size: (y, x) or (d, y, x)  stride::Shape(tuple), optional, default=(1,1) : convolution stride: (y, x) or (d, y, x)  dilate::Shape(tuple), optional, default=(1,1) : convolution dilate: (y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for convolution: (y, x) or (d, y, x)  num_filter::int (non-negative), required : convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of groups partition. This option is not supported by CuDNN, you can use SliceChannel to num_group,apply convolution and concat instead to achieve the same need.  workspace::long (non-negative), optional, default=1024 : Tmp workspace for convolution (MB).  no_bias::boolean, optional, default=False : Whether to disable bias parameter.  cudnn_tune::{'fastest', 'limited_workspace', 'off'},optional, default='off' : Whether to find convolution algo by running performance test.Leads to higher startup time but may give better speed.auto tune is turned off by default.Set environment varialbe MXNET_CUDNN_AUTOTUNE_DEFAULT=1 to turn on by default.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Correlation     Function .  Apply correlation to inputs   data1::SymbolicNode : Input data1 to the correlation.  data2::SymbolicNode : Input data2 to the correlation.  kernel_size::int (non-negative), optional, default=1 : kernel size for Correlation must be an odd number  max_displacement::int (non-negative), optional, default=1 : Max displacement of Correlation  stride1::int (non-negative), optional, default=1 : stride1 quantize data1 globally  stride2::int (non-negative), optional, default=1 : stride2 quantize data2 within the neighborhood centered around data1  pad_size::int (non-negative), optional, default=0 : pad for Correlation  is_multiply::boolean, optional, default=True : operation type is either multiplication or subduction   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Crop     Function .  Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used  This function support variable length positional :class: SymbolicNode  inputs.   data::SymbolicNode or SymbolicNode[] : Tensor or List of Tensors, the second input will be used as crop_like shape reference  num_args::int, required : Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here  offset::Shape(tuple), optional, default=(0,0) : crop offset coordinate: (y, x)  h_w::Shape(tuple), optional, default=(0,0) : crop height and weight: (h, w)  center_crop::boolean, optional, default=False : If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Custom     Function .  Custom operator implemented in frontend.   op_type::string : Type of custom operator. Must be registered first.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Deconvolution     Function .  Apply deconvolution to input then add a bias.   data::SymbolicNode : Input data to the DeconvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : deconvolution kernel size: (y, x)  stride::Shape(tuple), optional, default=(1,1) : deconvolution stride: (y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically  adj::Shape(tuple), optional, default=(0,0) : adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically  target_shape::Shape(tuple), optional, default=(0,0) : output shape with targe shape : (y, x)  num_filter::int (non-negative), required : deconvolution filter(channel) number  num_group::int (non-negative), optional, default=1 : number of groups partition  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  no_bias::boolean, optional, default=True : Whether to disable bias parameter.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Dropout     Function .  Apply dropout to input   data::SymbolicNode : Input data to dropout.  p::float, optional, default=0.5 : Fraction of the input that gets dropped out at training time   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.ElementWiseSum     Function .  Perform an elementwise sum over all the inputs.  This function support variable length positional :class: SymbolicNode  inputs.   num_args::int, required : Number of inputs to be summed.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Embedding     Function .  Get embedding for one-hot input. A n-dimensional input tensor will be trainsformed into a (n+1)-dimensional tensor, where a new dimension is added for the embedding results.   data::SymbolicNode : Input data to the EmbeddingOp.  weight::SymbolicNode : Enbedding weight matrix.  input_dim::int, required : input dim of one-hot encoding  output_dim::int, required : output dim of embedding   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Flatten     Function .  Flatten input   data::SymbolicNode : Input data to flatten.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.FullyConnected     Function .  Apply matrix multiplication to input then add a bias.   data::SymbolicNode : Input data to the FullyConnectedOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  num_hidden::int, required : Number of hidden nodes of the output.  no_bias::boolean, optional, default=False : Whether to disable bias parameter.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Group     Method .  Group(nodes :: SymbolicNode...)  Create a :class: SymbolicNode  by grouping nodes together.  source  #  MXNet.mx.IdentityAttachKLSparseReg     Function .  Apply a sparse regularization to the output a sigmoid activation function.   data::SymbolicNode : Input data.  sparseness_target::float, optional, default=0.1 : The sparseness target  penalty::float, optional, default=0.001 : The tradeoff parameter for the sparseness penalty  momentum::float, optional, default=0.9 : The momentum for running average   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.L2Normalization     Function .  Set the l2 norm of each instance to a constant.   data::SymbolicNode : Input data to the L2NormalizationOp.  eps::float, optional, default=1e-10 : Epsilon to prevent div 0   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.LRN     Function .  Apply convolution to input then add a bias.   data::SymbolicNode : Input data to the ConvolutionOp.  alpha::float, optional, default=0.0001 : value of the alpha variance scaling parameter in the normalization formula  beta::float, optional, default=0.75 : value of the beta power parameter in the normalization formula  knorm::float, optional, default=2 : value of the k parameter in normalization formula  nsize::int (non-negative), required : normalization window width in elements.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.LeakyReLU     Function .  Apply activation function to input.   data::SymbolicNode : Input data to activation function.  act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky' : Activation function to be applied.  slope::float, optional, default=0.25 : Init slope for the activation. (For leaky and elu only)  lower_bound::float, optional, default=0.125 : Lower bound of random slope. (For rrelu only)  upper_bound::float, optional, default=0.334 : Upper bound of random slope. (For rrelu only)   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.LinearRegressionOutput     Function .  Use linear regression for final output, this is used on final output of a net.   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.LogisticRegressionOutput     Function .  Use Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.MAERegressionOutput     Function .  Use mean absolute error regression for final output, this is used on final output of a net.   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.MakeLoss     Function .  Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency   data::SymbolicNode : Input data.  grad_scale::float, optional, default=1 : gradient scale as a supplement to unary and binary operators   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Pooling     Function .  Perform spatial pooling on inputs.   data::SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=False : Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape  kernel::Shape(tuple), required : pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  stride::Shape(tuple), optional, default=(1,1) : stride: for pooling (y, x) or (d, y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for pooling: (y, x) or (d, y, x)   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.RNN     Function .  Apply a recurrent layer to input.   data::SymbolicNode : Input data to RNN  parameters::SymbolicNode : Vector of all RNN trainable parameters  state::SymbolicNode : initial hidden state of the RNN  state_cell::SymbolicNode : initial cell state for LSTM networks (only for LSTM)  state_size::int (non-negative), required : size of the state for each layer  num_layers::int (non-negative), required : number of stacked layers  bidirectional::boolean, optional, default=False : whether to use bidirectional recurrent layers  mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required : the type of RNN to compute  p::float, optional, default=0 : Fraction of the input that gets dropped out at training time  state_outputs::boolean, optional, default=False : Whether to have the states as symbol outputs.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.ROIPooling     Function .  Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  pooled_size::Shape(tuple), required : fix pooled size: (h, w)  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Reshape     Function .  Reshape input to target shape   data::SymbolicNode : Input data to reshape.  target_shape::Shape(tuple), optional, default=(0,0) : (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims  keep_highest::boolean, optional, default=False : (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to yes, than the first dim in target_shape is ignored,and always fixed as input  shape::, optional, default=() : Target new shape. If the dim is same, set it to 0. If the dim is set to be -1, it will be inferred from the rest of dims. One and only one dim can be -1   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.SVMOutput     Function .  Support Vector Machine based transformation on input, backprop L2-SVM   data::SymbolicNode : Input data to svm.  label::SymbolicNode : Label data.  margin::float, optional, default=1 : Scale the DType(param_.margin) for activation size  regularization_coefficient::float, optional, default=1 : Scale the coefficient responsible for balacing coefficient size and error tradeoff  use_linear::boolean, optional, default=False : If set true, uses L1-SVM objective function. Default uses L2-SVM objective   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.SliceChannel     Function .  Slice input equally along specified axis   num_outputs::int, required : Number of outputs to be sliced.  axis::int, optional, default='1' : Dimension along which to slice.  squeeze_axis::boolean, optional, default=False : If true AND the sliced dimension becomes 1, squeeze that dimension.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode[].  source  #  MXNet.mx.Softmax     Function .  DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput   data::SymbolicNode : Input data to softmax.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.SoftmaxActivation     Function .  Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.   data::SymbolicNode : Input data to activation function.  mode::{'channel', 'instance'},optional, default='instance' : Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.SoftmaxOutput     Function .  Perform a softmax transformation on input, backprop with logloss.   data::SymbolicNode : Input data to softmax.  label::SymbolicNode : Label data, can also be probability value with same shape as data  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.SpatialTransformer     Function .  Apply spatial transformer to input feature map.   data::SymbolicNode : Input data to the SpatialTransformerOp.  loc::SymbolicNode : localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.  target_shape::Shape(tuple), optional, default=(0,0) : output shape(h, w) of spatial transformer: (y, x)  transform_type::{'affine'}, required : transformation type  sampler_type::{'bilinear'}, required : sampling type   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.SwapAxis     Function .  Apply swapaxis to input.   data::SymbolicNode : Input data to the SwapAxisOp.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.UpSampling     Function .  Perform nearest neighboor/bilinear up sampling to inputs  This function support variable length positional :class: SymbolicNode  inputs.   data::SymbolicNode[] : Array of tensors to upsample  scale::int (non-negative), required : Up sampling scale  num_filter::int (non-negative), optional, default=0 : Input filter. Only used by nearest sample_type.  sample_type::{'bilinear', 'nearest'}, required : upsampling method  multi_input_mode::{'concat', 'sum'},optional, default='concat' : How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.  num_args::int, required : Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale h_0,scale w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx.Variable     Method .  Variable(name :: Union{Symbol, AbstractString})  Create a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.  Arguments   Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: Variable .   source  #  MXNet.mx._CrossDeviceCopy     Function .  Special op to copy data cross device  :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx._Div     Function .  Multiply lhs by rhs   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._DivScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._Maximum     Function .  Elementwise max of lhs by rhs   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._MaximumScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._Minimum     Function .  Elementwise min of lhs by rhs   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._MinimumScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._Minus     Function .  Minus lhs and rhs   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._MinusScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._Mul     Function .  Multiply lhs and rhs   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._MulScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._NDArray     Function .  Stub for implementing an operator implemented in native frontend language with ndarray.   info::, required :   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx._Native     Function .  Stub for implementing an operator implemented in native frontend language.   info::, required :  need_top_grad::boolean, optional, default=True : Whether this layer needs out grad for backward. Should be false for loss layers.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: SymbolicNode.  source  #  MXNet.mx._Plus     Function .  Add lhs and rhs   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._PlusScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._Power     Function .  Elementwise power(lhs, rhs)   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._PowerScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._RDivScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._RMinusScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx._RPowerScalar     Function .   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.batch_dot     Function .  Calculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) \u2013  (batch, M, N)   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_axis     Function .  Broadcast data in the given axis to the given size. The original size of the broadcasting axis must be 1.   src::SymbolicNode : Left symbolic input to the function  axis::Shape(tuple), optional, default=() : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=() : Target sizes of the broadcasting axes.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_div     Function .  lhs divide rhs with broadcast   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_minus     Function .  lhs minus rhs with broadcast   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_mul     Function .  lhs multiple rhs with broadcast   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_plus     Function .  lhs add rhs with broadcast   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_power     Function .  lhs power rhs with broadcast   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.broadcast_to     Function .  Broadcast data to the target shape. The original size of the broadcasting axis must be 1.   src::SymbolicNode : Left symbolic input to the function  shape::Shape(tuple), optional, default=() : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.dot     Function .  Calculate dot product of two matrices or two vectors   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.expand_dims     Function .  Expand the shape of array by inserting a new axis.   src::SymbolicNode : Left symbolic input to the function  axis::int (non-negative), required : Position (amongst axes) where new axis is to be inserted.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.from_json     Method .  from_json(repr :: AbstractString, ::Type{SymbolicNode})  Load a :class: SymbolicNode  from a JSON string representation.  source  #  MXNet.mx.get_attr     Method .  get_attr(self :: SymbolicNode, key :: Symbol)  Get attribute attached to this :class: SymbolicNode  belonging to key. :return: The value belonging to key as a :class: Nullable .  source  #  MXNet.mx.get_internals     Method .  get_internals(self :: SymbolicNode)  Get a new grouped :class: SymbolicNode  whose output contains all the internal outputs of this :class: SymbolicNode .  source  #  MXNet.mx.infer_shape     Method .  infer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)  Do shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by :func: list_arguments . Alternatively, the shape information could be specified via keyword arguments.  :return: A 3-tuple containing shapes of all the arguments, shapes of all the outputs and          shapes of all the auxiliary variables. If shape inference failed due to incomplete          or incompatible inputs, the return value will be $(nothing, nothing, nothing)$.  source  #  MXNet.mx.infer_type     Method .  infer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)  Do type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by :func: list_arguments . Alternatively, the type information could be specified via keyword arguments.  :return: A 3-tuple containing types of all the arguments, types of all the outputs and          types of all the auxiliary variables. If type inference failed due to incomplete          or incompatible inputs, the return value will be $(nothing, nothing, nothing)$.  source  #  MXNet.mx.list_all_attr     Method .  list_all_attr(self :: SymbolicNode)  Get all attributes from the symbol graph. :return: Dictionary of attributes.  source  #  MXNet.mx.list_arguments     Method .  list_arguments(self :: SymbolicNode)  List all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a :class: FullyConnected  node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.  :return: A list of symbols indicating the names of the arguments.  source  #  MXNet.mx.list_attr     Method .  list_attr(self :: SymbolicNode)  Get all attributes from a symbol. :return: Dictionary of attributes.  source  #  MXNet.mx.list_auxiliary_states     Method .  list_auxiliary_states(self :: SymbolicNode)  List all auxiliary states in the symbool.  Auxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.  :return: A list of symbols indicating the names of the auxiliary states.  source  #  MXNet.mx.list_outputs     Method .  list_outputs(self :: SymbolicNode)  List all the outputs of this node.  :return: A list of symbols indicating the names of the outputs.  source  #  MXNet.mx.load     Method .  load(filename :: AbstractString, ::Type{SymbolicNode})  Load a :class: SymbolicNode  from a JSON file.  source  #  MXNet.mx.normal     Function .  Sample a normal distribution   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), required : The shape of the output   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.rsqrt     Function .  Take rsqrt of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, node :: SymbolicNode)  Save a :class: SymbolicNode  to a JSON file.  source  #  MXNet.mx.set_attr     Method .  set_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)  Set the attribute key to value for this :class: SymbolicNode .  Warning  It is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the :class: SymbolicNode . Changing the attributes of a :class: SymbolicNode  that is already been used somewhere else might cause unexpected behavior and inconsistency.  source  #  MXNet.mx.slice_axis     Function .  Slice the input along certain axis and return a sliced array.   src::SymbolicNode : Left symbolic input to the function  axis::int, required : The axis to be sliced  begin::int, required : The beginning index to be sliced  end::int, required : The end index to be sliced   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.smooth_l1     Function .  Calculate Smooth L1 Loss(lhs, scalar)   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.softmax_cross_entropy     Function .  Calculate cross_entropy(lhs, one_hot(rhs))   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.square     Function .  Take square of the src   src::SymbolicNode : Left symbolic input to the function   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.sum     Function .  Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.   src::SymbolicNode : Left symbolic input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.sum_axis     Function .  (Depreciated! Use sum instead!) Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.   src::SymbolicNode : Left symbolic input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source  #  MXNet.mx.to_json     Method .  to_json(self :: SymbolicNode)  Convert a :class: SymbolicNode  into a JSON string.  source  #  MXNet.mx.uniform     Function .  Sample a uniform distribution   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), required : The shape of the output   :param Symbol name: The name of the :class: SymbolicNode . (e.g.  :my_symbol ), optional. :param Dict{Symbol, AbstractString} attrs: The attributes associated with this :class: SymbolicNode .  :return: .  source", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/nn-factory/", 
            "text": "Neural Network Factora\n\n\nNeural network factory provide convenient helper functions to define common neural networks.\n\n\n#\n\n\nMXNet.mx.MLP\n \n \nMethod\n.\n\n\nMLP(input, spec)\n\n\n\n\nConstruct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.\n\n\n\n\nSymbolicNode input: the input to the mlp.\n\n\nspec: the mlp specification, a list of hidden dimensions. For example,         $[128, (512, :sigmoid), 10]$. The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).\n\n\nBase.Symbol hidden_activation: keyword argument, default $:relu$, indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the $spec$ argument. Also activation is not         applied to the last, i.e. the prediction layer. See :func:\nActivation\n for a         list of supported activation types.\n\n\n\n\nprefix: keyword argument, default $gensym()$, used as the prefix to         name the constructed layers.\n\n\n:return: the constructed MLP.\n\n\n\n\n\n\nsource", 
            "title": "Neural Networks Factory"
        }, 
        {
            "location": "/api/nn-factory/#neural-network-factora", 
            "text": "Neural network factory provide convenient helper functions to define common neural networks.  #  MXNet.mx.MLP     Method .  MLP(input, spec)  Construct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.   SymbolicNode input: the input to the mlp.  spec: the mlp specification, a list of hidden dimensions. For example,         $[128, (512, :sigmoid), 10]$. The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).  Base.Symbol hidden_activation: keyword argument, default $:relu$, indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the $spec$ argument. Also activation is not         applied to the last, i.e. the prediction layer. See :func: Activation  for a         list of supported activation types.   prefix: keyword argument, default $gensym()$, used as the prefix to         name the constructed layers.  :return: the constructed MLP.    source", 
            "title": "Neural Network Factora"
        }, 
        {
            "location": "/api/executor/", 
            "text": "Executor\n\n\n#\n\n\nMXNet.mx.Executor\n \n \nType\n.\n\n\nExecutor\n\n\n\n\nAn executor is a realization of a symbolic architecture defined by a :class:\nSymbolicNode\n. The actual forward and backward computation specified by the network architecture can be carried out with an executor.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.GRAD_REQ\n \n \nType\n.\n\n\nbind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)\n\n\n\n\nCreate an :class:\nExecutor\n by binding a :class:\nSymbolicNode\n to concrete :class:\nNDArray\n.\n\n\nArguments\n\n\n\n\nsym::SymbolicNode\n: the network architecture describing the computation graph.\n\n\nctx::Context\n: the context on which the computation should run.\n\n\nargs\n: either a list of :class:\nNDArray\n or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels. See :func:\nlist_arguments\n         and :func:\ninfer_shape\n.\n\n\nargs_grad\n:\n\n\naux_states\n:\n\n\ngrad_req\n:\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.debug_str\n \n \nMethod\n.\n\n\nGet a debug string about internal execution plan.\n\n\nCan be used to get an estimated about the memory cost.\n\n\n  net = ... # Symbol\n  dProvider = ... # DataProvider\n  exec = mx.simple_bind(net, mx.cpu(), data=size(dProvider.data_batch[1]))\n  dbg_str = mx.debug_str(exec)\n  println(split(ref, ['\n'])[end-2])\n\n\n\n\nsource", 
            "title": "Executor"
        }, 
        {
            "location": "/api/executor/#executor", 
            "text": "#  MXNet.mx.Executor     Type .  Executor  An executor is a realization of a symbolic architecture defined by a :class: SymbolicNode . The actual forward and backward computation specified by the network architecture can be carried out with an executor.  source  #  MXNet.mx.GRAD_REQ     Type .  bind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)  Create an :class: Executor  by binding a :class: SymbolicNode  to concrete :class: NDArray .  Arguments   sym::SymbolicNode : the network architecture describing the computation graph.  ctx::Context : the context on which the computation should run.  args : either a list of :class: NDArray  or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels. See :func: list_arguments          and :func: infer_shape .  args_grad :  aux_states :  grad_req :   source  #  MXNet.mx.debug_str     Method .  Get a debug string about internal execution plan.  Can be used to get an estimated about the memory cost.    net = ... # Symbol\n  dProvider = ... # DataProvider\n  exec = mx.simple_bind(net, mx.cpu(), data=size(dProvider.data_batch[1]))\n  dbg_str = mx.debug_str(exec)\n  println(split(ref, ['\n'])[end-2])  source", 
            "title": "Executor"
        }, 
        {
            "location": "/api/visualize/", 
            "text": "Network Visualization\n\n\n#\n\n\nMXNet.mx.to_graphviz\n \n \nMethod\n.\n\n\nto_graphviz(network)\n\n\n\n\n\n\nSymbolicNode network: the network to visualize.\n\n\nAbstractString title: keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.\n\n\ninput_shapes: keyword argument, default $nothing$. If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.  :return: the graph description in GraphViz $dot$ language.\n\n\n\n\nsource", 
            "title": "Network Visualization"
        }, 
        {
            "location": "/api/visualize/#network-visualization", 
            "text": "#  MXNet.mx.to_graphviz     Method .  to_graphviz(network)   SymbolicNode network: the network to visualize.  AbstractString title: keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.  input_shapes: keyword argument, default $nothing$. If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.  :return: the graph description in GraphViz $dot$ language.   source", 
            "title": "Network Visualization"
        }
    ]
}