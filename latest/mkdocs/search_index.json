{
    "docs": [
        {
            "location": "/", 
            "text": "MXNet Documentation\n\n\nMXNet.jl\n is the \nJulia\n package of \ndmlc/mxnet\n. MXNet.jl brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:\n\n\n\n\nEfficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.\n\n\nFlexible symbolic manipulation to composite and construct state-of-the-art deep learning models.\n\n\n\n\nFor more details, see documentation below. Please also checkout the \nexamples\n directory.\n\n\n\n\nTutorials\n\n\n\n\nDigit Recognition on MNIST\n\n\nSimple 3-layer MLP\n\n\nConvolutional Neural Networks\n\n\nPredicting with a trained model\n\n\n\n\n\n\nGenerating Random Sentence with LSTM RNN\n\n\nLSTM Cells\n\n\nUnfolding LSTM\n\n\nData Provider for Text Sequences\n\n\nTraining the LSTM\n\n\nSampling Random Sentences\n\n\nVisualizing the LSTM\n\n\n\n\n\n\n\n\n\n\nUser's Guide\n\n\n\n\nInstallation Guide\n\n\nAutomatic Installation\n\n\nManual Compilation\n\n\n\n\n\n\nOverview\n\n\nMXNet.jl Namespace\n\n\nLow Level Interface\n\n\nIntermediate Level Interface\n\n\nHigh Level Interface\n\n\n\n\n\n\nFAQ\n\n\nRunning MXNet on AWS GPU instances\n\n\n\n\n\n\n\n\n\n\nAPI Documentation\n\n\n\n\nContext\n\n\nModel\n\n\nEvaluation Metrics\n\n\nData Providers\n\n\nAbstractDataProvider interface\n\n\nAbstractDataBatch interface\n\n\nImplemented providers and other methods\n\n\n\n\n\n\nNDArray API\n\n\nSymbolic API\n\n\nNeural Network Factory\n\n\nExecutor\n\n\nNetwork Visualization", 
            "title": "Home"
        }, 
        {
            "location": "/#mxnet-documentation", 
            "text": "MXNet.jl  is the  Julia  package of  dmlc/mxnet . MXNet.jl brings flexible and efficient GPU computing and state-of-art deep learning to Julia. Some highlight of features include:   Efficient tensor/matrix computation across multiple devices, including multiple CPUs, GPUs and distributed server nodes.  Flexible symbolic manipulation to composite and construct state-of-the-art deep learning models.   For more details, see documentation below. Please also checkout the  examples  directory.", 
            "title": "MXNet Documentation"
        }, 
        {
            "location": "/#tutorials", 
            "text": "Digit Recognition on MNIST  Simple 3-layer MLP  Convolutional Neural Networks  Predicting with a trained model    Generating Random Sentence with LSTM RNN  LSTM Cells  Unfolding LSTM  Data Provider for Text Sequences  Training the LSTM  Sampling Random Sentences  Visualizing the LSTM", 
            "title": "Tutorials"
        }, 
        {
            "location": "/#users-guide", 
            "text": "Installation Guide  Automatic Installation  Manual Compilation    Overview  MXNet.jl Namespace  Low Level Interface  Intermediate Level Interface  High Level Interface    FAQ  Running MXNet on AWS GPU instances", 
            "title": "User's Guide"
        }, 
        {
            "location": "/#api-documentation", 
            "text": "Context  Model  Evaluation Metrics  Data Providers  AbstractDataProvider interface  AbstractDataBatch interface  Implemented providers and other methods    NDArray API  Symbolic API  Neural Network Factory  Executor  Network Visualization", 
            "title": "API Documentation"
        }, 
        {
            "location": "/tutorial/mnist/", 
            "text": "Digit Recognition on MNIST\n\n\nIn this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the \nMNIST handwritten digit dataset\n. The code for this tutorial could be found in \nexamples/mnist\n.\n\n\n\n\nSimple 3-layer MLP\n\n\nThis is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with\n\n\nusing MXNet\n\n\n\n\nto load the \nMXNet\n module. Then we are ready to define the network architecture via the \nsymbolic API\n. We start with a placeholder \ndata\n symbol,\n\n\ndata = mx.Variable(:data)\n\n\n\n\nand then cascading fully-connected layers and activation functions:\n\n\nfc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)\n\n\n\n\nNote each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like\n\n\nInput --\n 128 units (ReLU) --\n 64 units (ReLU) --\n 10 units\n\n\n\n\nwhere the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final \nSoftmaxOutput\n operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:\n\n\nmlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)\n\n\n\n\nAs we can see, the MLP is just a chain of layers. For this case, we can also use the \nmx.chain\n macro. The same architecture above can be defined as\n\n\nmlp = @mx.chain mx.Variable(:data)             =\n\n  mx.FullyConnected(name=:fc1, num_hidden=128) =\n\n  mx.Activation(name=:relu1, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc2, num_hidden=64)  =\n\n  mx.Activation(name=:relu2, act_type=:relu)   =\n\n  mx.FullyConnected(name=:fc3, num_hidden=10)  =\n\n  mx.SoftmaxOutput(name=:softmax)\n\n\n\n\nAfter defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into \nPkg.dir(\"MXNet\")/data/mnist\n if necessary. We wrap the code to construct the data provider into \nmnist-data.jl\n so that it could be shared by both the MLP example and the LeNet ConvNets example.\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size)\n\n\n\n\nIf you need to write your own data providers for customized data format, please refer to \nmx.AbstractDataProvider\n.\n\n\nGiven the architecture and data, we can instantiate an \nmodel\n to do the actual training. \nmx.FeedForward\n is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the \ncontext\n on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.\n\n\nmodel = mx.FeedForward(mlp, context=mx.cpu())\n\n\n\n\nYou can use a \nmx.gpu()\n or if a list of devices (e.g. \n[mx.gpu(0), mx.gpu(1)]\n) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.\n\n\nThe last thing we need to specify is the optimization algorithm (a.k.a. \noptimizer\n) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:\n\n\noptimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)\n\n\n\n\nNow we can do the training. Here the \nn_epoch\n parameter specifies that we want to train for 20 epochs. We also supply a \neval_data\n to monitor validation accuracy on the validation set.\n\n\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nHere is a sample output\n\n\nINFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\nIn the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:\n\n\n# input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) =\n\n                  mx.Activation(act_type=:tanh) =\n\n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n\n\n\nWe basically defined two convolution modules. Each convolution module is actually a chain of \nConvolution\n, \ntanh\n activation and then max \nPooling\n operations.\n\n\nEach sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by \nNDArray\n, a batch of 100 samples is a tensor of shape \n(28,28,1,100)\n. The convolution and pooling operates in the spatial axis, so \nkernel=(5,5)\n indicate a square region of 5-width and 5-height. The rest of the architecture follows as:\n\n\n# first fully-connected\nfc1   = @mx.chain mx.Flatten(data=conv2) =\n\n                  mx.FullyConnected(num_hidden=500) =\n\n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(data=fc2, name=:softmax)\n\n\n\n\nNote a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a \nFlatten\n operator to flat the tensor, before connecting it to the \nFullyConnected\n operator.\n\n\nThe rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:\n\n\nbatch_size = 100\ninclude(\nmnist-data.jl\n)\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)\n\n\n\n\nNote we specified \nflat=false\n to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.\n\n\n# fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)\n\n\n\n\nAnd here is a sample of running outputs:\n\n\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915\n\n\n\n\n\n\nPredicting with a trained model\n\n\nPredicting with a trained model is very simple. By calling \nmx.predict\n with the model and a data provider, we get the model output as a Julia Array:\n\n\nprobs = mx.predict(model, eval_provider)\n\n\n\n\nThe following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:\n\n\n# collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format(\nAccuracy on eval set: {1:.2f}%\n, 100correct/length(labels)))\n\n\n\n\nAlternatively, when the dataset is huge, one can provide a callback to \nmx.predict\n, then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from \nmx.predict\n. See also predict.", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#digit-recognition-on-mnist", 
            "text": "In this tutorial, we will work through examples of training a simple multi-layer perceptron and then a convolutional neural network (the LeNet architecture) on the  MNIST handwritten digit dataset . The code for this tutorial could be found in  examples/mnist .", 
            "title": "Digit Recognition on MNIST"
        }, 
        {
            "location": "/tutorial/mnist/#simple-3-layer-mlp", 
            "text": "This is a tiny 3-layer MLP that could be easily trained on CPU. The script starts with  using MXNet  to load the  MXNet  module. Then we are ready to define the network architecture via the  symbolic API . We start with a placeholder  data  symbol,  data = mx.Variable(:data)  and then cascading fully-connected layers and activation functions:  fc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\nact1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\nfc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\nact2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\nfc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)  Note each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like  Input --  128 units (ReLU) --  64 units (ReLU) --  10 units  where the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final  SoftmaxOutput  operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:  mlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)  As we can see, the MLP is just a chain of layers. For this case, we can also use the  mx.chain  macro. The same architecture above can be defined as  mlp = @mx.chain mx.Variable(:data)             = \n  mx.FullyConnected(name=:fc1, num_hidden=128) = \n  mx.Activation(name=:relu1, act_type=:relu)   = \n  mx.FullyConnected(name=:fc2, num_hidden=64)  = \n  mx.Activation(name=:relu2, act_type=:relu)   = \n  mx.FullyConnected(name=:fc3, num_hidden=10)  = \n  mx.SoftmaxOutput(name=:softmax)  After defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into  Pkg.dir(\"MXNet\")/data/mnist  if necessary. We wrap the code to construct the data provider into  mnist-data.jl  so that it could be shared by both the MLP example and the LeNet ConvNets example.  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size)  If you need to write your own data providers for customized data format, please refer to  mx.AbstractDataProvider .  Given the architecture and data, we can instantiate an  model  to do the actual training.  mx.FeedForward  is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the  context  on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device.  model = mx.FeedForward(mlp, context=mx.cpu())  You can use a  mx.gpu()  or if a list of devices (e.g.  [mx.gpu(0), mx.gpu(1)] ) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.  The last thing we need to specify is the optimization algorithm (a.k.a.  optimizer ) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:  optimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)  Now we can do the training. Here the  n_epoch  parameter specifies that we want to train for 20 epochs. We also supply a  eval_data  to monitor validation accuracy on the validation set.  mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  Here is a sample output  INFO: Start training on [CPU0]\nINFO: Initializing parameters...\nINFO: Creating KVStore...\nINFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.7554\nINFO:            time = 1.3165 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9502\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.9949\nINFO:            time = 0.9287 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9775", 
            "title": "Simple 3-layer MLP"
        }, 
        {
            "location": "/tutorial/mnist/#convolutional-neural-networks", 
            "text": "In the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:  # input\ndata = mx.Variable(:data)\n\n# first conv\nconv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n\n# second conv\nconv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) = \n                  mx.Activation(act_type=:tanh) = \n                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))  We basically defined two convolution modules. Each convolution module is actually a chain of  Convolution ,  tanh  activation and then max  Pooling  operations.  Each sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by  NDArray , a batch of 100 samples is a tensor of shape  (28,28,1,100) . The convolution and pooling operates in the spatial axis, so  kernel=(5,5)  indicate a square region of 5-width and 5-height. The rest of the architecture follows as:  # first fully-connected\nfc1   = @mx.chain mx.Flatten(data=conv2) = \n                  mx.FullyConnected(num_hidden=500) = \n                  mx.Activation(act_type=:tanh)\n\n# second fully-connected\nfc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n\n# softmax loss\nlenet = mx.Softmax(data=fc2, name=:softmax)  Note a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a  Flatten  operator to flat the tensor, before connecting it to the  FullyConnected  operator.  The rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:  batch_size = 100\ninclude( mnist-data.jl )\ntrain_provider, eval_provider = get_mnist_providers(batch_size; flat=false)  Note we specified  flat=false  to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it.  # fit model\nmodel = mx.FeedForward(lenet, context=mx.gpu())\n\n# optimizer\noptimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n\n# fit parameters\nmx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)  And here is a sample of running outputs:  INFO: == Epoch 001 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 0.6750\nINFO:            time = 4.9814 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9712\n...\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:       :accuracy = 1.0000\nINFO:            time = 4.0086 seconds\nINFO: ## Validation summary\nINFO:       :accuracy = 0.9915", 
            "title": "Convolutional Neural Networks"
        }, 
        {
            "location": "/tutorial/mnist/#predicting-with-a-trained-model", 
            "text": "Predicting with a trained model is very simple. By calling  mx.predict  with the model and a data provider, we get the model output as a Julia Array:  probs = mx.predict(model, eval_provider)  The following code shows a stupid way of getting all the labels from the data provider, and compute the prediction accuracy manually:  # collect all labels from eval data\nlabels = Array[]\nfor batch in eval_provider\n  push!(labels, copy(mx.get_label(batch)))\nend\nlabels = cat(1, labels...)\n\n# Now we use compute the accuracy\ncorrect = 0\nfor i = 1:length(labels)\n  # labels are 0...9\n  if indmax(probs[:,i]) == labels[i]+1\n    correct += 1\n  end\nend\nprintln(mx.format( Accuracy on eval set: {1:.2f}% , 100correct/length(labels)))  Alternatively, when the dataset is huge, one can provide a callback to  mx.predict , then the callback function will be invoked with the outputs of each mini-batch. The callback could, for example, write the data to disk for future inspection. In this case, no value is returned from  mx.predict . See also predict.", 
            "title": "Predicting with a trained model"
        }, 
        {
            "location": "/tutorial/char-lstm/", 
            "text": "Generating Random Sentence with LSTM RNN\n\n\nThis tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called \nchar-rnn\n is described in \nAndrej Karpathy's blog\n, with a reference implementation in Torch available \nhere\n.\n\n\nBecause MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the \nchar-rnn example for MXNet's Python binding\n, which demonstrates how to use low-level \nSymbolic API\n to build customized neural network models directly.\n\n\nThe most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the \nexamples/char-lstm\n directory. You will need to install \nIterators.jl\n and \nStatsBase.jl\n to run this example.\n\n\n\n\nLSTM Cells\n\n\nChristopher Olah has a \ngreat blog post about LSTM\n with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input \nx\n, as well as previous states (including \nc\n and \nh\n), and produce the next states. We define a helper type to bundle the two state variables together:\n\n\nBecause LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.\n\n\nNote all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.\n\n\nThe following figure is stolen (permission requested) from \nChristopher Olah's blog\n, which illustrate exactly what the code snippet above is doing.\n\n\n\n\nIn particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.\n\n\n\n\nUnfolding LSTM\n\n\nUsing the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.\n\n\nThe \nembed_W\n is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The \npred_W\n and \npred_b\n are weights and bias for the final prediction at each time step.\n\n\nThen we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however, \nnot\n shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.\n\n\nUnrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as \n:ptb\n, the data and label at step \nt\n will be named \n:ptb_data_$t\n and \n:ptb_label_$t\n. Late on when we prepare the data, we will define the data provider to match those names.\n\n\nNote at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.\n\n\nIn the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.\n\n\n\n\nData Provider for Text Sequences\n\n\nNow we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.\n\n\nNote the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.\n\n\nThe text sequence data provider implements the \nData Providers\n api. We define the \nCharSeqProvider\n as below:\n\n\nThe provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from \n$name_data_$t\n and \n$name_label_$t\n, we also provides the initial \nc\n and \nh\n states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.\n\n\nNext we implement the \neachbatch\n method from the \nmx.AbstractDataProvider\n interface for the provider. We start by defining the data and label arrays, and the \nDataBatch\n object we will provide in each iteration.\n\n\nThe actual data providing iteration is implemented as a Julia \ncoroutine\n. In this way, we can write the data loading logic as a simple coherent \nfor\n loop, and do not need to implement the interface functions like Base.start, Base.next, etc.\n\n\nBasically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.\n\n\n\n\nTraining the LSTM\n\n\nNow we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:\n\n\nNote all the parameters are defined in \nexamples/char-lstm/config.jl\n. Now we load the text file and define the data provider. The data \ninput.txt\n we used in this example is \na tiny Shakespeare dataset\n. But you can try with other text files.\n\n\nThe last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.\n\n\nNote we are also using a customized \nNLL\n evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.\n\n\n...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'\n\n\n\n\n\n\nSampling Random Sentences\n\n\nAfter training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:\n\n\n\n\nStarting from some fixed character, take \na\n for example, and feed   it as input to the LSTM.\n\n\nThe LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.\n\n\nIn the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).\n\n\nContinue running until we sampled enough characters.\n\n\n\n\nNote we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.\n\n\n## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?\n\n\n\n\nSee \nAndrej Karpathy's blog post\n on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in \nexamples/char-lstm/sampler.jl\n.\n\n\n\n\nVisualizing the LSTM\n\n\nFinally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than \nChristopher Olah's illustrations\n, but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in \nexamples/char-lstm/visualize.jl\n.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#generating-random-sentence-with-lstm-rnn", 
            "text": "This tutorial shows how to train a LSTM (Long short-term memory) RNN (recurrent neural network) to perform character-level sequence training and prediction. The original model, usually called  char-rnn  is described in  Andrej Karpathy's blog , with a reference implementation in Torch available  here .  Because MXNet.jl does not have a specialized model for recurrent neural networks yet, the example shown here is an implementation of LSTM by using the default FeedForward model via explicitly unfolding over time. We will be using fixed-length input sequence for training. The code is adapted from the  char-rnn example for MXNet's Python binding , which demonstrates how to use low-level  Symbolic API  to build customized neural network models directly.  The most important code snippets of this example is shown and explained here. To see and run the complete code, please refer to the  examples/char-lstm  directory. You will need to install  Iterators.jl  and  StatsBase.jl  to run this example.", 
            "title": "Generating Random Sentence with LSTM RNN"
        }, 
        {
            "location": "/tutorial/char-lstm/#lstm-cells", 
            "text": "Christopher Olah has a  great blog post about LSTM  with beautiful and clear illustrations. So we will not repeat the definition and explanation of what an LSTM cell is here. Basically, an LSTM cell takes input  x , as well as previous states (including  c  and  h ), and produce the next states. We define a helper type to bundle the two state variables together:  Because LSTM weights are shared at every time when we do explicit unfolding, so we also define a helper type to hold all the weights (and bias) for an LSTM cell for convenience.  Note all the variables are of type SymbolicNode. We will construct the LSTM network as a symbolic computation graph, which is then instantiated with NDArray for actual computation.  The following figure is stolen (permission requested) from  Christopher Olah's blog , which illustrate exactly what the code snippet above is doing.   In particular, instead of defining the four gates independently, we do the computation together and then use SliceChannel to split them into four outputs. The computation of gates are all done with the symbolic API. The return value is a LSTM state containing the output of a LSTM cell.", 
            "title": "LSTM Cells"
        }, 
        {
            "location": "/tutorial/char-lstm/#unfolding-lstm", 
            "text": "Using the LSTM cell defined above, we are now ready to define a function to unfold a LSTM network with L layers and T time steps. The first part of the function is just defining all the symbolic variables for the shared weights and states.  The  embed_W  is the weights used for character embedding \u2013- i.e. mapping the one-hot encoded characters into real vectors. The  pred_W  and  pred_b  are weights and bias for the final prediction at each time step.  Then we define the weights for each LSTM cell. Note there is one cell for each layer, and it will be replicated (unrolled) over time. The states are, however,  not  shared over time. Instead, here we define the initial states here at the beginning of a sequence, and we will update them with the output states at each time step as we explicitly unroll the LSTM.  Unrolling over time is a straightforward procedure of stacking the embedding layer, and then LSTM cells, on top of which the prediction layer. During unrolling, we update the states and collect all the outputs. Note each time step takes data and label as inputs. If the LSTM is named as  :ptb , the data and label at step  t  will be named  :ptb_data_$t  and  :ptb_label_$t . Late on when we prepare the data, we will define the data provider to match those names.  Note at each time step, the prediction is connected to a SoftmaxOutput operator, which could back propagate when corresponding labels are provided. The states are then connected to the next time step, which allows back propagate through time. However, at the end of the sequence, the final states are not connected to anything. This dangling outputs is problematic, so we explicitly connect each of them to a BlockGrad operator, which simply back propagates 0-gradient and closes the computation graph.  In the end, we just group all the prediction outputs at each time step as a single SymbolicNode and return. Optionally we will also group the final states, this is used when we use the trained LSTM to sample sentences.", 
            "title": "Unfolding LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#data-provider-for-text-sequences", 
            "text": "Now we need to construct a data provider that takes a text file, divide the text into mini-batches of fixed-length character-sequences, and provide them as one-hot encoded vectors.  Note the is no fancy feature extraction at all. Each character is simply encoded as a one-hot vector: a 0-1 vector of the size given by the vocabulary. Here we just construct the vocabulary by collecting all the unique characters in the training text \u2013 there are not too many of them (including punctuations and whitespace) for English text. Each input character is then encoded as a vector of 0s on all coordinates, and 1 on the coordinate corresponding to that character. The character-to-coordinate mapping is giving by the vocabulary.  The text sequence data provider implements the  Data Providers  api. We define the  CharSeqProvider  as below:  The provided data and labels follow the naming convention of inputs used when unrolling the LSTM. Note in the code below, apart from  $name_data_$t  and  $name_label_$t , we also provides the initial  c  and  h  states for each layer. This is because we are using the high-level FeedForward API, which has no idea about time and states. So we will feed the initial states for each sequence from the data provider. Since the initial states is always zero, we just need to always provide constant zero blobs.  Next we implement the  eachbatch  method from the  mx.AbstractDataProvider  interface for the provider. We start by defining the data and label arrays, and the  DataBatch  object we will provide in each iteration.  The actual data providing iteration is implemented as a Julia  coroutine . In this way, we can write the data loading logic as a simple coherent  for  loop, and do not need to implement the interface functions like Base.start, Base.next, etc.  Basically, we partition the text into batches, each batch containing several contiguous text sequences. Note at each time step, the LSTM is trained to predict the next character, so the label is the same as the data, but shifted ahead by one index.", 
            "title": "Data Provider for Text Sequences"
        }, 
        {
            "location": "/tutorial/char-lstm/#training-the-lstm", 
            "text": "Now we have implemented all the supporting infrastructures for our char-lstm. To train the model, we just follow the standard high-level API. Firstly, we construct a LSTM symbolic architecture:  Note all the parameters are defined in  examples/char-lstm/config.jl . Now we load the text file and define the data provider. The data  input.txt  we used in this example is  a tiny Shakespeare dataset . But you can try with other text files.  The last step is to construct a model, an optimizer and fit the mode to the data. We are using the ADAM optimizer [Adam]_ in this example.  Note we are also using a customized  NLL  evaluation metric, which calculate the negative log-likelihood during training. Here is an output sample at the end of the training process.  ...\nINFO: Speed: 357.72 samples/sec\nINFO: == Epoch 020 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4672\nINFO:         perplexity = 4.3373\nINFO:               time = 87.2631 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6374\nINFO:         perplexity = 5.1418\nINFO: Saved checkpoint to 'char-lstm/checkpoints/ptb-0020.params'\nINFO: Speed: 368.74 samples/sec\nINFO: Speed: 361.04 samples/sec\nINFO: Speed: 360.02 samples/sec\nINFO: Speed: 362.34 samples/sec\nINFO: Speed: 360.80 samples/sec\nINFO: Speed: 362.77 samples/sec\nINFO: Speed: 357.18 samples/sec\nINFO: Speed: 355.30 samples/sec\nINFO: Speed: 362.33 samples/sec\nINFO: Speed: 359.23 samples/sec\nINFO: Speed: 358.09 samples/sec\nINFO: Speed: 356.89 samples/sec\nINFO: Speed: 371.91 samples/sec\nINFO: Speed: 372.24 samples/sec\nINFO: Speed: 356.59 samples/sec\nINFO: Speed: 356.64 samples/sec\nINFO: Speed: 360.24 samples/sec\nINFO: Speed: 360.32 samples/sec\nINFO: Speed: 362.38 samples/sec\nINFO: == Epoch 021 ==========\nINFO: ## Training summary\nINFO:                NLL = 1.4655\nINFO:         perplexity = 4.3297\nINFO:               time = 86.9243 seconds\nINFO: ## Validation summary\nINFO:                NLL = 1.6366\nINFO:         perplexity = 5.1378\nINFO: Saved checkpoint to 'examples/char-lstm/checkpoints/ptb-0021.params'", 
            "title": "Training the LSTM"
        }, 
        {
            "location": "/tutorial/char-lstm/#sampling-random-sentences", 
            "text": "After training the LSTM, we can now sample random sentences from the trained model. The sampler works in the following way:   Starting from some fixed character, take  a  for example, and feed   it as input to the LSTM.  The LSTM will produce an output distribution over the vocabulary and   a state in the first time step. We sample a character from the   output distribution, fix it as the second character.  In the next time step, we feed the previously sampled character as   input and continue running the LSTM by also taking the previous   states (instead of the 0 initial states).  Continue running until we sampled enough characters.   Note we are running with mini-batches, so several sentences could be sampled simultaneously. Here are some sampled outputs from a network I trained for around half an hour on the Shakespeare dataset. Note all the line-breaks, punctuations and upper-lower case letters are produced by the sampler itself. I did not do any post-processing.  ## Sample 1\nall have sir,\nAway will fill'd in His time, I'll keep her, do not madam, if they here? Some more ha?\n\n## Sample 2\nam.\n\nCLAUDIO:\nHone here, let her, the remedge, and I know not slept a likely, thou some soully free?\n\n## Sample 3\narrel which noble thing\nThe exchnachsureding worns: I ne'er drunken Biancas, fairer, than the lawfu?\n\n## Sample 4\naugh assalu, you'ld tell me corn;\nFarew. First, for me of a loved. Has thereat I knock you presents?\n\n## Sample 5\name the first answer.\n\nMARIZARINIO:\nDoor of Angelo as her lord, shrield liken Here fellow the fool ?\n\n## Sample 6\nad well.\n\nCLAUDIO:\nSoon him a fellows here; for her fine edge in a bogms' lord's wife.\n\nLUCENTIO:\nI?\n\n## Sample 7\nadrezilian measure.\n\nLUCENTIO:\nSo, help'd you hath nes have a than dream's corn, beautio, I perchas?\n\n## Sample 8\nas eatter me;\nThe girlly: and no other conciolation!\n\nBISTRUMIO:\nI have be rest girl. O, that I a h?\n\n## Sample 9\nand is intend you sort:\nWhat held her all 'clama's for maffice. Some servant.' what I say me the cu?\n\n## Sample 10\nan thoughts will said in our pleasue,\nNot scanin on him that you live; believaries she.\n\nISABELLLLL?  See  Andrej Karpathy's blog post  on more examples and links including Linux source codes, Algebraic Geometry Theorems, and even cooking recipes. The code for sampling can be found in  examples/char-lstm/sampler.jl .", 
            "title": "Sampling Random Sentences"
        }, 
        {
            "location": "/tutorial/char-lstm/#visualizing-the-lstm", 
            "text": "Finally, you could visualize the LSTM by calling to_graphviz on the constructed LSTM symbolic architecture. We only show an example of 1-layer and 2-time-step LSTM below. The automatic layout produced by GraphViz is definitely much less clear than  Christopher Olah's illustrations , but could otherwise be very useful for debugging. As we can see, the LSTM unfolded over time is just a (very) deep neural network. The complete code for producing this visualization can be found in  examples/char-lstm/visualize.jl .", 
            "title": "Visualizing the LSTM"
        }, 
        {
            "location": "/user-guide/install/", 
            "text": "Installation Guide\n\n\n\n\nAutomatic Installation\n\n\nTo install MXNet.jl, simply type\n\n\nPkg.add(\nMXNet\n)\n\n\n\n\nin the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead\n\n\nPkg.checkout(\nMXNet\n)\n\n\n\n\nMXNet.jl is built on top of \nlibmxnet\n. Upon installation, Julia will try to automatically download and build libmxnet.\n\n\nThe libmxnet source is downloaded to \nPkg.dir(\"MXNet\")/deps/src/mxnet\n. The automatic build is using default configurations, with OpenCV, CUDA disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, you can compile and install libmxnet manually. Please see below for more details.\n\n\n\n\nManual Compilation\n\n\nIt is possible to compile libmxnet separately and point MXNet.jl to a the existing library in case automatic compilation fails due to unresolved dependencies in an un-standard environment; Or when one want to work with a seperate, maybe customized libmxnet.\n\n\nTo build libmxnet, please refer to \nthe installation guide of libmxnet\n. After successfully installing libmxnet, set the \nMXNET_HOME\n \nenvironment variable\n to the location of libmxnet. In other words, the compiled \nlibmxnet.so\n should be found in \n$MXNET_HOME/lib\n.\n\n\n\n\nnote\n\n\nThe constant \nMXNET_HOME\n is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by \nBase.compilecache(\"MXNet\")\n.\n\n\n\n\nWhen the \nMXNET_HOME\n environment variable is detected and the corresponding \nlibmxnet.so\n could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.\n\n\nBasically, MXNet.jl will search \nlibmxnet.so\n or \nlibmxnet.dll\n in the following paths (and in that order):\n\n\n\n\n$MXNET_HOME/lib\n: customized libmxnet builds\n\n\nPkg.dir(\"MXNet\")/deps/usr/lib\n: automatic builds\n\n\nAny system wide library search path\n\n\n\n\nNote that MXNet.jl can not load \nlibmxnet.so\n even if it is on one of the paths above in case a library it depends upon is missing from the \nLD_LIBRARY_PATH\n. Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to \nLD_LIBRARY_PATH\n.", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#installation-guide", 
            "text": "", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/user-guide/install/#automatic-installation", 
            "text": "To install MXNet.jl, simply type  Pkg.add( MXNet )  in the Julia REPL. Or to use the latest git version of MXNet.jl, use the following command instead  Pkg.checkout( MXNet )  MXNet.jl is built on top of  libmxnet . Upon installation, Julia will try to automatically download and build libmxnet.  The libmxnet source is downloaded to  Pkg.dir(\"MXNet\")/deps/src/mxnet . The automatic build is using default configurations, with OpenCV, CUDA disabled. If the compilation failed due to unresolved dependency, or if you want to customize the build, you can compile and install libmxnet manually. Please see below for more details.", 
            "title": "Automatic Installation"
        }, 
        {
            "location": "/user-guide/install/#manual-compilation", 
            "text": "It is possible to compile libmxnet separately and point MXNet.jl to a the existing library in case automatic compilation fails due to unresolved dependencies in an un-standard environment; Or when one want to work with a seperate, maybe customized libmxnet.  To build libmxnet, please refer to  the installation guide of libmxnet . After successfully installing libmxnet, set the  MXNET_HOME   environment variable  to the location of libmxnet. In other words, the compiled  libmxnet.so  should be found in  $MXNET_HOME/lib .   note  The constant  MXNET_HOME  is pre-compiled in MXNet.jl package cache. If you updated the environment variable after installing MXNet.jl, make sure to update the pre-compilation cache by  Base.compilecache(\"MXNet\") .   When the  MXNET_HOME  environment variable is detected and the corresponding  libmxnet.so  could be loaded successfully, MXNet.jl will skip automatic building during installation and use the specified libmxnet instead.  Basically, MXNet.jl will search  libmxnet.so  or  libmxnet.dll  in the following paths (and in that order):   $MXNET_HOME/lib : customized libmxnet builds  Pkg.dir(\"MXNet\")/deps/usr/lib : automatic builds  Any system wide library search path   Note that MXNet.jl can not load  libmxnet.so  even if it is on one of the paths above in case a library it depends upon is missing from the  LD_LIBRARY_PATH . Thus, if you are going to compile to add CUDA, the path to the CUDA libraries will have to be added to  LD_LIBRARY_PATH .", 
            "title": "Manual Compilation"
        }, 
        {
            "location": "/user-guide/overview/", 
            "text": "Overview\n\n\n\n\nMXNet.jl Namespace\n\n\nMost the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a \nmx\n module. The convention of accessing the MXNet.jl interface is the to use the \nmx.\n prefix explicitly:\n\n\nusing MXNet\n\nx = mx.zeros(2,3)              # MXNet NDArray\ny = zeros(eltype(x), size(x))  # Julia Array\ncopy!(y, x)                    # Overloaded function in Julia Base\nz = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\nmx.copy!(z, y)                 # Same as copy!(z, y)\n\n\n\n\nNote functions like \nsize\n, \ncopy!\n that is extensively overloaded for various types works out of the box. But functions like \nzeros\n and \nones\n will be ambiguous, so we always use the \nmx.\n prefix. If you prefer, the \nmx.\n prefix can be used explicitly for all MXNet.jl functions, including \nsize\n and \ncopy!\n as shown in the last line.\n\n\n\n\nLow Level Interface\n\n\n\n\nNDArrays\n\n\nNDArray is the basic building blocks of the actual computations in MXNet. It is like a Julia \nArray\n object, with some important differences listed here:\n\n\n\n\nThe actual data could live on different \nContext\n (e.g. GPUs). For   some contexts, iterating into the elements one by one is very slow,   thus indexing into NDArray is not supported in general. The easiest   way to inspect the contents of an NDArray is to use the \ncopy\n   function to copy the contents as a Julia \nArray\n.\n\n\nOperations on NDArray (including basic arithmetics and neural   network related operators) are executed in parallel with automatic   dependency tracking to ensure correctness.\n\n\nThere is no generics in NDArray, the \neltype\n is always   \nmx.MX_float\n. Because for applications in machine learning, single   precision floating point numbers are typical a best choice balancing   between precision, speed and portability. Also since libmxnet is   designed to support multiple languages as front-ends, it is much   simpler to implement with a fixed data type.\n\n\n\n\nWhile most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the NDArray API is useful for implementing \nOptimizer\n or customized operators in Julia directly.\n\n\nThe followings are common ways to create NDArray objects:\n\n\n\n\nmx.empty(shape[, context])\n: create on uninitialized array of a   given shape on a specific device. For example,   \nmx.empty(2,3)\n, \nmx.((2,3), mx.gpu(2))\n.\n\n\nmx.zeros(shape[, context])\n and \nmx.ones(shape[, context])\n:   similar to the Julia's built-in \nzeros\n and \nones\n.\n\n\nmx.copy(jl_arr, context)\n: copy the contents of a Julia \nArray\n to   a specific device.\n\n\n\n\nMost of the convenient functions like \nsize\n, \nlength\n, \nndims\n, \neltype\n on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take \nslices\n:\n\n\na = mx.ones(2,3)\nb = mx.slice(a, 1:2)\nb[:] = 2\nprintln(copy(a))\n# =\n\n# Float32[2.0 2.0 1.0\n#         2.0 2.0 1.0]\n\n\n\n\nA slice is a sub-region sharing the same memory with the original NDArray object. A slice is always a contiguous piece of memory, so only slicing on the \nlast\n dimension is supported. The example above also shows a way to set the contents of an NDArray.\n\n\na = mx.empty(2,3)\na[:] = 0.5              # set all elements to a scalar\na[:] = rand(size(a))    # set contents with a Julia Array\ncopy!(a, rand(size(a))) # set value by copying a Julia Array\nb = mx.empty(size(a))\nb[:] = a                # copying and assignment between NDArrays\n\n\n\n\nNote due to the intrinsic design of the Julia language, a normal assignment\n\n\na = b\n\n\n\n\ndoes \nnot\n mean copying the contents of \nb\n to \na\n. Instead, it just make the variable \na\n pointing to a new object, which is \nb\n. Similarly, inplace arithmetics does not work as expected:\n\n\na = mx.ones(2)\nr = a           # keep a reference to a\nb = mx.ones(2)\na += b          # translates to a = a + b\nprintln(copy(a))\n# =\n Float32[2.0f0,2.0f0]\nprintln(copy(r))\n# =\n Float32[1.0f0,1.0f0]\n\n\n\n\nAs we can see, \na\n has expected value, but instead of inplace updating, a new NDArray is created and \na\n is set to point to this new object. If we look at \nr\n, which still reference to the old \na\n, its content has not changed. There is currently no way in Julia to overload the operators like \n+=\n to get customized behavior.\n\n\nInstead, you will need to write \na[:] = a+b\n, or if you want \nreal\n inplace \n+=\n operation, MXNet.jl provides a simple macro \n@mx.inplace\n:\n\n\n@mx.inplace a += b\nmacroexpand(:(@mx.inplace a += b))\n# =\n :(MXNet.mx.add_to!(a,b))\n\n\n\n\nAs we can see, it translate the \n+=\n operator to an explicit \nadd_to!\n function call, which invokes into libmxnet to add the contents of \nb\n into \na\n directly. For example, the following is the update rule in the SGD \nOptimizer\n (both \ngrad\n and \nweight\n are NDArray objects):\n\n\n@inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)\n\n\n\n\nNote there is no much magic in \nmx.inplace\n: it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by \ngrad_scale\n and adding the weight decay all create temporary NDArray objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp NDArray vs. pre-allocating:\n\n\nusing Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op()))) \n 1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))\n\n\n\n\nThe comparison on my laptop shows that \nnormal_op\n while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing \nN_REP\n), is only about twice slower than the pre-allocated one.\n\n\n\n\n\n\n\n\nRow\n\n\nFunction\n\n\nAverage\n\n\nRelative\n\n\nReplications\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\"inplace_op\"\n\n\n0.0074854\n\n\n1.0\n\n\n100\n\n\n\n\n\n\n2\n\n\n\"normal_op\"\n\n\n0.0174202\n\n\n2.32723\n\n\n100\n\n\n\n\n\n\n\n\nSo it will usually not be a big problem unless you are at the bottleneck of the computation.\n\n\n\n\nDistributed Key-value Store\n\n\nThe type \nKVStore\n and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.\n\n\nThe following example shows how to create a local \nKVStore\n, initialize a value and then pull it back.\n\n\nkv    = mx.KVStore(:local)\nshape = (2,3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape)*2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\nprintln(copy(a))\n# =\n\n# Float32[2.0 2.0 2.0\n#        2.0 2.0 2.0]\n\n\n\n\n\n\nIntermediate Level Interface\n\n\n\n\nSymbols and Composition\n\n\nThe way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like \nTheano\n, except that we avoided long expression compiliation time by providing \nlarger\n neural network related building blocks to guarantee computation performance. See also \nthis note\n for the design and trade-off of the MXNet symbolic composition system.\n\n\nThe basic type is \nmx.Symbol\n. The following is a trivial example of composing two symbols with the \n+\n operation.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B\n\n\n\n\nWe get a new \nsymbol\n by composing existing \nsymbols\n by some \noperations\n. A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a ReLU activation function.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet = mx.Activation(data=net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(data=net, name=:fc2, num_hidden=64)\nnet = mx.Softmax(data=net, name=:out)\n\n\n\n\nEach time we take the previous symbol, and compose with an operation. Unlike the simple \n+\n example above, the \noperations\n here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.\n\n\nEach of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g. \nnum_hidden\n, \nact_type\n) to further customize the composition results.\n\n\nWhen applying those operations, we can also specify a \nname\n for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.\n\n\nEach symbol takes some arguments. For example, in the \n+\n case above, to compute the value of \nC\n, we will need to know the values of the two inputs \nA\n and \nB\n. For neural networks, the arguments are primarily two categories: \ninputs\n and \nparameters\n. \ninputs\n are data and labels for the networks, while \nparameters\n are typically trainable \nweights\n, \nbias\n, \nfilters\n.\n\n\nWhen composing symbols, their arguments accumulates. We can list all the arguments by\n\n\njulia\n mx.list_arguments(net)\n6-element Array{Symbol,1}:\n :data         # Input data, name from the first data variable\n :fc1_weight   # Weights of the fully connected layer named :fc1\n :fc1_bias     # Bias of the layer :fc1\n :fc2_weight   # Weights of the layer :fc2\n :fc2_bias     # Bias of the layer :fc2\n :out_label    # Input label, required by the softmax layer named :out\n\n\n\n\nNote the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:\n\n\nnet = mx.Variable(:data)\nw   = mx.Variable(:myweight)\nnet = mx.FullyConnected(data=data, weight=w, name=:fc1, num_hidden=128)\nmx.list_arguments(net)\n# =\n\n# 3-element Array{Symbol,1}:\n#  :data\n#  :myweight\n#  :fc1_bias\n\n\n\n\nThe simple fact is that a \nVariable\n is just a placeholder \nmx.Symbol\n. In composition, we can use arbitrary symbols for arguments. For example:\n\n\nnet  = mx.Variable(:data)\nnet  = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet2 = mx.Variable(:data2)\nnet2 = mx.FullyConnected(data=net2, name=:net2, num_hidden=128)\nmx.list_arguments(net2)\n# =\n\n# 3-element Array{Symbol,1}:\n#  :data2\n#  :net2_weight\n#  :net2_bias\ncomposed_net = net2(data2=net, name=:composed)\nmx.list_arguments(composed_net)\n# =\n\n# 5-element Array{Symbol,1}:\n#  :data\n#  :fc1_weight\n#  :fc1_bias\n#  :net2_weight\n#  :net2_bias\n\n\n\n\nNote we use a composed symbol, \nnet\n as the argument \ndata2\n for \nnet2\n to get a new symbol, which we named \n:composed\n. It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.\n\n\n\n\nShape Inference\n\n\nGiven enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like \nnum_hidden\n, the shapes for the weights and bias in a neural network could be inferred.\n\n\nnet = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=10)\narg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))\n\n\n\n\nThe returned shapes corresponds to arguments with the same order as returned by \nmx.list_arguments\n. The \nout_shapes\n are shapes for outputs, and \naux_shapes\n can be safely ignored for now.\n\n\nfor (n,s) in zip(mx.list_arguments(net), arg_shapes)\n  println(\n$n =\n $s\n)\nend\n# =\n\n# data =\n (10,64)\n# fc1_weight =\n (10,10)\n# fc1_bias =\n (10,)\nfor (n,s) in zip(mx.list_outputs(net), out_shapes)\n  println(\n$n =\n $s\n)\nend\n# =\n\n# fc1_output =\n (10,64)\n\n\n\n\n\n\nBinding and Executing\n\n\nIn order to execute the computation graph specified a composed symbol, we will \nbind\n the free variables to concrete values, specified as \nmx.NDArray\n. This will create an \nmx.Executor\n on a given \nmx.Context\n. A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.\n\n\nA = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A .* B\na = mx.ones(3) * 4\nb = mx.ones(3) * 2\nc_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =\n a, :B =\n b))\n\nmx.forward(c_exec)\ncopy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n# =\n\n# 3-element Array{Float32,1}:\n#  8.0\n#  8.0\n#  8.0\n\n\n\n\nFor neural networks, it is easier to use \nsimple_bind\n. By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the \nModel\n interface.\n\n\nTODO\n Provide pointers to model tutorial and further details about binding and symbolic API.\n\n\n\n\nHigh Level Interface\n\n\nThe high level interface include model training and prediction API, etc.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/overview/#mxnetjl-namespace", 
            "text": "Most the functions and types in MXNet.jl are organized in a flat namespace. Because many some functions are conflicting with existing names in the Julia Base module, we wrap them all in a  mx  module. The convention of accessing the MXNet.jl interface is the to use the  mx.  prefix explicitly:  using MXNet\n\nx = mx.zeros(2,3)              # MXNet NDArray\ny = zeros(eltype(x), size(x))  # Julia Array\ncopy!(y, x)                    # Overloaded function in Julia Base\nz = mx.ones(size(x), mx.gpu()) # MXNet NDArray on GPU\nmx.copy!(z, y)                 # Same as copy!(z, y)  Note functions like  size ,  copy!  that is extensively overloaded for various types works out of the box. But functions like  zeros  and  ones  will be ambiguous, so we always use the  mx.  prefix. If you prefer, the  mx.  prefix can be used explicitly for all MXNet.jl functions, including  size  and  copy!  as shown in the last line.", 
            "title": "MXNet.jl Namespace"
        }, 
        {
            "location": "/user-guide/overview/#low-level-interface", 
            "text": "", 
            "title": "Low Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#ndarrays", 
            "text": "NDArray is the basic building blocks of the actual computations in MXNet. It is like a Julia  Array  object, with some important differences listed here:   The actual data could live on different  Context  (e.g. GPUs). For   some contexts, iterating into the elements one by one is very slow,   thus indexing into NDArray is not supported in general. The easiest   way to inspect the contents of an NDArray is to use the  copy    function to copy the contents as a Julia  Array .  Operations on NDArray (including basic arithmetics and neural   network related operators) are executed in parallel with automatic   dependency tracking to ensure correctness.  There is no generics in NDArray, the  eltype  is always    mx.MX_float . Because for applications in machine learning, single   precision floating point numbers are typical a best choice balancing   between precision, speed and portability. Also since libmxnet is   designed to support multiple languages as front-ends, it is much   simpler to implement with a fixed data type.   While most of the computation is hidden in libmxnet by operators corresponding to various neural network layers. Getting familiar with the NDArray API is useful for implementing  Optimizer  or customized operators in Julia directly.  The followings are common ways to create NDArray objects:   mx.empty(shape[, context]) : create on uninitialized array of a   given shape on a specific device. For example,    mx.empty(2,3) ,  mx.((2,3), mx.gpu(2)) .  mx.zeros(shape[, context])  and  mx.ones(shape[, context]) :   similar to the Julia's built-in  zeros  and  ones .  mx.copy(jl_arr, context) : copy the contents of a Julia  Array  to   a specific device.   Most of the convenient functions like  size ,  length ,  ndims ,  eltype  on array objects should work out-of-the-box. Although indexing is not supported, it is possible to take  slices :  a = mx.ones(2,3)\nb = mx.slice(a, 1:2)\nb[:] = 2\nprintln(copy(a))\n# = \n# Float32[2.0 2.0 1.0\n#         2.0 2.0 1.0]  A slice is a sub-region sharing the same memory with the original NDArray object. A slice is always a contiguous piece of memory, so only slicing on the  last  dimension is supported. The example above also shows a way to set the contents of an NDArray.  a = mx.empty(2,3)\na[:] = 0.5              # set all elements to a scalar\na[:] = rand(size(a))    # set contents with a Julia Array\ncopy!(a, rand(size(a))) # set value by copying a Julia Array\nb = mx.empty(size(a))\nb[:] = a                # copying and assignment between NDArrays  Note due to the intrinsic design of the Julia language, a normal assignment  a = b  does  not  mean copying the contents of  b  to  a . Instead, it just make the variable  a  pointing to a new object, which is  b . Similarly, inplace arithmetics does not work as expected:  a = mx.ones(2)\nr = a           # keep a reference to a\nb = mx.ones(2)\na += b          # translates to a = a + b\nprintln(copy(a))\n# =  Float32[2.0f0,2.0f0]\nprintln(copy(r))\n# =  Float32[1.0f0,1.0f0]  As we can see,  a  has expected value, but instead of inplace updating, a new NDArray is created and  a  is set to point to this new object. If we look at  r , which still reference to the old  a , its content has not changed. There is currently no way in Julia to overload the operators like  +=  to get customized behavior.  Instead, you will need to write  a[:] = a+b , or if you want  real  inplace  +=  operation, MXNet.jl provides a simple macro  @mx.inplace :  @mx.inplace a += b\nmacroexpand(:(@mx.inplace a += b))\n# =  :(MXNet.mx.add_to!(a,b))  As we can see, it translate the  +=  operator to an explicit  add_to!  function call, which invokes into libmxnet to add the contents of  b  into  a  directly. For example, the following is the update rule in the SGD  Optimizer  (both  grad  and  weight  are NDArray objects):  @inplace weight += -lr * (grad_scale * grad + self.weight_decay * weight)  Note there is no much magic in  mx.inplace : it only does a shallow translation. In the SGD update rule example above, the computation like scaling the gradient by  grad_scale  and adding the weight decay all create temporary NDArray objects. To mitigate this issue, libmxnet has a customized memory allocator designed specifically to handle this kind of situations. The following snippet does a simple benchmark on allocating temp NDArray vs. pre-allocating:  using Benchmark\nusing MXNet\n\nN_REP = 1000\nSHAPE = (128, 64)\nCTX   = mx.cpu()\nLR    = 0.1\n\nfunction inplace_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  # pre-allocate temp objects\n  grad_lr = mx.empty(SHAPE, CTX)\n\n  for i = 1:N_REP\n    copy!(grad_lr, grad)\n    @mx.inplace grad_lr .*= LR\n    @mx.inplace weight -= grad_lr\n  end\n  return weight\nend\n\nfunction normal_op()\n  weight = mx.zeros(SHAPE, CTX)\n  grad   = mx.ones(SHAPE, CTX)\n\n  for i = 1:N_REP\n    weight[:] -= LR * grad\n  end\n  return weight\nend\n\n# make sure the results are the same\n@assert(maximum(abs(copy(normal_op() - inplace_op())))   1e-6)\n\nprintln(compare([inplace_op, normal_op], 100))  The comparison on my laptop shows that  normal_op  while allocating a lot of temp NDArray in the loop (the performance gets worse when increasing  N_REP ), is only about twice slower than the pre-allocated one.     Row  Function  Average  Relative  Replications      1  \"inplace_op\"  0.0074854  1.0  100    2  \"normal_op\"  0.0174202  2.32723  100     So it will usually not be a big problem unless you are at the bottleneck of the computation.", 
            "title": "NDArrays"
        }, 
        {
            "location": "/user-guide/overview/#distributed-key-value-store", 
            "text": "The type  KVStore  and related methods are used for data sharing across different devices or machines. It provides a simple and efficient integer - NDArray key-value storage system that each device can pull or push.  The following example shows how to create a local  KVStore , initialize a value and then pull it back.  kv    = mx.KVStore(:local)\nshape = (2,3)\nkey   = 3\n\nmx.init!(kv, key, mx.ones(shape)*2)\na = mx.empty(shape)\nmx.pull!(kv, key, a) # pull value into a\nprintln(copy(a))\n# = \n# Float32[2.0 2.0 2.0\n#        2.0 2.0 2.0]", 
            "title": "Distributed Key-value Store"
        }, 
        {
            "location": "/user-guide/overview/#intermediate-level-interface", 
            "text": "", 
            "title": "Intermediate Level Interface"
        }, 
        {
            "location": "/user-guide/overview/#symbols-and-composition", 
            "text": "The way we build deep learning models in MXNet.jl is to use the powerful symbolic composition system. It is like  Theano , except that we avoided long expression compiliation time by providing  larger  neural network related building blocks to guarantee computation performance. See also  this note  for the design and trade-off of the MXNet symbolic composition system.  The basic type is  mx.Symbol . The following is a trivial example of composing two symbols with the  +  operation.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A + B  We get a new  symbol  by composing existing  symbols  by some  operations . A hierarchical architecture of a deep neural network could be realized by recursive composition. For example, the following code snippet shows a simple 2-layer MLP construction, using a hidden layer of 128 units and a ReLU activation function.  net = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet = mx.Activation(data=net, name=:relu1, act_type=:relu)\nnet = mx.FullyConnected(data=net, name=:fc2, num_hidden=64)\nnet = mx.Softmax(data=net, name=:out)  Each time we take the previous symbol, and compose with an operation. Unlike the simple  +  example above, the  operations  here are \"bigger\" ones, that correspond to common computation layers in deep neural networks.  Each of those operation takes one or more input symbols for composition, with optional hyper-parameters (e.g.  num_hidden ,  act_type ) to further customize the composition results.  When applying those operations, we can also specify a  name  for the result symbol. This is convenient if we want to refer to this symbol later on. If not supplied, a name will be automatically generated.  Each symbol takes some arguments. For example, in the  +  case above, to compute the value of  C , we will need to know the values of the two inputs  A  and  B . For neural networks, the arguments are primarily two categories:  inputs  and  parameters .  inputs  are data and labels for the networks, while  parameters  are typically trainable  weights ,  bias ,  filters .  When composing symbols, their arguments accumulates. We can list all the arguments by  julia  mx.list_arguments(net)\n6-element Array{Symbol,1}:\n :data         # Input data, name from the first data variable\n :fc1_weight   # Weights of the fully connected layer named :fc1\n :fc1_bias     # Bias of the layer :fc1\n :fc2_weight   # Weights of the layer :fc2\n :fc2_bias     # Bias of the layer :fc2\n :out_label    # Input label, required by the softmax layer named :out  Note the names of the arguments are generated according to the provided name for each layer. We can also specify those names explicitly:  net = mx.Variable(:data)\nw   = mx.Variable(:myweight)\nnet = mx.FullyConnected(data=data, weight=w, name=:fc1, num_hidden=128)\nmx.list_arguments(net)\n# = \n# 3-element Array{Symbol,1}:\n#  :data\n#  :myweight\n#  :fc1_bias  The simple fact is that a  Variable  is just a placeholder  mx.Symbol . In composition, we can use arbitrary symbols for arguments. For example:  net  = mx.Variable(:data)\nnet  = mx.FullyConnected(data=net, name=:fc1, num_hidden=128)\nnet2 = mx.Variable(:data2)\nnet2 = mx.FullyConnected(data=net2, name=:net2, num_hidden=128)\nmx.list_arguments(net2)\n# = \n# 3-element Array{Symbol,1}:\n#  :data2\n#  :net2_weight\n#  :net2_bias\ncomposed_net = net2(data2=net, name=:composed)\nmx.list_arguments(composed_net)\n# = \n# 5-element Array{Symbol,1}:\n#  :data\n#  :fc1_weight\n#  :fc1_bias\n#  :net2_weight\n#  :net2_bias  Note we use a composed symbol,  net  as the argument  data2  for  net2  to get a new symbol, which we named  :composed . It also shows that a symbol itself is a call-able object, which can be invoked to fill in missing arguments and get more complicated symbol compositions.", 
            "title": "Symbols and Composition"
        }, 
        {
            "location": "/user-guide/overview/#shape-inference", 
            "text": "Given enough information, the shapes of all arguments in a composed symbol could be inferred automatically. For example, given the input shape, and some hyper-parameters like  num_hidden , the shapes for the weights and bias in a neural network could be inferred.  net = mx.Variable(:data)\nnet = mx.FullyConnected(data=net, name=:fc1, num_hidden=10)\narg_shapes, out_shapes, aux_shapes = mx.infer_shape(net, data=(10, 64))  The returned shapes corresponds to arguments with the same order as returned by  mx.list_arguments . The  out_shapes  are shapes for outputs, and  aux_shapes  can be safely ignored for now.  for (n,s) in zip(mx.list_arguments(net), arg_shapes)\n  println( $n =  $s )\nend\n# = \n# data =  (10,64)\n# fc1_weight =  (10,10)\n# fc1_bias =  (10,)\nfor (n,s) in zip(mx.list_outputs(net), out_shapes)\n  println( $n =  $s )\nend\n# = \n# fc1_output =  (10,64)", 
            "title": "Shape Inference"
        }, 
        {
            "location": "/user-guide/overview/#binding-and-executing", 
            "text": "In order to execute the computation graph specified a composed symbol, we will  bind  the free variables to concrete values, specified as  mx.NDArray . This will create an  mx.Executor  on a given  mx.Context . A context describes the computation devices (CPUs, GPUs, etc.) and an executor will carry out the computation (forward/backward) specified in the corresponding symbolic composition.  A = mx.Variable(:A)\nB = mx.Variable(:B)\nC = A .* B\na = mx.ones(3) * 4\nb = mx.ones(3) * 2\nc_exec = mx.bind(C, context=mx.cpu(), args=Dict(:A =  a, :B =  b))\n\nmx.forward(c_exec)\ncopy(c_exec.outputs[1])  # copy turns NDArray into Julia Array\n# = \n# 3-element Array{Float32,1}:\n#  8.0\n#  8.0\n#  8.0  For neural networks, it is easier to use  simple_bind . By providing the shape for input arguments, it will perform a shape inference for the rest of the arguments and create the NDArray automatically. In practice, the binding and executing steps are hidden under the  Model  interface.  TODO  Provide pointers to model tutorial and further details about binding and symbolic API.", 
            "title": "Binding and Executing"
        }, 
        {
            "location": "/user-guide/overview/#high-level-interface", 
            "text": "The high level interface include model training and prediction API, etc.", 
            "title": "High Level Interface"
        }, 
        {
            "location": "/user-guide/faq/", 
            "text": "FAQ\n\n\n\n\nRunning MXNet on AWS GPU instances\n\n\nSee the discussions and notes \nhere\n.", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#faq", 
            "text": "", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/faq/#running-mxnet-on-aws-gpu-instances", 
            "text": "See the discussions and notes  here .", 
            "title": "Running MXNet on AWS GPU instances"
        }, 
        {
            "location": "/api/context/", 
            "text": "Context\n\n\n#\n\n\nMXNet.mx.Context\n \n \nType\n.\n\n\nContext(dev_type, dev_id)\n\n\n\n\nA context describes the device type and id on which computation should be carried on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.cpu\n \n \nFunction\n.\n\n\ncpu(dev_id)\n\n\n\n\nGet a CPU context with a specific id. \ncpu()\n is usually the default context for many operations when no context is specified.\n\n\nArguments\n\n\n\n\ndev_id::Int = 0\n: the CPU id.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.gpu\n \n \nFunction\n.\n\n\ngpu(dev_id)\n\n\n\n\nGet a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.\n\n\nArguments\n\n\n\n\ndev_id :: Int = 0\n the GPU device id.\n\n\n\n\nsource", 
            "title": "Context"
        }, 
        {
            "location": "/api/context/#context", 
            "text": "#  MXNet.mx.Context     Type .  Context(dev_type, dev_id)  A context describes the device type and id on which computation should be carried on.  source  #  MXNet.mx.cpu     Function .  cpu(dev_id)  Get a CPU context with a specific id.  cpu()  is usually the default context for many operations when no context is specified.  Arguments   dev_id::Int = 0 : the CPU id.   source  #  MXNet.mx.gpu     Function .  gpu(dev_id)  Get a GPU context with a specific id. The K GPUs on a node is typically numbered as 0,...,K-1.  Arguments   dev_id :: Int = 0  the GPU device id.   source", 
            "title": "Context"
        }, 
        {
            "location": "/api/model/", 
            "text": "Model\n\n\nThe model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.\n\n\n#\n\n\nMXNet.mx.AbstractModel\n \n \nType\n.\n\n\nAbstractModel\n\n\n\n\nThe abstract super type of all models in MXNet.jl.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nType\n.\n\n\nFeedForward\n\n\n\n\nThe feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of \ntime index\n, but it is relatively easy to implement unrolled RNN / LSTM under this framework (\nTODO\n: add example). For models that handles sequential data explicitly, please use \nTODO\n...\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FeedForward\n \n \nMethod\n.\n\n\nFeedForward(arch :: SymbolicNode, ctx)\n\n\n\n\nArguments:\n\n\n\n\narch\n: the architecture of the network constructed using the symbolic API.\n\n\nctx\n: the devices on which this model should do computation. It could be a single \nContext\n        or a list of \nContext\n objects. In the latter case, data parallelization will be used        for training. If no context is provided, the default context \ncpu()\n will be used.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._split_inputs\n \n \nMethod\n.\n\n\nGet a split of \nbatch_size\n into \nn_split\n pieces for data parallelization. Returns a vector of length \nn_split\n, with each entry a \nUnitRange{Int}\n indicating the slice index for that piece.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fit\n \n \nMethod\n.\n\n\nfit(model :: FeedForward, optimizer, data; kwargs...)\n\n\n\n\nTrain the \nmodel\n on \ndata\n with the \noptimizer\n.\n\n\n\n\nmodel::FeedForward\n: the model to be trained.\n\n\noptimizer::AbstractOptimizer\n: the optimization algorithm to use.\n\n\ndata::AbstractDataProvider\n: the training data provider.\n\n\nn_epoch::Int\n: default 10, the number of full data-passes to run.\n\n\neval_data::AbstractDataProvider\n: keyword argument, default \nnothing\n. The data provider for         the validation set.\n\n\neval_metric::AbstractEvalMetric\n: keyword argument, default \nAccuracy()\n. The metric used         to evaluate the training performance. If \neval_data\n is provided, the same metric is also         calculated on the validation set.\n\n\nkvstore\n: keyword argument, default \n:local\n. The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore: \nKVStore\n or \nBase.Symbol\n\n\ninitializer::AbstractInitializer\n: keyword argument, default \nUniformInitializer(0.01)\n.\n\n\nforce_init::Bool\n: keyword argument, default false. By default, the random initialization using the         provided \ninitializer\n will be skipped if the model weights already exists, maybe from a previous         call to \ntrain\n or an explicit call to \ninit_model\n or \nload_checkpoint\n. When         this option is set, it will always do random initialization at the begining of training.\n\n\ncallbacks::Vector{AbstractCallback}\n: keyword argument, default \n[]\n. Callbacks to be invoked at each epoch or mini-batch,         see \nAbstractCallback\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.init_model\n \n \nMethod\n.\n\n\ninit_model(self, initializer; overwrite=false, input_shapes...)\n\n\n\n\nInitialize the weights in the model.\n\n\nThis method will be called automatically when training a model. So there is usually no need to call this method unless one needs to inspect a model with only randomly initialized weights.\n\n\nArguments:\n\n\n\n\nself::FeedForward\n: the model to be initialized.\n\n\ninitializer::AbstractInitializer\n: an initializer describing how the weights should be initialized.\n\n\noverwrite::Bool\n: keyword argument, force initialization even when weights already exists.\n\n\ninput_shapes\n: the shape of all data and label inputs to this model, given as keyword arguments.                 For example, \ndata=(28,28,1,100), label=(100,)\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.predict\n \n \nMethod\n.\n\n\npredict(self, data; overwrite=false, callback=nothing)\n\n\n\n\nPredict using an existing model. The model should be already initialized, or trained or loaded from a checkpoint. There is an overloaded function that allows to pass the callback as the first argument, so it is possible to do\n\n\npredict(model, data) do batch_output\n  # consume or write batch_output to file\nend\n\n\n\n\nArguments:\n\n\n\n\nself::FeedForward\n:  the model.\n\n\ndata::AbstractDataProvider\n: the data to perform prediction on.\n\n\noverwrite::Bool\n: an \nExecutor\n is initialized the first time predict is called. The memory                    allocation of the \nExecutor\n depends on the mini-batch size of the test                    data provider. If you call predict twice with data provider of the same batch-size,                    then the executor can be potentially be re-used. So, if \noverwrite\n is false,                    we will try to re-use, and raise an error if batch-size changed. If \noverwrite\n                    is true (the default), a new \nExecutor\n will be created to replace the old one.\n\n\n\n\n\n\nNote\n\n\nPrediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO for copying mini-batches of data. Since there is no concern about convergence in prediction, it is better to set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a concern.\n\n\nFor the same reason, currently prediction will only use the first device even if multiple devices are provided to construct the model.\n\n\n\n\n\n\nNote\n\n\nIf you perform further after prediction. The weights are not automatically synchronized if \noverwrite\n is set to false and the old predictor is re-used. In this case setting \noverwrite\n to true (the default) will re-initialize the predictor the next time you call predict and synchronize the weights again.\n\n\n\n\nSee also \ntrain\n, \nfit\n, \ninit_model\n, and \nload_checkpoint\n\n\nsource\n\n\n#\n\n\nMXNet.mx.train\n \n \nMethod\n.\n\n\ntrain(model :: FeedForward, ...)\n\n\n\n\nAlias to \nfit\n.\n\n\nsource", 
            "title": "Models"
        }, 
        {
            "location": "/api/model/#model", 
            "text": "The model API provides convenient high-level interface to do training and predicting on a network described using the symbolic API.  #  MXNet.mx.AbstractModel     Type .  AbstractModel  The abstract super type of all models in MXNet.jl.  source  #  MXNet.mx.FeedForward     Type .  FeedForward  The feedforward model provides convenient interface to train and predict on feedforward architectures like multi-layer MLP, ConvNets, etc. There is no explicitly handling of  time index , but it is relatively easy to implement unrolled RNN / LSTM under this framework ( TODO : add example). For models that handles sequential data explicitly, please use  TODO ...  source  #  MXNet.mx.FeedForward     Method .  FeedForward(arch :: SymbolicNode, ctx)  Arguments:   arch : the architecture of the network constructed using the symbolic API.  ctx : the devices on which this model should do computation. It could be a single  Context         or a list of  Context  objects. In the latter case, data parallelization will be used        for training. If no context is provided, the default context  cpu()  will be used.   source  #  MXNet.mx._split_inputs     Method .  Get a split of  batch_size  into  n_split  pieces for data parallelization. Returns a vector of length  n_split , with each entry a  UnitRange{Int}  indicating the slice index for that piece.  source  #  MXNet.mx.fit     Method .  fit(model :: FeedForward, optimizer, data; kwargs...)  Train the  model  on  data  with the  optimizer .   model::FeedForward : the model to be trained.  optimizer::AbstractOptimizer : the optimization algorithm to use.  data::AbstractDataProvider : the training data provider.  n_epoch::Int : default 10, the number of full data-passes to run.  eval_data::AbstractDataProvider : keyword argument, default  nothing . The data provider for         the validation set.  eval_metric::AbstractEvalMetric : keyword argument, default  Accuracy() . The metric used         to evaluate the training performance. If  eval_data  is provided, the same metric is also         calculated on the validation set.  kvstore : keyword argument, default  :local . The key-value store used to synchronize gradients         and parameters when multiple devices are used for training.  :type kvstore:  KVStore  or  Base.Symbol  initializer::AbstractInitializer : keyword argument, default  UniformInitializer(0.01) .  force_init::Bool : keyword argument, default false. By default, the random initialization using the         provided  initializer  will be skipped if the model weights already exists, maybe from a previous         call to  train  or an explicit call to  init_model  or  load_checkpoint . When         this option is set, it will always do random initialization at the begining of training.  callbacks::Vector{AbstractCallback} : keyword argument, default  [] . Callbacks to be invoked at each epoch or mini-batch,         see  AbstractCallback .   source  #  MXNet.mx.init_model     Method .  init_model(self, initializer; overwrite=false, input_shapes...)  Initialize the weights in the model.  This method will be called automatically when training a model. So there is usually no need to call this method unless one needs to inspect a model with only randomly initialized weights.  Arguments:   self::FeedForward : the model to be initialized.  initializer::AbstractInitializer : an initializer describing how the weights should be initialized.  overwrite::Bool : keyword argument, force initialization even when weights already exists.  input_shapes : the shape of all data and label inputs to this model, given as keyword arguments.                 For example,  data=(28,28,1,100), label=(100,) .   source  #  MXNet.mx.predict     Method .  predict(self, data; overwrite=false, callback=nothing)  Predict using an existing model. The model should be already initialized, or trained or loaded from a checkpoint. There is an overloaded function that allows to pass the callback as the first argument, so it is possible to do  predict(model, data) do batch_output\n  # consume or write batch_output to file\nend  Arguments:   self::FeedForward :  the model.  data::AbstractDataProvider : the data to perform prediction on.  overwrite::Bool : an  Executor  is initialized the first time predict is called. The memory                    allocation of the  Executor  depends on the mini-batch size of the test                    data provider. If you call predict twice with data provider of the same batch-size,                    then the executor can be potentially be re-used. So, if  overwrite  is false,                    we will try to re-use, and raise an error if batch-size changed. If  overwrite                     is true (the default), a new  Executor  will be created to replace the old one.    Note  Prediction is computationally much less costly than training, so the bottleneck sometimes becomes the IO for copying mini-batches of data. Since there is no concern about convergence in prediction, it is better to set the mini-batch size as large as possible (limited by your device memory) if prediction speed is a concern.  For the same reason, currently prediction will only use the first device even if multiple devices are provided to construct the model.    Note  If you perform further after prediction. The weights are not automatically synchronized if  overwrite  is set to false and the old predictor is re-used. In this case setting  overwrite  to true (the default) will re-initialize the predictor the next time you call predict and synchronize the weights again.   See also  train ,  fit ,  init_model , and  load_checkpoint  source  #  MXNet.mx.train     Method .  train(model :: FeedForward, ...)  Alias to  fit .  source", 
            "title": "Model"
        }, 
        {
            "location": "/api/initializer/", 
            "text": "Initializer\n\n\n#\n\n\nMXNet.mx.AbstractInitializer\n \n \nType\n.\n\n\nAbstractInitializer\n\n\n\n\nThe abstract base class for all initializers.\n\n\nTo define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:\n\n\n_init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nOr, if full behavior customization is needed, override the following function\n\n\ninit(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nType\n.\n\n\nNormalInitializer\n\n\n\n\nInitialize weights according to a univariate Gaussian distribution.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.NormalInitializer\n \n \nMethod\n.\n\n\nNormalIninitializer(; mu=0, sigma=0.01)\n\n\n\n\nConstruct a \nNormalInitializer\n with mean \nmu\n and variance \nsigma\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nType\n.\n\n\nUniformInitializer\n\n\n\n\nInitialize weights according to a uniform distribution within the provided scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UniformInitializer\n \n \nMethod\n.\n\n\nUniformInitializer(scale=0.07)\n\n\n\n\nConstruct a \nUniformInitializer\n with the specified scale.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.XavierDistribution\n \n \nType\n.\n\n\nXavierInitializer\n\n\n\n\nThe initializer documented in the paper [Bengio and Glorot 2010]: \nUnderstanding the difficulty of training deep feedforward neuralnetworks\n.\n\n\nThere are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.\n\n\nSeveral different ways of calculating the variance are given in the literature or are used by various libraries.\n\n\n\n\n[Bengio and Glorot 2010]: \nmx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)\n\n\n[K. He, X. Zhang, S. Ren, and J. Sun 2015]: \nmx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)\n\n\ncaffe_avg: \nmx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)\n\n\n\n\nsource", 
            "title": "Initializers"
        }, 
        {
            "location": "/api/initializer/#initializer", 
            "text": "#  MXNet.mx.AbstractInitializer     Type .  AbstractInitializer  The abstract base class for all initializers.  To define a new initializer, it is enough to derive a new type, and implement one or more of the following methods:  _init_weight(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_bias(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_gamma(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)\n_init_beta(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  Or, if full behavior customization is needed, override the following function  init(self :: AbstractInitializer, name :: Base.Symbol, array :: NDArray)  source  #  MXNet.mx.NormalInitializer     Type .  NormalInitializer  Initialize weights according to a univariate Gaussian distribution.  source  #  MXNet.mx.NormalInitializer     Method .  NormalIninitializer(; mu=0, sigma=0.01)  Construct a  NormalInitializer  with mean  mu  and variance  sigma .  source  #  MXNet.mx.UniformInitializer     Type .  UniformInitializer  Initialize weights according to a uniform distribution within the provided scale.  source  #  MXNet.mx.UniformInitializer     Method .  UniformInitializer(scale=0.07)  Construct a  UniformInitializer  with the specified scale.  source  #  MXNet.mx.XavierDistribution     Type .  XavierInitializer  The initializer documented in the paper [Bengio and Glorot 2010]:  Understanding the difficulty of training deep feedforward neuralnetworks .  There are several different version of the XavierInitializer used in the wild. The general idea is that the variance of the initialization distribution is controlled by the dimensionality of the input and output. As a distribution one can either choose a normal distribution with \u03bc = 0 and \u03c3\u00b2 or a uniform distribution from -\u03c3 to \u03c3.  Several different ways of calculating the variance are given in the literature or are used by various libraries.   [Bengio and Glorot 2010]:  mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 1)  [K. He, X. Zhang, S. Ren, and J. Sun 2015]:  mx.XavierInitializer(distribution = mx.xv_gaussian, regularization = mx.xv_in, magnitude = 2)  caffe_avg:  mx.XavierInitializer(distribution = mx.xv_uniform, regularization = mx.xv_avg, magnitude = 3)   source", 
            "title": "Initializer"
        }, 
        {
            "location": "/api/optimizer/", 
            "text": "Optimizers\n\n\n#\n\n\nMXNet.mx.AbstractLearningRateScheduler\n \n \nType\n.\n\n\nAbstractLearningRateScheduler\n\n\n\n\nBase type for all learning rate scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractMomentumScheduler\n \n \nType\n.\n\n\nAbstractMomentumScheduler\n\n\n\n\nBase type for all momentum scheduler.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizer\n \n \nType\n.\n\n\nAbstractOptimizer\n\n\n\n\nBase type for all optimizers.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractOptimizerOptions\n \n \nType\n.\n\n\nAbstractOptimizerOptions\n\n\n\n\nBase class for all optimizer options.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.OptimizationState\n \n \nType\n.\n\n\nOptimizationState\n\n\n\n\nAttributes:\n\n\n\n\nbatch_size\n: The size of the mini-batch used in stochastic training.\n\n\ncurr_epoch\n: The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.\n\n\ncurr_batch\n: The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.\n\n\ncurr_iter\n: The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does \nnot\n reset in each epoch. So it track the \ntotal\n number of mini-batches seen so far.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_learning_rate\n \n \nFunction\n.\n\n\nget_learning_rate(scheduler, state)\n\n\n\n\nArguments\n\n\n\n\nscheduler::AbstractLearningRateScheduler\n: a learning rate scheduler.\n\n\nstate::OptimizationState\n: the current state about epoch, mini-batch and iteration count.\n\n\n\n\nReturns the current learning rate.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_momentum\n \n \nFunction\n.\n\n\nget_momentum(scheduler, state)\n\n\n\n\n\n\nscheduler::AbstractMomentumScheduler\n: the momentum scheduler.\n\n\nstate::OptimizationState\n: the state about current epoch, mini-batch and iteration count.\n\n\n\n\nReturns the current momentum.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_updater\n \n \nMethod\n.\n\n\nget_updater(optimizer)\n\n\n\n\nA utility function to create an updater function, that uses its closure to store all the states needed for each weights.\n\n\n\n\noptimizer::AbstractOptimizer\n: the underlying optimizer.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normalized_gradient\n \n \nMethod\n.\n\n\nnormalized_gradient(opts, state, grad)\n\n\n\n\n\n\nopts::AbstractOptimizerOptions\n: options for the optimizer, should contain the field         \ngrad_scale\n, \ngrad_clip\n and \nweight_decay\n.\n\n\nstate::OptimizationState\n: the current optimization state.\n\n\nweight::NDArray\n: the trainable weights.\n\n\n\n\ngrad::NDArray\n: the original gradient of the weights.\n\n\nGet the properly normalized gradient (re-scaled and clipped if necessary).\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Exp\n \n \nType\n.\n\n\nLearningRate.Exp\n\n\n\n\n$\u001bta_t = \u001bta_0gamma^t$. Here $t$ is the epoch count, or the iteration count if \ndecay_on_iteration\n is set to true.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Fixed\n \n \nType\n.\n\n\nLearningRate.Fixed\n\n\n\n\nFixed learning rate scheduler always return the same learning rate.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LearningRate.Inv\n \n \nType\n.\n\n\nLearningRate.Inv\n\n\n\n\n$\u001bta_t = \u001bta_0 * (1 + gamma * t)^(-power)$. Here $t$ is the epoch count, or the iteration count if \ndecay_on_iteration\n is set to true.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.Fixed\n \n \nType\n.\n\n\nMomentum.Fixed\n\n\n\n\nFixed momentum scheduler always returns the same value.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Momentum.Null\n \n \nType\n.\n\n\nMomentum.Null\n\n\n\n\nThe null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.\n\n\nsource\n\n\n\n\nBuilt-in optimizers\n\n\n\n\nStochastic Gradient Descent\n\n\n#\n\n\nMXNet.mx.SGD\n \n \nType\n.\n\n\nSGD\n\n\n\n\nStochastic gradient descent optimizer.\n\n\nSGD(; kwargs...)\n\n\n\n\nArguments:\n\n\n\n\nlr::Real\n: default \n0.01\n, learning rate.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a      dynamic learning rate scheduler. If set, will overwrite the \nlr\n      parameter.\n\n\nmomentum::Real\n: default \n0.0\n, the momentum.\n\n\nmomentum_scheduler::AbstractMomentumScheduler\n: default \nnothing\n,      a dynamic momentum scheduler. If set, will overwrite the \nmomentum\n      parameter.\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient      into the bounded range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.0001\n, weight decay is equivalent to      adding a global l2 regularizer to the parameters.\n\n\n\n\nsource\n\n\n\n\nADAM\n\n\n#\n\n\nMXNet.mx.ADAM\n \n \nType\n.\n\n\n ADAM\n\n\n\n\nThe solver described in Diederik Kingma, Jimmy Ba: \nAdam: A Method for Stochastic Optimization\n. arXiv:1412.6980 [cs.LG].\n\n\nADAM(; kwargs...)\n\n\n\n\n\n\nlr::Real\n: default \n0.001\n, learning rate.\n\n\nlr_scheduler::AbstractLearningRateScheduler\n: default \nnothing\n, a      dynamic learning rate scheduler. If set, will overwrite the \nlr\n      parameter.\n\n\nbeta1::Real\n: default \n0.9\n.\n\n\nbeta2::Real\n: default \n0.999\n.\n\n\nepsilon::Real\n: default \n1e-8\n.\n\n\ngrad_clip::Real\n: default \n0\n, if positive, will clip the gradient      into the range \n[-grad_clip, grad_clip]\n.\n\n\nweight_decay::Real\n: default \n0.00001\n, weight decay is equivalent      to adding a global l2 regularizer for all the parameters.\n\n\n\n\nsource", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#optimizers", 
            "text": "#  MXNet.mx.AbstractLearningRateScheduler     Type .  AbstractLearningRateScheduler  Base type for all learning rate scheduler.  source  #  MXNet.mx.AbstractMomentumScheduler     Type .  AbstractMomentumScheduler  Base type for all momentum scheduler.  source  #  MXNet.mx.AbstractOptimizer     Type .  AbstractOptimizer  Base type for all optimizers.  source  #  MXNet.mx.AbstractOptimizerOptions     Type .  AbstractOptimizerOptions  Base class for all optimizer options.  source  #  MXNet.mx.OptimizationState     Type .  OptimizationState  Attributes:   batch_size : The size of the mini-batch used in stochastic training.  curr_epoch : The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.  curr_batch : The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.  curr_iter : The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does  not  reset in each epoch. So it track the  total  number of mini-batches seen so far.   source  #  MXNet.mx.get_learning_rate     Function .  get_learning_rate(scheduler, state)  Arguments   scheduler::AbstractLearningRateScheduler : a learning rate scheduler.  state::OptimizationState : the current state about epoch, mini-batch and iteration count.   Returns the current learning rate.  source  #  MXNet.mx.get_momentum     Function .  get_momentum(scheduler, state)   scheduler::AbstractMomentumScheduler : the momentum scheduler.  state::OptimizationState : the state about current epoch, mini-batch and iteration count.   Returns the current momentum.  source  #  MXNet.mx.get_updater     Method .  get_updater(optimizer)  A utility function to create an updater function, that uses its closure to store all the states needed for each weights.   optimizer::AbstractOptimizer : the underlying optimizer.   source  #  MXNet.mx.normalized_gradient     Method .  normalized_gradient(opts, state, grad)   opts::AbstractOptimizerOptions : options for the optimizer, should contain the field          grad_scale ,  grad_clip  and  weight_decay .  state::OptimizationState : the current optimization state.  weight::NDArray : the trainable weights.   grad::NDArray : the original gradient of the weights.  Get the properly normalized gradient (re-scaled and clipped if necessary).    source  #  MXNet.mx.LearningRate.Exp     Type .  LearningRate.Exp  $\u001bta_t = \u001bta_0gamma^t$. Here $t$ is the epoch count, or the iteration count if  decay_on_iteration  is set to true.  source  #  MXNet.mx.LearningRate.Fixed     Type .  LearningRate.Fixed  Fixed learning rate scheduler always return the same learning rate.  source  #  MXNet.mx.LearningRate.Inv     Type .  LearningRate.Inv  $\u001bta_t = \u001bta_0 * (1 + gamma * t)^(-power)$. Here $t$ is the epoch count, or the iteration count if  decay_on_iteration  is set to true.  source  #  MXNet.mx.Momentum.Fixed     Type .  Momentum.Fixed  Fixed momentum scheduler always returns the same value.  source  #  MXNet.mx.Momentum.Null     Type .  Momentum.Null  The null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.  source", 
            "title": "Optimizers"
        }, 
        {
            "location": "/api/optimizer/#built-in-optimizers", 
            "text": "", 
            "title": "Built-in optimizers"
        }, 
        {
            "location": "/api/optimizer/#stochastic-gradient-descent", 
            "text": "#  MXNet.mx.SGD     Type .  SGD  Stochastic gradient descent optimizer.  SGD(; kwargs...)  Arguments:   lr::Real : default  0.01 , learning rate.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a      dynamic learning rate scheduler. If set, will overwrite the  lr       parameter.  momentum::Real : default  0.0 , the momentum.  momentum_scheduler::AbstractMomentumScheduler : default  nothing ,      a dynamic momentum scheduler. If set, will overwrite the  momentum       parameter.  grad_clip::Real : default  0 , if positive, will clip the gradient      into the bounded range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.0001 , weight decay is equivalent to      adding a global l2 regularizer to the parameters.   source", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/api/optimizer/#adam", 
            "text": "#  MXNet.mx.ADAM     Type .   ADAM  The solver described in Diederik Kingma, Jimmy Ba:  Adam: A Method for Stochastic Optimization . arXiv:1412.6980 [cs.LG].  ADAM(; kwargs...)   lr::Real : default  0.001 , learning rate.  lr_scheduler::AbstractLearningRateScheduler : default  nothing , a      dynamic learning rate scheduler. If set, will overwrite the  lr       parameter.  beta1::Real : default  0.9 .  beta2::Real : default  0.999 .  epsilon::Real : default  1e-8 .  grad_clip::Real : default  0 , if positive, will clip the gradient      into the range  [-grad_clip, grad_clip] .  weight_decay::Real : default  0.00001 , weight decay is equivalent      to adding a global l2 regularizer for all the parameters.   source", 
            "title": "ADAM"
        }, 
        {
            "location": "/api/callback/", 
            "text": "Callback in training\n\n\n#\n\n\nMXNet.mx.AbstractBatchCallback\n \n \nType\n.\n\n\nAbstractBatchCallback\n\n\n\n\nAbstract type of callbacks to be called every mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractCallback\n \n \nType\n.\n\n\nAbstractCallback\n\n\n\n\nAbstract type of callback functions used in training.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEpochCallback\n \n \nType\n.\n\n\nAbstractEpochCallback\n\n\n\n\nAbstract type of callbacks to be called every epoch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.do_checkpoint\n \n \nMethod\n.\n\n\ndo_checkpoint(prefix; frequency=1, save_epoch_0=false)\n\n\n\n\nCreate an \nAbstractEpochCallback\n that save checkpoints of the model to disk. The checkpoints can be loaded back later on.\n\n\nArguments\n\n\n\n\nprefix::AbstractString\n: the prefix of the filenames to save the model. The model         architecture will be saved to prefix-symbol.json, while the weights will be saved         to prefix-0012.params, for example, for the 12-th epoch.\n\n\nfrequency::Int\n: keyword argument, default 1. The frequency (measured in epochs) to         save checkpoints.\n\n\nsave_epoch_0::Bool\n: keyword argument, default false. Whether we should save a         checkpoint for epoch 0 (model initialized but not seen any data yet).\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_batch\n \n \nMethod\n.\n\n\nevery_n_batch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every \nn\n mini-batches.\n\n\nArguments\n\n\n\n\ncall_on_0::Bool\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on batch 0.\n\n\n\n\nFor example, the \nspeedometer\n callback is defined as\n\n\nevery_n_iter(frequency, call_on_0=true) do state :: OptimizationState\n  if state.curr_batch == 0\n    # reset timer\n  else\n    # compute and print speed\n  end\nend\n\n\n\n\nSee also \nevery_n_epoch\n and \nspeedometer\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.every_n_epoch\n \n \nMethod\n.\n\n\nevery_n_epoch(callback :: Function, n :: Int; call_on_0 = false)\n\n\n\n\nA convenient function to construct a callback that runs every \nn\n full data-passes.\n\n\n\n\ncall_on_0::Int\n: keyword argument, default false. Unless set, the callback         will \nnot\n be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.\n\n\n\n\nSee also \nevery_n_iter\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.speedometer\n \n \nMethod\n.\n\n\nspeedometer(; frequency=50)\n\n\n\n\nCreate an \nAbstractBatchCallback\n that measure the training speed    (number of samples processed per second) every k mini-batches.\n\n\nArguments\n\n\n\n\nfrequency::Int\n: keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.\n\n\n\n\nsource", 
            "title": "Callbacks in training"
        }, 
        {
            "location": "/api/callback/#callback-in-training", 
            "text": "#  MXNet.mx.AbstractBatchCallback     Type .  AbstractBatchCallback  Abstract type of callbacks to be called every mini-batch.  source  #  MXNet.mx.AbstractCallback     Type .  AbstractCallback  Abstract type of callback functions used in training.  source  #  MXNet.mx.AbstractEpochCallback     Type .  AbstractEpochCallback  Abstract type of callbacks to be called every epoch.  source  #  MXNet.mx.do_checkpoint     Method .  do_checkpoint(prefix; frequency=1, save_epoch_0=false)  Create an  AbstractEpochCallback  that save checkpoints of the model to disk. The checkpoints can be loaded back later on.  Arguments   prefix::AbstractString : the prefix of the filenames to save the model. The model         architecture will be saved to prefix-symbol.json, while the weights will be saved         to prefix-0012.params, for example, for the 12-th epoch.  frequency::Int : keyword argument, default 1. The frequency (measured in epochs) to         save checkpoints.  save_epoch_0::Bool : keyword argument, default false. Whether we should save a         checkpoint for epoch 0 (model initialized but not seen any data yet).   source  #  MXNet.mx.every_n_batch     Method .  every_n_batch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every  n  mini-batches.  Arguments   call_on_0::Bool : keyword argument, default false. Unless set, the callback         will  not  be run on batch 0.   For example, the  speedometer  callback is defined as  every_n_iter(frequency, call_on_0=true) do state :: OptimizationState\n  if state.curr_batch == 0\n    # reset timer\n  else\n    # compute and print speed\n  end\nend  See also  every_n_epoch  and  speedometer .  source  #  MXNet.mx.every_n_epoch     Method .  every_n_epoch(callback :: Function, n :: Int; call_on_0 = false)  A convenient function to construct a callback that runs every  n  full data-passes.   call_on_0::Int : keyword argument, default false. Unless set, the callback         will  not  be run on epoch 0. Epoch 0 means no training has been performed         yet. This is useful if you want to inspect the randomly initialized model         that has not seen any data yet.   See also  every_n_iter .  source  #  MXNet.mx.speedometer     Method .  speedometer(; frequency=50)  Create an  AbstractBatchCallback  that measure the training speed    (number of samples processed per second) every k mini-batches.  Arguments   frequency::Int : keyword argument, default 50. The frequency (number of         min-batches) to measure and report the speed.   source", 
            "title": "Callback in training"
        }, 
        {
            "location": "/api/metric/", 
            "text": "Evaluation Metrics\n\n\nEvaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.\n\n\n#\n\n\nMXNet.mx.ACE\n \n \nType\n.\n\n\nACE\n\n\n\n\nCalculates the averaged cross-entropy (logloss) for classification.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.AbstractEvalMetric\n \n \nType\n.\n\n\nAbstractEvalMetric\n\n\n\n\nThe base class for all evaluation metrics. The sub-types should implement the following interfaces:\n\n\n\n\nupdate!\n\n\nreset!\n\n\nget\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Accuracy\n \n \nType\n.\n\n\nAccuracy\n\n\n\n\nMulticlass classification accuracy.\n\n\nCalculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MSE\n \n \nType\n.\n\n\nMSE\n\n\n\n\nMean Squared Error. TODO: add support for multi-dimensional outputs.\n\n\nCalculates the mean squared error regression loss in one dimension.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiACE\n \n \nType\n.\n\n\nMultiACE\n\n\n\n\nCalculates the averaged cross-entropy per class and overall (see \nACE\n). This can be used to quantify the influence of different classes on the overall loss.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MultiMetric\n \n \nType\n.\n\n\nMultiMetric(metrics::Vector{AbstractEvalMetric})\n\n\n\n\nCombine multiple metrics in one and get a result for all of them.\n\n\nUsage\n\n\nTo calculate both mean-squared error \nAccuracy\n and log-loss \nACE\n:\n\n\n  mx.fit(..., eval_metric = mx.MultiMetric([mx.Accuracy(), mx.ACE()]))\n\n\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nMethod\n.\n\n\nget(metric)\n\n\n\n\nGet the accumulated metrics.\n\n\nReturns \nVector{Tuple{Base.Symbol, Real}}\n, a list of name-value pairs. For example, \n[(:accuracy, 0.9)]\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.reset!\n \n \nMethod\n.\n\n\nreset!(metric)\n\n\n\n\nReset the accumulation counter.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.update!\n \n \nMethod\n.\n\n\nupdate!(metric, labels, preds)\n\n\n\n\nUpdate and accumulate metrics.\n\n\nArguments:\n\n\n\n\nmetric::AbstractEvalMetric\n: the metric object.\n\n\nlabels::Vector{NDArray}\n: the labels from the data provider.\n\n\npreds::Vector{NDArray}\n: the outputs (predictions) of the network.\n\n\n\n\nsource", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/metric/#evaluation-metrics", 
            "text": "Evaluation metrics provide a way to evaluate the performance of a learned model. This is typically used during training to monitor performance on the validation set.  #  MXNet.mx.ACE     Type .  ACE  Calculates the averaged cross-entropy (logloss) for classification.  source  #  MXNet.mx.AbstractEvalMetric     Type .  AbstractEvalMetric  The base class for all evaluation metrics. The sub-types should implement the following interfaces:   update!  reset!  get   source  #  MXNet.mx.Accuracy     Type .  Accuracy  Multiclass classification accuracy.  Calculates the mean accuracy per sample for softmax in one dimension. For a multi-dimensional softmax the mean accuracy over all dimensions is calculated.  source  #  MXNet.mx.MSE     Type .  MSE  Mean Squared Error. TODO: add support for multi-dimensional outputs.  Calculates the mean squared error regression loss in one dimension.  source  #  MXNet.mx.MultiACE     Type .  MultiACE  Calculates the averaged cross-entropy per class and overall (see  ACE ). This can be used to quantify the influence of different classes on the overall loss.  source  #  MXNet.mx.MultiMetric     Type .  MultiMetric(metrics::Vector{AbstractEvalMetric})  Combine multiple metrics in one and get a result for all of them.  Usage  To calculate both mean-squared error  Accuracy  and log-loss  ACE :    mx.fit(..., eval_metric = mx.MultiMetric([mx.Accuracy(), mx.ACE()]))  source  #  Base.get     Method .  get(metric)  Get the accumulated metrics.  Returns  Vector{Tuple{Base.Symbol, Real}} , a list of name-value pairs. For example,  [(:accuracy, 0.9)] .  source  #  MXNet.mx.reset!     Method .  reset!(metric)  Reset the accumulation counter.  source  #  MXNet.mx.update!     Method .  update!(metric, labels, preds)  Update and accumulate metrics.  Arguments:   metric::AbstractEvalMetric : the metric object.  labels::Vector{NDArray} : the labels from the data provider.  preds::Vector{NDArray} : the outputs (predictions) of the network.   source", 
            "title": "Evaluation Metrics"
        }, 
        {
            "location": "/api/io/", 
            "text": "Data Providers\n\n\nData providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.\n\n\n\n\nAbstractDataProvider interface\n\n\n#\n\n\nMXNet.mx.AbstractDataProvider\n \n \nType\n.\n\n\nAbstractDataProvider\n\n\n\n\nThe root type for all data provider. A data provider should implement the following interfaces:\n\n\n\n\nget_batch_size\n\n\nprovide_data\n\n\nprovide_label\n\n\n\n\nAs well as the Julia iterator interface (see \nthe Julia manual\n). Normally this involves defining:\n\n\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\nBase.done(provider, state) -\n Bool\n\n\nBase.next(provider, state) -\n (AbstractDataBatch, AbstractDataProvider)\n\n\n\n\nsource\n\n\nThe difference between \ndata\n and \nlabel\n is that during training stage, both \ndata\n and \nlabel\n will be feeded into the model, while during prediction stage, only \ndata\n is loaded. Otherwise, they could be anything, with any names, and of any shapes. The provided data and label names here should match the input names in a target \nSymbolicNode\n.\n\n\nA data provider should also implement the Julia iteration interface, in order to allow iterating through the data set. The provider will be called in the following way:\n\n\nfor batch in eachbatch(provider)\n    data = get_data(provider, batch)\nend\n\n\n\n\nwhich will be translated by Julia compiler into\n\n\nstate = Base.start(eachbatch(provider))\nwhile !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\nend\n\n\n\n\nBy default, \neachbatch\n simply returns the provider itself, so the iterator interface is implemented on the provider type itself. But the extra layer of abstraction allows us to implement a data provider easily via a Julia \nTask\n coroutine. See the data provider defined in \nthe char-lstm example\n for an example of using coroutine to define data providers.\n\n\nThe detailed interface functions for the iterator API is listed below:\n\n\nBase.eltype(provider) -\n AbstractDataBatch\n\n\n\n\nReturns the specific subtype representing a data batch. See \nAbstractDataBatch\n. * \nprovider::AbstractDataProvider\n: the data provider.\n\n\nBase.start(provider) -\n AbstractDataProviderState\n\n\n\n\nThis function is always called before iterating into the dataset. It should initialize the iterator, reset the index, and do data shuffling if needed. * \nprovider::AbstractDataProvider\n: the data provider.\n\n\nBase.done(provider, state) -\n Bool\n\n\n\n\nTrue if there is no more data to iterate in this dataset. * \nprovider::AbstractDataProvider\n: the data provider. * \nstate::AbstractDataProviderState\n: the state returned by \nBase.start\n and \nBase.next\n.\n\n\nBase.next(provider) -\n (AbstractDataBatch, AbstractDataProviderState)\n\n\n\n\nReturns the current data batch, and the state for the next iteration. * \nprovider::AbstractDataProvider\n: the data provider.\n\n\nNote sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that\n\n\n\n\nBase.start\n will always be called, and called only once before the iteration starts.\n\n\nBase.done\n will always be called at the beginning of every iteration and always be called once.\n\n\nIf \nBase.done\n return true, the iteration will stop, until the next round, again, starting with a call to \nBase.start\n.\n\n\nBase.next\n will always be called only once in each iteration. It will always be called after one and only one call to \nBase.done\n; but if \nBase.done\n returns true, \nBase.next\n will not be called.\n\n\n\n\nWith those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in \nMXDataProvider\n for example.\n\n\n\n\nNote\n\n\nPlease do not use the one data provider simultaneously in two different places, either in parallel, or in a nested loop. For example, the behavior for the following code is undefined\n\n\n```julia\nfor batch in data\n    # updating the parameters\n\n\n# now let's test the performance on the training set\nfor b2 in data\n    # ...\nend\n\n\n\nend\n```\n\n\n\n\n#\n\n\nMXNet.mx.get_batch_size\n \n \nFunction\n.\n\n\nget_batch_size(provider) -\n Int\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns the mini-batch size of the provided data. All the provided data should have the same mini-batch size (i.e. the last dimension).\n\n\nsource\n\n\n#\n\n\nMXNet.mx.provide_data\n \n \nFunction\n.\n\n\nprovide_data(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns a vector of (name, shape) pairs describing the names of the data it provides, and the corresponding shapes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.provide_label\n \n \nFunction\n.\n\n\nprovide_label(provider) -\n Vector{Tuple{Base.Symbol, Tuple}}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\n\n\nReturns a vector of (name, shape) pairs describing the names of the labels it provides, and the corresponding shapes.\n\n\nsource\n\n\n\n\nAbstractDataBatch interface\n\n\n#\n\n\nMXNet.mx.AbstractDataProviderState\n \n \nType\n.\n\n\nAbstractDataProviderState\n\n\n\n\nBase type for data provider states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.count_samples\n \n \nFunction\n.\n\n\ncount_samples(provider, batch) -\n Int\n\n\n\n\nArguments:\n\n\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns the number of samples in this batch. This number should be greater than 0, but less than or equal to the batch size. This is used to indicate at the end of the data set, there might not be enough samples for a whole mini-batch.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_data\n \n \nFunction\n.\n\n\nget_data(provider, batch) -\n Vector{NDArray}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns a vector of data in this batch, should be in the same order as declared in \nprovide_data() \nAbstractDataProvider.provide_data\n.\n\n\nThe last dimension of each \nNDArray\n should always match the batch_size, even when \ncount_samples\n returns a value less than the batch size. In this case,      the data provider is free to pad the remaining contents with any value.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_label\n \n \nFunction\n.\n\n\nget_label(provider, batch) -\n Vector{NDArray}\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\n\n\nReturns a vector of labels in this batch. Similar to \nget_data\n.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nFunction\n.\n\n\nget(metric)\n\n\n\n\nGet the accumulated metrics.\n\n\nReturns \nVector{Tuple{Base.Symbol, Real}}\n, a list of name-value pairs. For example, \n[(:accuracy, 0.9)]\n.\n\n\nsource\n\n\nget(provider, batch, name) -\n NDArray\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\nname::Symbol\n: the name of the data to get, should be one of the names provided in either \nprovide_data() \nAbstractDataProvider.provide_data\n or \nprovide_label() \nAbstractDataProvider.provide_label\n.\n\n\n\n\nReturns the corresponding data array corresponding to that name.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_data!\n \n \nFunction\n.\n\n\nload_data!(provider, batch, targets)\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load data into.\n\n\n\n\nThe targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of \nSlicedNDArray\n. This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where \nslice\n specify the range of samples in the mini-batch that should be loaded into the corresponding \nndarray\n.\n\n\nThis utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_label!\n \n \nFunction\n.\n\n\nload_label!(provider, batch, targets)\n\n\n\n\n\n\nprovider::AbstractDataProvider provider\n: the data provider.\n\n\nbatch::AbstractDataBatch batch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load label into.\n\n\n\n\nThe same as \nload_data!\n, except that this is for loading labels.\n\n\nsource\n\n\n\n\nImplemented providers and other methods\n\n\n#\n\n\nMXNet.mx.AbstractDataBatch\n \n \nType\n.\n\n\nAbstractDataBatch\n\n\n\n\nBase type for a data mini-batch. It should implement the following interfaces:\n\n\n\n\ncount_samples\n\n\nget_data\n\n\nget_label\n\n\n\n\nThe following utility functions will be automatically defined:\n\n\n\n\nget\n\n\nload_data!\n\n\nload_label!\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ArrayDataProvider\n \n \nType\n.\n\n\nArrayDataProvider\n\n\n\n\nA convenient tool to iterate \nNDArray\n or Julia \nArray\n.\n\n\nArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)\n\n\n\n\nConstruct a data provider from \nNDArray\n or Julia Arrays.\n\n\nArguments:\n\n\n\n\ndata\n: the data, could be * a \nNDArray\n, or a Julia Array. This is equivalent to \n:data =\n data\n. * a name-data pair, like \n:mydata =\n array\n, where \n:mydata\n is the name of the data * and \narray\n is an \nNDArray\n or a Julia Array. * a list of name-data pairs.\n\n\nlabel\n: the same as the \ndata\n parameter. When this argument is omitted, the constructed provider will provide no labels.\n\n\nbatch_size::Int\n: the batch size, default is 0, which means treating the whole array as a single mini-batch.\n\n\nshuffle::Bool\n: turn on if the data should be shuffled at every epoch.\n\n\ndata_padding::Real\n: when the mini-batch goes beyond the dataset boundary, there might be less samples to include than a mini-batch. This value specify a scalar to pad the contents of all the missing data points.\n\n\nlabel_padding::Real\n: the same as \ndata_padding\n, except for the labels.\n\n\n\n\nTODO: remove \ndata_padding\n and \nlabel_padding\n, and implement rollover that copies the last or first several training samples to feed the padding.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.DataBatch\n \n \nType\n.\n\n\nDataBatch\n\n\n\n\nA basic subclass of \nAbstractDataBatch\n, that implement the interface by accessing member fields.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MXDataProvider\n \n \nType\n.\n\n\nMXDataProvider\n\n\n\n\nA data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SlicedNDArray\n \n \nType\n.\n\n\nSlicedNDArray\n\n\n\n\nA alias type of \nTuple{UnitRange{Int},NDArray}\n.\n\n\nsource\n\n\n#\n\n\nBase.get\n \n \nMethod\n.\n\n\nget(provider, batch, name) -\n NDArray\n\n\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\nname::Symbol\n: the name of the data to get, should be one of the names provided in either \nprovide_data() \nAbstractDataProvider.provide_data\n or \nprovide_label() \nAbstractDataProvider.provide_label\n.\n\n\n\n\nReturns the corresponding data array corresponding to that name.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.CSVIter\n \n \nMethod\n.\n\n\nCSVIter(data_csv, data_shape, label_csv, label_shape)\n\n\n\n\nCan also be called with the alias \nCSVProvider\n. Create iterator for dataset in csv.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\ndata_csv::string, required\n: Dataset Param: Data csv path.\n\n\ndata_shape::Shape(tuple), required\n: Dataset Param: Shape of the data.\n\n\nlabel_csv::string, optional, default='NULL'\n: Dataset Param: Label csv path. If is NULL, all labels will be returned as 0\n\n\nlabel_shape::Shape(tuple), optional, default=(1,)\n: Dataset Param: Shape of the label.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ImageRecordIter\n \n \nMethod\n.\n\n\nImageRecordIter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, rand_crop, crop_y_start, crop_x_start, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, scale, max_random_contrast, max_random_illumination, verbose)\n\n\n\n\nCan also be called with the alias \nImageRecordProvider\n. Create iterator for dataset packed in recordio.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\npath_imglist::string, optional, default=''\n: Dataset Param: Path to image list.\n\n\npath_imgrec::string, optional, default='./data/imgrec.rec'\n: Dataset Param: Path to image record file.\n\n\naug_seq::string, optional, default='aug_default'\n: Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.\n\n\nlabel_width::int, optional, default='1'\n: Dataset Param: How many labels for an image.\n\n\ndata_shape::Shape(tuple), required\n: Dataset Param: Shape of each instance generated by the DataIter.\n\n\npreprocess_threads::int, optional, default='4'\n: Backend Param: Number of thread to do preprocessing.\n\n\nverbose::boolean, optional, default=True\n: Auxiliary Param: Whether to output parser information.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nshuffle::boolean, optional, default=False\n: Augmentation Param: Whether to shuffle data.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nbatch_size::int (non-negative), required\n: Batch Param: Batch size.\n\n\nround_batch::boolean, optional, default=True\n: Batch Param: Use round robin to handle overflow batch.\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Backend Param: Number of prefetched parameters\n\n\ndtype::{'float16', 'float32', 'float64'},optional, default='float32'\n: Data type.\n\n\nrand_crop::boolean, optional, default=False\n: Augmentation Param: Whether to random crop on the image\n\n\ncrop_y_start::int, optional, default='-1'\n: Augmentation Param: Where to nonrandom crop on y.\n\n\ncrop_x_start::int, optional, default='-1'\n: Augmentation Param: Where to nonrandom crop on x.\n\n\nmax_rotate_angle::int, optional, default='0'\n: Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].\n\n\nmax_aspect_ratio::float, optional, default=0\n: Augmentation Param: denotes the max ratio of random aspect ratio augmentation.\n\n\nmax_shear_ratio::float, optional, default=0\n: Augmentation Param: denotes the max random shearing ratio.\n\n\nmax_crop_size::int, optional, default='-1'\n: Augmentation Param: Maximum crop size.\n\n\nmin_crop_size::int, optional, default='-1'\n: Augmentation Param: Minimum crop size.\n\n\nmax_random_scale::float, optional, default=1\n: Augmentation Param: Maxmum scale ratio.\n\n\nmin_random_scale::float, optional, default=1\n: Augmentation Param: Minimum scale ratio.\n\n\nmax_img_size::float, optional, default=1e+10\n: Augmentation Param: Maxmum image size after resizing.\n\n\nmin_img_size::float, optional, default=0\n: Augmentation Param: Minimum image size after resizing.\n\n\nrandom_h::int, optional, default='0'\n: Augmentation Param: Maximum value of H channel in HSL color space.\n\n\nrandom_s::int, optional, default='0'\n: Augmentation Param: Maximum value of S channel in HSL color space.\n\n\nrandom_l::int, optional, default='0'\n: Augmentation Param: Maximum value of L channel in HSL color space.\n\n\nrotate::int, optional, default='-1'\n: Augmentation Param: Rotate angle.\n\n\nfill_value::int, optional, default='255'\n: Augmentation Param: Maximum value of illumination variation.\n\n\ninter_method::int, optional, default='1'\n: Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.\n\n\npad::int, optional, default='0'\n: Augmentation Param: Padding size.\n\n\nmirror::boolean, optional, default=False\n: Augmentation Param: Whether to mirror the image.\n\n\nrand_mirror::boolean, optional, default=False\n: Augmentation Param: Whether to mirror the image randomly.\n\n\nmean_img::string, optional, default=''\n: Augmentation Param: Mean Image to be subtracted.\n\n\nmean_r::float, optional, default=0\n: Augmentation Param: Mean value on R channel.\n\n\nmean_g::float, optional, default=0\n: Augmentation Param: Mean value on G channel.\n\n\nmean_b::float, optional, default=0\n: Augmentation Param: Mean value on B channel.\n\n\nmean_a::float, optional, default=0\n: Augmentation Param: Mean value on Alpha channel.\n\n\nscale::float, optional, default=1\n: Augmentation Param: Scale in color space.\n\n\nmax_random_contrast::float, optional, default=0\n: Augmentation Param: Maximum ratio of contrast variation.\n\n\nmax_random_illumination::float, optional, default=0\n: Augmentation Param: Maximum value of illumination variation.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MNISTIter\n \n \nMethod\n.\n\n\nMNISTIter(image, label, batch_size, shuffle, flat, seed, silent, num_parts, part_index, prefetch_buffer, dtype)\n\n\n\n\nCan also be called with the alias \nMNISTProvider\n. Create iterator for MNIST hand-written digit number recognition dataset.\n\n\nArguments:\n\n\n\n\ndata_name::Symbol\n: keyword argument, default \n:data\n. The name of the data.\n\n\nlabel_name::Symbol\n: keyword argument, default \n:softmax_label\n. The name of the label. Could be \nnothing\n if no label is presented in this dataset.\n\n\nimage::string, optional, default='./train-images-idx3-ubyte'\n: Dataset Param: Mnist image path.\n\n\nlabel::string, optional, default='./train-labels-idx1-ubyte'\n: Dataset Param: Mnist label path.\n\n\nbatch_size::int, optional, default='128'\n: Batch Param: Batch Size.\n\n\nshuffle::boolean, optional, default=True\n: Augmentation Param: Whether to shuffle data.\n\n\nflat::boolean, optional, default=False\n: Augmentation Param: Whether to flat the data into 1D.\n\n\nseed::int, optional, default='0'\n: Augmentation Param: Random Seed.\n\n\nsilent::boolean, optional, default=False\n: Auxiliary Param: Whether to print out data info.\n\n\nnum_parts::int, optional, default='1'\n: partition the data into multiple parts\n\n\npart_index::int, optional, default='0'\n: the index of the part will read\n\n\nprefetch_buffer::long (non-negative), optional, default=4\n: Backend Param: Number of prefetched parameters\n\n\ndtype::{'float16', 'float32', 'float64'},optional, default='float32'\n: Data type.\n\n\n\n\nReturns the constructed \nMXDataProvider\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_data!\n \n \nMethod\n.\n\n\nload_data!(provider, batch, targets)\n\n\n\n\nArguments:\n\n\n\n\nprovider::AbstractDataProvider\n: the data provider.\n\n\nbatch::AbstractDataBatch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load data into.\n\n\n\n\nThe targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of \nSlicedNDArray\n. This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where \nslice\n specify the range of samples in the mini-batch that should be loaded into the corresponding \nndarray\n.\n\n\nThis utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load_label!\n \n \nMethod\n.\n\n\nload_label!(provider, batch, targets)\n\n\n\n\n\n\nprovider::AbstractDataProvider provider\n: the data provider.\n\n\nbatch::AbstractDataBatch batch\n: the data batch object.\n\n\ntargets::Vector{Vector{SlicedNDArray}}\n: the targets to load label into.\n\n\n\n\nThe same as \nload_data!\n, except that this is for loading labels.\n\n\nsource", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#data-providers", 
            "text": "Data providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.", 
            "title": "Data Providers"
        }, 
        {
            "location": "/api/io/#abstractdataprovider-interface", 
            "text": "#  MXNet.mx.AbstractDataProvider     Type .  AbstractDataProvider  The root type for all data provider. A data provider should implement the following interfaces:   get_batch_size  provide_data  provide_label   As well as the Julia iterator interface (see  the Julia manual ). Normally this involves defining:   Base.eltype(provider) -  AbstractDataBatch  Base.start(provider) -  AbstractDataProviderState  Base.done(provider, state) -  Bool  Base.next(provider, state) -  (AbstractDataBatch, AbstractDataProvider)   source  The difference between  data  and  label  is that during training stage, both  data  and  label  will be feeded into the model, while during prediction stage, only  data  is loaded. Otherwise, they could be anything, with any names, and of any shapes. The provided data and label names here should match the input names in a target  SymbolicNode .  A data provider should also implement the Julia iteration interface, in order to allow iterating through the data set. The provider will be called in the following way:  for batch in eachbatch(provider)\n    data = get_data(provider, batch)\nend  which will be translated by Julia compiler into  state = Base.start(eachbatch(provider))\nwhile !Base.done(provider, state)\n    (batch, state) = Base.next(provider, state)\n    data = get_data(provider, batch)\nend  By default,  eachbatch  simply returns the provider itself, so the iterator interface is implemented on the provider type itself. But the extra layer of abstraction allows us to implement a data provider easily via a Julia  Task  coroutine. See the data provider defined in  the char-lstm example  for an example of using coroutine to define data providers.  The detailed interface functions for the iterator API is listed below:  Base.eltype(provider) -  AbstractDataBatch  Returns the specific subtype representing a data batch. See  AbstractDataBatch . *  provider::AbstractDataProvider : the data provider.  Base.start(provider) -  AbstractDataProviderState  This function is always called before iterating into the dataset. It should initialize the iterator, reset the index, and do data shuffling if needed. *  provider::AbstractDataProvider : the data provider.  Base.done(provider, state) -  Bool  True if there is no more data to iterate in this dataset. *  provider::AbstractDataProvider : the data provider. *  state::AbstractDataProviderState : the state returned by  Base.start  and  Base.next .  Base.next(provider) -  (AbstractDataBatch, AbstractDataProviderState)  Returns the current data batch, and the state for the next iteration. *  provider::AbstractDataProvider : the data provider.  Note sometimes you are wrapping an existing data iterator (e.g. the built-in libmxnet data iterator) that is built with a different convention. It might be difficult to adapt to the interfaces stated here. In this case, you can safely assume that   Base.start  will always be called, and called only once before the iteration starts.  Base.done  will always be called at the beginning of every iteration and always be called once.  If  Base.done  return true, the iteration will stop, until the next round, again, starting with a call to  Base.start .  Base.next  will always be called only once in each iteration. It will always be called after one and only one call to  Base.done ; but if  Base.done  returns true,  Base.next  will not be called.   With those assumptions, it will be relatively easy to adapt any existing iterator. See the implementation of the built-in  MXDataProvider  for example.   Note  Please do not use the one data provider simultaneously in two different places, either in parallel, or in a nested loop. For example, the behavior for the following code is undefined  ```julia\nfor batch in data\n    # updating the parameters  # now let's test the performance on the training set\nfor b2 in data\n    # ...\nend  end\n```   #  MXNet.mx.get_batch_size     Function .  get_batch_size(provider) -  Int  Arguments:   provider::AbstractDataProvider : the data provider.   Returns the mini-batch size of the provided data. All the provided data should have the same mini-batch size (i.e. the last dimension).  source  #  MXNet.mx.provide_data     Function .  provide_data(provider) -  Vector{Tuple{Base.Symbol, Tuple}}  Arguments:   provider::AbstractDataProvider : the data provider.   Returns a vector of (name, shape) pairs describing the names of the data it provides, and the corresponding shapes.  source  #  MXNet.mx.provide_label     Function .  provide_label(provider) -  Vector{Tuple{Base.Symbol, Tuple}}  Arguments:   provider::AbstractDataProvider : the data provider.   Returns a vector of (name, shape) pairs describing the names of the labels it provides, and the corresponding shapes.  source", 
            "title": "AbstractDataProvider interface"
        }, 
        {
            "location": "/api/io/#abstractdatabatch-interface", 
            "text": "#  MXNet.mx.AbstractDataProviderState     Type .  AbstractDataProviderState  Base type for data provider states.  source  #  MXNet.mx.count_samples     Function .  count_samples(provider, batch) -  Int  Arguments:   batch::AbstractDataBatch : the data batch object.   Returns the number of samples in this batch. This number should be greater than 0, but less than or equal to the batch size. This is used to indicate at the end of the data set, there might not be enough samples for a whole mini-batch.  source  #  MXNet.mx.get_data     Function .  get_data(provider, batch) -  Vector{NDArray}  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.   Returns a vector of data in this batch, should be in the same order as declared in  provide_data()  AbstractDataProvider.provide_data .  The last dimension of each  NDArray  should always match the batch_size, even when  count_samples  returns a value less than the batch size. In this case,      the data provider is free to pad the remaining contents with any value.  source  #  MXNet.mx.get_label     Function .  get_label(provider, batch) -  Vector{NDArray}  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.   Returns a vector of labels in this batch. Similar to  get_data .  source  #  Base.get     Function .  get(metric)  Get the accumulated metrics.  Returns  Vector{Tuple{Base.Symbol, Real}} , a list of name-value pairs. For example,  [(:accuracy, 0.9)] .  source  get(provider, batch, name) -  NDArray   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  name::Symbol : the name of the data to get, should be one of the names provided in either  provide_data()  AbstractDataProvider.provide_data  or  provide_label()  AbstractDataProvider.provide_label .   Returns the corresponding data array corresponding to that name.  source  #  MXNet.mx.load_data!     Function .  load_data!(provider, batch, targets)  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load data into.   The targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of  SlicedNDArray . This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where  slice  specify the range of samples in the mini-batch that should be loaded into the corresponding  ndarray .  This utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.  source  #  MXNet.mx.load_label!     Function .  load_label!(provider, batch, targets)   provider::AbstractDataProvider provider : the data provider.  batch::AbstractDataBatch batch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load label into.   The same as  load_data! , except that this is for loading labels.  source", 
            "title": "AbstractDataBatch interface"
        }, 
        {
            "location": "/api/io/#implemented-providers-and-other-methods", 
            "text": "#  MXNet.mx.AbstractDataBatch     Type .  AbstractDataBatch  Base type for a data mini-batch. It should implement the following interfaces:   count_samples  get_data  get_label   The following utility functions will be automatically defined:   get  load_data!  load_label!   source  #  MXNet.mx.ArrayDataProvider     Type .  ArrayDataProvider  A convenient tool to iterate  NDArray  or Julia  Array .  ArrayDataProvider(data[, label]; batch_size, shuffle, data_padding, label_padding)  Construct a data provider from  NDArray  or Julia Arrays.  Arguments:   data : the data, could be * a  NDArray , or a Julia Array. This is equivalent to  :data =  data . * a name-data pair, like  :mydata =  array , where  :mydata  is the name of the data * and  array  is an  NDArray  or a Julia Array. * a list of name-data pairs.  label : the same as the  data  parameter. When this argument is omitted, the constructed provider will provide no labels.  batch_size::Int : the batch size, default is 0, which means treating the whole array as a single mini-batch.  shuffle::Bool : turn on if the data should be shuffled at every epoch.  data_padding::Real : when the mini-batch goes beyond the dataset boundary, there might be less samples to include than a mini-batch. This value specify a scalar to pad the contents of all the missing data points.  label_padding::Real : the same as  data_padding , except for the labels.   TODO: remove  data_padding  and  label_padding , and implement rollover that copies the last or first several training samples to feed the padding.  source  #  MXNet.mx.DataBatch     Type .  DataBatch  A basic subclass of  AbstractDataBatch , that implement the interface by accessing member fields.  source  #  MXNet.mx.MXDataProvider     Type .  MXDataProvider  A data provider that wrap built-in data iterators from libmxnet. See below for a list of built-in data iterators.  source  #  MXNet.mx.SlicedNDArray     Type .  SlicedNDArray  A alias type of  Tuple{UnitRange{Int},NDArray} .  source  #  Base.get     Method .  get(provider, batch, name) -  NDArray   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  name::Symbol : the name of the data to get, should be one of the names provided in either  provide_data()  AbstractDataProvider.provide_data  or  provide_label()  AbstractDataProvider.provide_label .   Returns the corresponding data array corresponding to that name.  source  #  MXNet.mx.CSVIter     Method .  CSVIter(data_csv, data_shape, label_csv, label_shape)  Can also be called with the alias  CSVProvider . Create iterator for dataset in csv.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  data_csv::string, required : Dataset Param: Data csv path.  data_shape::Shape(tuple), required : Dataset Param: Shape of the data.  label_csv::string, optional, default='NULL' : Dataset Param: Label csv path. If is NULL, all labels will be returned as 0  label_shape::Shape(tuple), optional, default=(1,) : Dataset Param: Shape of the label.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.ImageRecordIter     Method .  ImageRecordIter(path_imglist, path_imgrec, aug_seq, label_width, data_shape, preprocess_threads, verbose, num_parts, part_index, shuffle, seed, verbose, batch_size, round_batch, prefetch_buffer, dtype, rand_crop, crop_y_start, crop_x_start, max_rotate_angle, max_aspect_ratio, max_shear_ratio, max_crop_size, min_crop_size, max_random_scale, min_random_scale, max_img_size, min_img_size, random_h, random_s, random_l, rotate, fill_value, data_shape, inter_method, pad, seed, mirror, rand_mirror, mean_img, mean_r, mean_g, mean_b, mean_a, scale, max_random_contrast, max_random_illumination, verbose)  Can also be called with the alias  ImageRecordProvider . Create iterator for dataset packed in recordio.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  path_imglist::string, optional, default='' : Dataset Param: Path to image list.  path_imgrec::string, optional, default='./data/imgrec.rec' : Dataset Param: Path to image record file.  aug_seq::string, optional, default='aug_default' : Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.  label_width::int, optional, default='1' : Dataset Param: How many labels for an image.  data_shape::Shape(tuple), required : Dataset Param: Shape of each instance generated by the DataIter.  preprocess_threads::int, optional, default='4' : Backend Param: Number of thread to do preprocessing.  verbose::boolean, optional, default=True : Auxiliary Param: Whether to output parser information.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  shuffle::boolean, optional, default=False : Augmentation Param: Whether to shuffle data.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  batch_size::int (non-negative), required : Batch Param: Batch size.  round_batch::boolean, optional, default=True : Batch Param: Use round robin to handle overflow batch.  prefetch_buffer::long (non-negative), optional, default=4 : Backend Param: Number of prefetched parameters  dtype::{'float16', 'float32', 'float64'},optional, default='float32' : Data type.  rand_crop::boolean, optional, default=False : Augmentation Param: Whether to random crop on the image  crop_y_start::int, optional, default='-1' : Augmentation Param: Where to nonrandom crop on y.  crop_x_start::int, optional, default='-1' : Augmentation Param: Where to nonrandom crop on x.  max_rotate_angle::int, optional, default='0' : Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].  max_aspect_ratio::float, optional, default=0 : Augmentation Param: denotes the max ratio of random aspect ratio augmentation.  max_shear_ratio::float, optional, default=0 : Augmentation Param: denotes the max random shearing ratio.  max_crop_size::int, optional, default='-1' : Augmentation Param: Maximum crop size.  min_crop_size::int, optional, default='-1' : Augmentation Param: Minimum crop size.  max_random_scale::float, optional, default=1 : Augmentation Param: Maxmum scale ratio.  min_random_scale::float, optional, default=1 : Augmentation Param: Minimum scale ratio.  max_img_size::float, optional, default=1e+10 : Augmentation Param: Maxmum image size after resizing.  min_img_size::float, optional, default=0 : Augmentation Param: Minimum image size after resizing.  random_h::int, optional, default='0' : Augmentation Param: Maximum value of H channel in HSL color space.  random_s::int, optional, default='0' : Augmentation Param: Maximum value of S channel in HSL color space.  random_l::int, optional, default='0' : Augmentation Param: Maximum value of L channel in HSL color space.  rotate::int, optional, default='-1' : Augmentation Param: Rotate angle.  fill_value::int, optional, default='255' : Augmentation Param: Maximum value of illumination variation.  inter_method::int, optional, default='1' : Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.  pad::int, optional, default='0' : Augmentation Param: Padding size.  mirror::boolean, optional, default=False : Augmentation Param: Whether to mirror the image.  rand_mirror::boolean, optional, default=False : Augmentation Param: Whether to mirror the image randomly.  mean_img::string, optional, default='' : Augmentation Param: Mean Image to be subtracted.  mean_r::float, optional, default=0 : Augmentation Param: Mean value on R channel.  mean_g::float, optional, default=0 : Augmentation Param: Mean value on G channel.  mean_b::float, optional, default=0 : Augmentation Param: Mean value on B channel.  mean_a::float, optional, default=0 : Augmentation Param: Mean value on Alpha channel.  scale::float, optional, default=1 : Augmentation Param: Scale in color space.  max_random_contrast::float, optional, default=0 : Augmentation Param: Maximum ratio of contrast variation.  max_random_illumination::float, optional, default=0 : Augmentation Param: Maximum value of illumination variation.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.MNISTIter     Method .  MNISTIter(image, label, batch_size, shuffle, flat, seed, silent, num_parts, part_index, prefetch_buffer, dtype)  Can also be called with the alias  MNISTProvider . Create iterator for MNIST hand-written digit number recognition dataset.  Arguments:   data_name::Symbol : keyword argument, default  :data . The name of the data.  label_name::Symbol : keyword argument, default  :softmax_label . The name of the label. Could be  nothing  if no label is presented in this dataset.  image::string, optional, default='./train-images-idx3-ubyte' : Dataset Param: Mnist image path.  label::string, optional, default='./train-labels-idx1-ubyte' : Dataset Param: Mnist label path.  batch_size::int, optional, default='128' : Batch Param: Batch Size.  shuffle::boolean, optional, default=True : Augmentation Param: Whether to shuffle data.  flat::boolean, optional, default=False : Augmentation Param: Whether to flat the data into 1D.  seed::int, optional, default='0' : Augmentation Param: Random Seed.  silent::boolean, optional, default=False : Auxiliary Param: Whether to print out data info.  num_parts::int, optional, default='1' : partition the data into multiple parts  part_index::int, optional, default='0' : the index of the part will read  prefetch_buffer::long (non-negative), optional, default=4 : Backend Param: Number of prefetched parameters  dtype::{'float16', 'float32', 'float64'},optional, default='float32' : Data type.   Returns the constructed  MXDataProvider .  source  #  MXNet.mx.load_data!     Method .  load_data!(provider, batch, targets)  Arguments:   provider::AbstractDataProvider : the data provider.  batch::AbstractDataBatch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load data into.   The targets is a list of the same length as number of data provided by this provider. Each element in the list is a list of  SlicedNDArray . This list described a spliting scheme of this data batch into different slices, each slice is specified by a slice-ndarray pair, where  slice  specify the range of samples in the mini-batch that should be loaded into the corresponding  ndarray .  This utility function is used in data parallelization, where a mini-batch is splited and computed on several different devices.  source  #  MXNet.mx.load_label!     Method .  load_label!(provider, batch, targets)   provider::AbstractDataProvider provider : the data provider.  batch::AbstractDataBatch batch : the data batch object.  targets::Vector{Vector{SlicedNDArray}} : the targets to load label into.   The same as  load_data! , except that this is for loading labels.  source", 
            "title": "Implemented providers and other methods"
        }, 
        {
            "location": "/api/ndarray/", 
            "text": "NDArray API\n\n\n#\n\n\nMXNet.mx.NDArray\n \n \nType\n.\n\n\nNDArray\n\n\n\n\nWrapper of the \nNDArray\n type in \nlibmxnet\n. This is the basic building block of tensor-based computation.\n\n\n\n\nNote\n\n\nsince C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use \nlanguage-native\n convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).\n\n\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nMethod\n.\n\n\n*(arg0, arg1)\n\n\n\n\nCurrently only multiplication a scalar with an \nNDArray\n is implemented. Matrix multiplication is to be added soon.\n\n\nsource\n\n\n#\n\n\nBase.:+\n \n \nMethod\n.\n\n\n+(args...)\n.+(args...)\n\n\n\n\nSummation. Multiple arguments of either scalar or \nNDArray\n could be added together. Note at least the first or second argument needs to be an \nNDArray\n to avoid ambiguity of built-in summation.\n\n\nsource\n\n\n#\n\n\nBase.:-\n \n \nMethod\n.\n\n\n-(arg0, arg1)\n-(arg0)\n.-(arg0, arg1)\n\n\n\n\nSubtraction \narg0 - arg1\n, of scalar types or \nNDArray\n. Or create the negative of \narg0\n.\n\n\nsource\n\n\n#\n\n\nBase.:.*\n \n \nMethod\n.\n\n\n.*(arg0, arg1)\n\n\n\n\nElementwise multiplication of \narg0\n and \narg\n, could be either scalar or \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.:./\n \n \nMethod\n.\n\n\n./(arg0 :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise dividing an \nNDArray\n by a scalar or another \nNDArray\n of the same shape.\n\n\nsource\n\n\n#\n\n\nBase.:/\n \n \nMethod\n.\n\n\n/(arg0 :: NDArray, arg :: Real)\n\n\n\n\nDivide an \nNDArray\n by a scalar. Matrix division (solving linear systems) is not implemented yet.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.dot\n \n \nMethod\n.\n\n\ndot(lhs, rhs)\n\n\n\n\nCalculate dot product of two matrices or two vectors\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.norm\n \n \nMethod\n.\n\n\nnorm(src)\n\n\n\n\nTake L2 norm of the src.The result will be ndarray of shape (1,) on the same device.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase._div\n \n \nMethod\n.\n\n\n_div(lhs, rhs)\n\n\n\n\nMultiply lhs by rhs\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nMethod\n.\n\n\nabs(src)\n\n\n\n\nTake absolute value of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nMethod\n.\n\n\nceil(src)\n\n\n\n\nTake ceil value of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.convert\n \n \nMethod\n.\n\n\nconvert(::Type{Array{T}}, arr :: NDArray)\n\n\n\n\nConvert an \nNDArray\n into a Julia \nArray\n of specific type. Data will be copied.\n\n\nsource\n\n\n#\n\n\nBase.copy!\n \n \nMethod\n.\n\n\ncopy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})\n\n\n\n\nCopy contents of \nsrc\n into \ndst\n.\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(arr :: NDArray)\ncopy(arr :: NDArray, ctx :: Context)\ncopy(arr :: Array, ctx :: Context)\n\n\n\n\nCreate a copy of an array. When no \nContext\n is given, create a Julia \nArray\n. Otherwise, create an \nNDArray\n on the specified context.\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nMethod\n.\n\n\ncos(src)\n\n\n\n\nTake cos of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.eltype\n \n \nMethod\n.\n\n\neltype(arr :: NDArray)\n\n\n\n\nGet the element type of an \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nMethod\n.\n\n\nexp(src)\n\n\n\n\nTake exp of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nMethod\n.\n\n\nfloor(src)\n\n\n\n\nTake floor value of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(arr :: NDArray, idx)\n\n\n\n\nShortcut for \nslice\n. A typical use is to write\n\n\n  arr[:] += 5\n\n\n\n\nwhich translates into\n\n\n  arr[:] = arr[:] + 5\n\n\n\n\nwhich furthur translates into\n\n\n  setindex!(getindex(arr, Colon()), 5, Colon())\n\n\n\n\n\n\nNote\n\n\nThe behavior is quite different from indexing into Julia's \nArray\n. For example, \narr[2:5]\n create a \ncopy\n of the sub-array for Julia \nArray\n, while for \nNDArray\n, this is a \nslice\n that shares the memory.\n\n\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\nShortcut for \nslice\n. \nNOTE\n the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call \nslice\n, which shares the underlying memory.\n\n\nsource\n\n\n#\n\n\nBase.length\n \n \nMethod\n.\n\n\nlength(arr :: NDArray)\n\n\n\n\nGet the number of elements in an \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nMethod\n.\n\n\nlog(src)\n\n\n\n\nTake log of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.max\n \n \nMethod\n.\n\n\nmax(src, axis, keepdims)\n\n\n\n\nTake max of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.min\n \n \nMethod\n.\n\n\nmin(src, axis, keepdims)\n\n\n\n\nTake min of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.ndims\n \n \nMethod\n.\n\n\nndims(arr :: NDArray)\n\n\n\n\nGet the number of dimensions of an \nNDArray\n. Is equivalent to \nlength(size(arr))\n.\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nMethod\n.\n\n\nround(src)\n\n\n\n\nTake round value of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.setindex!\n \n \nMethod\n.\n\n\nsetindex!(arr :: NDArray, val, idx)\n\n\n\n\nAssign values to an \nNDArray\n. Elementwise assignment is not implemented, only the following scenarios are supported\n\n\n\n\narr[:] = val\n: whole array assignment, \nval\n could be a scalar or an array (Julia \nArray\n or \nNDArray\n) of the same shape.\n\n\narr[start:stop] = val\n: assignment to a \nslice\n, \nval\n could be a scalar or an array of the same shape to the slice. See also \nslice\n.\n\n\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nMethod\n.\n\n\nsign(src)\n\n\n\n\nTake sign value of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nMethod\n.\n\n\nsin(src)\n\n\n\n\nTake sin of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.size\n \n \nMethod\n.\n\n\nsize(arr :: NDArray)\nsize(arr :: NDArray, dim :: Int)\n\n\n\n\nGet the shape of an \nNDArray\n. The shape is in Julia's column-major convention. See also the notes on NDArray shapes \nNDArray\n.\n\n\nsource\n\n\n#\n\n\nBase.slice\n \n \nMethod\n.\n\n\nslice(arr :: NDArray, start:stop)\n\n\n\n\nCreate a view into a sub-slice of an \nNDArray\n. Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an \nNDArray\n of shape (2,3,4), \nslice(array, 2:3)\n will create a \nNDArray\n of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nMethod\n.\n\n\nsqrt(src)\n\n\n\n\nTake sqrt of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nsum(src, axis, keepdims)\n\n\n\n\nTake sum of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(src, axes)\n\n\n\n\nTranspose the input matrix and return a new one\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxes::Shape(tuple), optional, default=()\n: Target axis order. By default the axes will be inverted.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._broadcast\n \n \nMethod\n.\n\n\n_broadcast(src, axis, size)\n\n\n\n\nBroadcast array in the given axis to the given size\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: source ndarray\n\n\naxis::int\n: axis to broadcast\n\n\nsize::int\n: size of broadcast\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._copyto\n \n \nMethod\n.\n\n\n_copyto(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._div_scalar\n \n \nMethod\n.\n\n\n_div_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._get_ndarray_functions\n \n \nMethod\n.\n\n\nThe libxmnet APIs are automatically imported from \nlibmxnet.so\n. The functions listed here operate on \nNDArray\n objects. The arguments to the functions are typically ordered as\n\n\n  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)\n\n\n\n\nunless \nNDARRAY_ARG_BEFORE_SCALAR\n is not set. In this case, the scalars are put before the input arguments:\n\n\n  func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)\n\n\n\n\nIf \nACCEPT_EMPTY_MUTATE_TARGET\n is set. An overloaded function without the output arguments will also be defined:\n\n\n  func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)\n\n\n\n\nUpon calling, the output arguments will be automatically initialized with empty NDArrays.\n\n\nThose functions always return the output arguments. If there is only one output (the typical situation), that object (\nNDArray\n) is returned. Otherwise, a tuple containing all the outputs will be returned.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._imdecode\n \n \nMethod\n.\n\n\n_imdecode(mean, index, x0, y0, x1, y1, c, size)\n\n\n\n\nDecode an image, clip to (x0, y0, x1, y1), substract mean, and write to buffer\n\n\nArguments\n\n\n\n\nmean::NDArray\n: image mean\n\n\nindex::int\n: buffer position for output\n\n\nx0::int\n: x0\n\n\ny0::int\n: y0\n\n\nx1::int\n: x1\n\n\ny1::int\n: y1\n\n\nc::int\n: channel\n\n\nsize::int\n: length of str_img\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum\n \n \nMethod\n.\n\n\n_maximum(lhs, rhs)\n\n\n\n\nElementwise max of lhs by rhs\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._maximum_scalar\n \n \nMethod\n.\n\n\n_maximum_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum\n \n \nMethod\n.\n\n\n_minimum(lhs, rhs)\n\n\n\n\nElementwise min of lhs by rhs\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minimum_scalar\n \n \nMethod\n.\n\n\n_minimum_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus\n \n \nMethod\n.\n\n\n_minus(lhs, rhs)\n\n\n\n\nMinus lhs and rhs\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._minus_scalar\n \n \nMethod\n.\n\n\n_minus_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul\n \n \nMethod\n.\n\n\n_mul(lhs, rhs)\n\n\n\n\nMultiply lhs and rhs\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._mul_scalar\n \n \nMethod\n.\n\n\n_mul_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._onehot_encode\n \n \nMethod\n.\n\n\n_onehot_encode(lhs, rhs)\n\n\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus\n \n \nMethod\n.\n\n\n_plus(lhs, rhs)\n\n\n\n\nAdd lhs and rhs\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._plus_scalar\n \n \nMethod\n.\n\n\n_plus_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power\n \n \nMethod\n.\n\n\n_power(lhs, rhs)\n\n\n\n\nElementwise power(lhs, rhs)\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._power_scalar\n \n \nMethod\n.\n\n\n_power_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_gaussian\n \n \nMethod\n.\n\n\n_random_gaussian()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._random_uniform\n \n \nMethod\n.\n\n\n_random_uniform()\n\n\n\n\nArguments\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rdiv_scalar\n \n \nMethod\n.\n\n\n_rdiv_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rminus_scalar\n \n \nMethod\n.\n\n\n_rminus_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._rpower_scalar\n \n \nMethod\n.\n\n\n_rpower_scalar(src, scalar)\n\n\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nscalar::float\n: scalar input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_normal\n \n \nMethod\n.\n\n\n_sample_normal(loc, scale, shape)\n\n\n\n\nSample a normal distribution\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._sample_uniform\n \n \nMethod\n.\n\n\n_sample_uniform(low, high, shape)\n\n\n\n\nSample a uniform distribution\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._set_value\n \n \nMethod\n.\n\n\n_set_value(src)\n\n\n\n\nArguments\n\n\n\n\nsrc::real_t\n: Source input to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.add_to!\n \n \nMethod\n.\n\n\nadd_to!(dst :: NDArray, args :: Union{Real, NDArray}...)\n\n\n\n\nAdd a bunch of arguments into \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.argmax_channel\n \n \nMethod\n.\n\n\nargmax_channel(src)\n\n\n\n\nTake argmax indices of each channel of the src.The result will be ndarray of shape (num_channel,) on the same device.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nMethod\n.\n\n\nbatch_dot(lhs, rhs)\n\n\n\n\nCalculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) \u2013\n (batch, M, N)\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nMethod\n.\n\n\nbroadcast_axis(src, axis, size)\n\n\n\n\nBroadcast data in the given axis to the given size. The original size of the broadcasting axis must be 1.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=()\n: Target sizes of the broadcasting axes.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nMethod\n.\n\n\nbroadcast_div(lhs, rhs)\n\n\n\n\nlhs divide rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nMethod\n.\n\n\nbroadcast_minus(lhs, rhs)\n\n\n\n\nlhs minus rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nMethod\n.\n\n\nbroadcast_mul(lhs, rhs)\n\n\n\n\nlhs multiple rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nMethod\n.\n\n\nbroadcast_plus(lhs, rhs)\n\n\n\n\nlhs add rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nMethod\n.\n\n\nbroadcast_power(lhs, rhs)\n\n\n\n\nlhs power rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nMethod\n.\n\n\nbroadcast_to(src, shape)\n\n\n\n\nBroadcast data to the target shape. The original size of the broadcasting axis must be 1.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.choose_element_0index\n \n \nMethod\n.\n\n\nchoose_element_0index(lhs, rhs)\n\n\n\n\nChoose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.clip\n \n \nMethod\n.\n\n\nclip(src, a_min, a_max)\n\n\n\n\nClip ndarray elements to range (a_min, a_max)\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input\n\n\na_min::real_t\n: Minimum value\n\n\na_max::real_t\n: Maximum value\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.context\n \n \nMethod\n.\n\n\ncontext(arr :: NDArray)\n\n\n\n\nGet the context that this \nNDArray\n lives on.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.crop\n \n \nMethod\n.\n\n\ncrop(src, begin, end)\n\n\n\n\nCrop the input matrix and return a new one\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\nbegin::Shape(tuple), required\n: starting coordinates\n\n\nend::Shape(tuple), required\n: ending coordinates\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.div_from!\n \n \nMethod\n.\n\n\ndiv_from!(dst :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise divide a scalar or an \nNDArray\n of the same shape from \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.element_mask\n \n \nMethod\n.\n\n\nelement_mask(lhs, rhs)\n\n\n\n\nrhs elmentwise mask lhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\nempty(shape :: Tuple, ctx :: Context)\nempty(shape :: Tuple)\nempty(dim1, dim2, ...)\n\n\n\n\nAllocate memory for an uninitialized \nNDArray\n with specific shape of type Float32.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.empty\n \n \nMethod\n.\n\n\nempty(DType, shape :: Tuple, ctx :: Context)\nempty(DType, shape :: Tuple)\nempty(DType, dim1, dim2, ...)\n\n\n\n\nAllocate memory for an uninitialized \nNDArray\n with a specified type.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nMethod\n.\n\n\nexpand_dims(src, axis)\n\n\n\n\nExpand the shape of array by inserting a new axis.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::int (non-negative), required\n: Position (amongst axes) where new axis is to be inserted.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.fill_element_0index\n \n \nMethod\n.\n\n\nfill_element_0index(lhs, mhs, rhs)\n\n\n\n\nFill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand to the function.\n\n\nmhs::NDArray\n: Middle operand to the function.\n\n\nrhs::NDArray\n: Right operand to the function.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.flip\n \n \nMethod\n.\n\n\nflip(src, axis)\n\n\n\n\nFlip the input matrix along axis and return a new one\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::int, required\n: The dimension to flip\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.is_shared\n \n \nMethod\n.\n\n\nis_shared(j_arr, arr)\n\n\n\n\nTest whether \nj_arr\n is sharing data with \narr\n.\n\n\nArguments:\n\n\n\n\nArray j_arr: the Julia Array.\n\n\nNDArray arr: the \nNDArray\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename, ::Type{NDArray})\n\n\n\n\nLoad NDArrays from binary file.\n\n\nArguments:\n\n\n\n\nfilename::String\n: the path of the file to load. It could be S3 or HDFS address.\n\n\n\n\nReturns either \nDict{Symbol, NDArray}\n or \nVector{NDArray}\n.\n\n\nfilename\n can point to \ns3\n or \nhdfs\n resources if the \nlibmxnet\n is built with the corresponding components enabled. Examples: * \ns3://my-bucket/path/my-s3-ndarray\n * \nhdfs://my-bucket/path/my-hdfs-ndarray\n * \n/path-to/my-local-ndarray\n\n\nsource\n\n\n#\n\n\nMXNet.mx.max_axis\n \n \nMethod\n.\n\n\nmax_axis(src, axis, keepdims)\n\n\n\n\n(Depreciated! Use max instead!) Take max of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.min_axis\n \n \nMethod\n.\n\n\nmin_axis(src, axis, keepdims)\n\n\n\n\n(Depreciated! Use min instead!) Take min of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.mul_to!\n \n \nMethod\n.\n\n\nmul_to!(dst :: NDArray, arg :: Union{Real, NDArray})\n\n\n\n\nElementwise multiplication into \ndst\n of either a scalar or an \nNDArray\n of the same shape. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones(shape :: Tuple, ctx :: Context)\nones(shape :: Tuple)\nones(dim1, dim2, ...)\n\n\n\n\nCreate an \nNDArray\n with specific shape and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ones\n \n \nMethod\n.\n\n\nones(DType, shape :: Tuple, ctx :: Context)\nones(DType, shape :: Tuple)\nones(DType, dim1, dim2, ...)\n\n\n\n\nCreate an \nNDArray\n with specific shape \n type, and initialize with 1.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nMethod\n.\n\n\nrsqrt(src)\n\n\n\n\nTake rsqrt of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, data)\n\n\n\n\nSave NDarrays to binary file. Filename could be S3 or HDFS address, if \nlibmxnet\n is built with corresponding support (see \nload\n).\n\n\n\n\nfilename::String\n: path to the binary file to write to.\n\n\ndata\n: data to save to file. Data can be a\nNDArray\n, a \nVector{NDArray}\n, or a \nDict{Base.Symbol, NDArray}\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nMethod\n.\n\n\nslice_axis(src, axis, begin, end)\n\n\n\n\nSlice the input along certain axis and return a sliced array.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::int, required\n: The axis to be sliced\n\n\nbegin::int, required\n: The beginning index to be sliced\n\n\nend::int, required\n: The end index to be sliced\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nMethod\n.\n\n\nsmooth_l1(src)\n\n\n\n\nCalculate Smooth L1 Loss(lhs, scalar)\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nMethod\n.\n\n\nsoftmax_cross_entropy(lhs, rhs)\n\n\n\n\nCalculate cross_entropy(lhs, one_hot(rhs))\n\n\nArguments\n\n\n\n\nlhs::NDArray\n: Left operand  to the function\n\n\nrhs::NDArray\n: Right operand to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nMethod\n.\n\n\nsquare(src)\n\n\n\n\nTake square of the src\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sub_from!\n \n \nMethod\n.\n\n\nsub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)\n\n\n\n\nSubtract a bunch of arguments from \ndst\n. Inplace updating.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nMethod\n.\n\n\nsum_axis(src, axis, keepdims)\n\n\n\n\n(Depreciated! Use sum instead!) Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::NDArray\n: Source input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.try_get_shared\n \n \nMethod\n.\n\n\ntry_get_shared(arr)\n\n\n\n\nTry to create a Julia array by sharing the data with the underlying \nNDArray\n.\n\n\nArguments:\n\n\n\n\narr::NDArray\n: the array to be shared.\n\n\n\n\n\n\nNote\n\n\nThe returned array does not guarantee to share data with the underlying \nNDArray\n. In particular, data sharing is possible only when the \nNDArray\n lives on CPU.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros(shape :: Tuple, ctx :: Context)\nzeros(shape :: Tuple)\nzeros(dim1, dim2, ...)\n\n\n\n\nCreate zero-ed \nNDArray\n with specific shape.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.zeros\n \n \nMethod\n.\n\n\nzeros(DType, shape :: Tuple, ctx :: Context)\nzeros(DType, shape :: Tuple)\nzeros(DType, dim1, dim2, ...)\n\n\n\n\nCreate zero-ed \nNDArray\n with specific shape and type\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@inplace\n \n \nMacro\n.\n\n\n@inplace\n\n\n\n\nJulia does not support re-definiton of \n+=\n operator (like \n__iadd__\n in python), When one write \na += b\n, it gets translated to \na = a+b\n. \na+b\n will allocate new memory for the results, and the newly allocated \nNDArray\n object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.\n\n\nThis macro is a simple utility to implement this behavior. Write\n\n\n  @mx.inplace a += b\n\n\n\n\nwill translate into\n\n\n  mx.add_to!(a, b)\n\n\n\n\nwhich will do inplace adding of the contents of \nb\n into \na\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.@nd_as_jl\n \n \nMacro\n.\n\n\nManipulating as Julia Arrays\n\n\n@nd_as_jl(captures..., statement)\n\n\n\n\nA convenient macro that allows to operate \nNDArray\n as Julia Arrays. For example,\n\n\n  x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end\n\n\n\n\nUnder the hood, the macro convert all the declared captures from \nNDArray\n into Julia Arrays, by using \ntry_get_shared\n. And automatically commit the modifications back into the \nNDArray\n that is declared as \nrw\n. This is useful for fast prototyping and when implement non-critical computations, such as \nAbstractEvalMetric\n.\n\n\n\n\nNote\n\n\n\n\n\n\nMultiple \nrw\n and / or \nro\n capture declaration could be made.\n\n\nThe macro does \nnot\n check to make sure that \nro\n captures are not modified. If the original \nNDArray\n lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the \nNDArray\n, so modifying the Julia Array will also modify the underlying \nNDArray\n.\n\n\nMore importantly, since the \nNDArray\n is asynchronized, we will wait for \nwriting\n for \nrw\n variables but wait only for \nreading\n in \nro\n variables. If we write into those \nro\n variables, \nand\n if the memory is shared, racing condition might happen, and the behavior is undefined.\n\n\nWhen an \nNDArray\n is declared to be captured as \nrw\n, its contents is always sync back in the end.\n\n\nThe execution results of the expanded macro is always \nnothing\n.\n\n\nThe statements are wrapped in a \nlet\n, thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.\n\n\n\n\nsource", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/ndarray/#ndarray-api", 
            "text": "#  MXNet.mx.NDArray     Type .  NDArray  Wrapper of the  NDArray  type in  libmxnet . This is the basic building block of tensor-based computation.   Note  since C/C++ use row-major ordering for arrays while Julia follows a   column-major ordering. To keep things consistent, we keep the underlying data   in their original layout, but use  language-native  convention when we talk   about shapes. For example, a mini-batch of 100 MNIST images is a tensor of   C/C++/Python shape (100,1,28,28), while in Julia, the same piece of memory   have shape (28,28,1,100).   source  #  Base.:*     Method .  *(arg0, arg1)  Currently only multiplication a scalar with an  NDArray  is implemented. Matrix multiplication is to be added soon.  source  #  Base.:+     Method .  +(args...)\n.+(args...)  Summation. Multiple arguments of either scalar or  NDArray  could be added together. Note at least the first or second argument needs to be an  NDArray  to avoid ambiguity of built-in summation.  source  #  Base.:-     Method .  -(arg0, arg1)\n-(arg0)\n.-(arg0, arg1)  Subtraction  arg0 - arg1 , of scalar types or  NDArray . Or create the negative of  arg0 .  source  #  Base.:.*     Method .  .*(arg0, arg1)  Elementwise multiplication of  arg0  and  arg , could be either scalar or  NDArray .  source  #  Base.:./     Method .  ./(arg0 :: NDArray, arg :: Union{Real, NDArray})  Elementwise dividing an  NDArray  by a scalar or another  NDArray  of the same shape.  source  #  Base.:/     Method .  /(arg0 :: NDArray, arg :: Real)  Divide an  NDArray  by a scalar. Matrix division (solving linear systems) is not implemented yet.  source  #  Base.LinAlg.dot     Method .  dot(lhs, rhs)  Calculate dot product of two matrices or two vectors  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  Base.LinAlg.norm     Method .  norm(src)  Take L2 norm of the src.The result will be ndarray of shape (1,) on the same device.  Arguments   src::NDArray : Source input to the function   source  #  Base._div     Method .  _div(lhs, rhs)  Multiply lhs by rhs  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  Base.abs     Method .  abs(src)  Take absolute value of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.ceil     Method .  ceil(src)  Take ceil value of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.convert     Method .  convert(::Type{Array{T}}, arr :: NDArray)  Convert an  NDArray  into a Julia  Array  of specific type. Data will be copied.  source  #  Base.copy!     Method .  copy!(dst :: Union{NDArray, Array}, src :: Union{NDArray, Array})  Copy contents of  src  into  dst .  source  #  Base.copy     Method .  copy(arr :: NDArray)\ncopy(arr :: NDArray, ctx :: Context)\ncopy(arr :: Array, ctx :: Context)  Create a copy of an array. When no  Context  is given, create a Julia  Array . Otherwise, create an  NDArray  on the specified context.  source  #  Base.cos     Method .  cos(src)  Take cos of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.eltype     Method .  eltype(arr :: NDArray)  Get the element type of an  NDArray .  source  #  Base.exp     Method .  exp(src)  Take exp of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.floor     Method .  floor(src)  Take floor value of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.getindex     Method .  getindex(arr :: NDArray, idx)  Shortcut for  slice . A typical use is to write    arr[:] += 5  which translates into    arr[:] = arr[:] + 5  which furthur translates into    setindex!(getindex(arr, Colon()), 5, Colon())   Note  The behavior is quite different from indexing into Julia's  Array . For example,  arr[2:5]  create a  copy  of the sub-array for Julia  Array , while for  NDArray , this is a  slice  that shares the memory.   source  #  Base.getindex     Method .  Shortcut for  slice .  NOTE  the behavior for Julia's built-in index slicing is to create a copy of the sub-array, while here we simply call  slice , which shares the underlying memory.  source  #  Base.length     Method .  length(arr :: NDArray)  Get the number of elements in an  NDArray .  source  #  Base.log     Method .  log(src)  Take log of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.max     Method .  max(src, axis, keepdims)  Take max of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.min     Method .  min(src, axis, keepdims)  Take min of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.ndims     Method .  ndims(arr :: NDArray)  Get the number of dimensions of an  NDArray . Is equivalent to  length(size(arr)) .  source  #  Base.round     Method .  round(src)  Take round value of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.setindex!     Method .  setindex!(arr :: NDArray, val, idx)  Assign values to an  NDArray . Elementwise assignment is not implemented, only the following scenarios are supported   arr[:] = val : whole array assignment,  val  could be a scalar or an array (Julia  Array  or  NDArray ) of the same shape.  arr[start:stop] = val : assignment to a  slice ,  val  could be a scalar or an array of the same shape to the slice. See also  slice .   source  #  Base.sign     Method .  sign(src)  Take sign value of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.sin     Method .  sin(src)  Take sin of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.size     Method .  size(arr :: NDArray)\nsize(arr :: NDArray, dim :: Int)  Get the shape of an  NDArray . The shape is in Julia's column-major convention. See also the notes on NDArray shapes  NDArray .  source  #  Base.slice     Method .  slice(arr :: NDArray, start:stop)  Create a view into a sub-slice of an  NDArray . Note only slicing at the slowest changing dimension is supported. In Julia's column-major perspective, this is the last dimension. For example, given an  NDArray  of shape (2,3,4),  slice(array, 2:3)  will create a  NDArray  of shape (2,3,2), sharing the data with the original array. This operation is used in data parallelization to split mini-batch into sub-batches for different devices.  source  #  Base.sqrt     Method .  sqrt(src)  Take sqrt of the src  Arguments   src::NDArray : Source input to the function   source  #  Base.sum     Method .  sum(src, axis, keepdims)  Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  Base.transpose     Method .  transpose(src, axes)  Transpose the input matrix and return a new one  Arguments   src::NDArray : Source input to the function  axes::Shape(tuple), optional, default=() : Target axis order. By default the axes will be inverted.   source  #  MXNet.mx._broadcast     Method .  _broadcast(src, axis, size)  Broadcast array in the given axis to the given size  Arguments   src::NDArray : source ndarray  axis::int : axis to broadcast  size::int : size of broadcast   source  #  MXNet.mx._copyto     Method .  _copyto(src)  Arguments   src::NDArray : Source input to the function.   source  #  MXNet.mx._div_scalar     Method .  _div_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._get_ndarray_functions     Method .  The libxmnet APIs are automatically imported from  libmxnet.so . The functions listed here operate on  NDArray  objects. The arguments to the functions are typically ordered as    func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ..., arg_out1, arg_out2, ...)  unless  NDARRAY_ARG_BEFORE_SCALAR  is not set. In this case, the scalars are put before the input arguments:    func_name(scalar1, scalar2, ..., arg_in1, arg_in2, ..., arg_out1, arg_out2, ...)  If  ACCEPT_EMPTY_MUTATE_TARGET  is set. An overloaded function without the output arguments will also be defined:    func_name(arg_in1, arg_in2, ..., scalar1, scalar2, ...)  Upon calling, the output arguments will be automatically initialized with empty NDArrays.  Those functions always return the output arguments. If there is only one output (the typical situation), that object ( NDArray ) is returned. Otherwise, a tuple containing all the outputs will be returned.  source  #  MXNet.mx._imdecode     Method .  _imdecode(mean, index, x0, y0, x1, y1, c, size)  Decode an image, clip to (x0, y0, x1, y1), substract mean, and write to buffer  Arguments   mean::NDArray : image mean  index::int : buffer position for output  x0::int : x0  y0::int : y0  x1::int : x1  y1::int : y1  c::int : channel  size::int : length of str_img   source  #  MXNet.mx._maximum     Method .  _maximum(lhs, rhs)  Elementwise max of lhs by rhs  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._maximum_scalar     Method .  _maximum_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._minimum     Method .  _minimum(lhs, rhs)  Elementwise min of lhs by rhs  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._minimum_scalar     Method .  _minimum_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._minus     Method .  _minus(lhs, rhs)  Minus lhs and rhs  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._minus_scalar     Method .  _minus_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._mul     Method .  _mul(lhs, rhs)  Multiply lhs and rhs  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._mul_scalar     Method .  _mul_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._onehot_encode     Method .  _onehot_encode(lhs, rhs)  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx._plus     Method .  _plus(lhs, rhs)  Add lhs and rhs  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._plus_scalar     Method .  _plus_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._power     Method .  _power(lhs, rhs)  Elementwise power(lhs, rhs)  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx._power_scalar     Method .  _power_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._random_gaussian     Method .  _random_gaussian()  Arguments  source  #  MXNet.mx._random_uniform     Method .  _random_uniform()  Arguments  source  #  MXNet.mx._rdiv_scalar     Method .  _rdiv_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._rminus_scalar     Method .  _rminus_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._rpower_scalar     Method .  _rpower_scalar(src, scalar)  Arguments   src::NDArray : Source input to the function  scalar::float : scalar input to the function   source  #  MXNet.mx._sample_normal     Method .  _sample_normal(loc, scale, shape)  Sample a normal distribution  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), required : The shape of the output   source  #  MXNet.mx._sample_uniform     Method .  _sample_uniform(low, high, shape)  Sample a uniform distribution  Arguments   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), required : The shape of the output   source  #  MXNet.mx._set_value     Method .  _set_value(src)  Arguments   src::real_t : Source input to the function.   source  #  MXNet.mx.add_to!     Method .  add_to!(dst :: NDArray, args :: Union{Real, NDArray}...)  Add a bunch of arguments into  dst . Inplace updating.  source  #  MXNet.mx.argmax_channel     Method .  argmax_channel(src)  Take argmax indices of each channel of the src.The result will be ndarray of shape (num_channel,) on the same device.  Arguments   src::NDArray : Source input to the function   source  #  MXNet.mx.batch_dot     Method .  batch_dot(lhs, rhs)  Calculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) \u2013  (batch, M, N)  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.broadcast_axis     Method .  broadcast_axis(src, axis, size)  Broadcast data in the given axis to the given size. The original size of the broadcasting axis must be 1.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=() : Target sizes of the broadcasting axes.   source  #  MXNet.mx.broadcast_div     Method .  broadcast_div(lhs, rhs)  lhs divide rhs with broadcast  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.broadcast_minus     Method .  broadcast_minus(lhs, rhs)  lhs minus rhs with broadcast  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.broadcast_mul     Method .  broadcast_mul(lhs, rhs)  lhs multiple rhs with broadcast  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.broadcast_plus     Method .  broadcast_plus(lhs, rhs)  lhs add rhs with broadcast  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.broadcast_power     Method .  broadcast_power(lhs, rhs)  lhs power rhs with broadcast  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.broadcast_to     Method .  broadcast_to(src, shape)  Broadcast data to the target shape. The original size of the broadcasting axis must be 1.  Arguments   src::NDArray : Source input to the function  shape::Shape(tuple), optional, default=() : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .   source  #  MXNet.mx.choose_element_0index     Method .  choose_element_0index(lhs, rhs)  Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.clip     Method .  clip(src, a_min, a_max)  Clip ndarray elements to range (a_min, a_max)  Arguments   src::NDArray : Source input  a_min::real_t : Minimum value  a_max::real_t : Maximum value   source  #  MXNet.mx.context     Method .  context(arr :: NDArray)  Get the context that this  NDArray  lives on.  source  #  MXNet.mx.crop     Method .  crop(src, begin, end)  Crop the input matrix and return a new one  Arguments   src::NDArray : Source input to the function  begin::Shape(tuple), required : starting coordinates  end::Shape(tuple), required : ending coordinates   source  #  MXNet.mx.div_from!     Method .  div_from!(dst :: NDArray, arg :: Union{Real, NDArray})  Elementwise divide a scalar or an  NDArray  of the same shape from  dst . Inplace updating.  source  #  MXNet.mx.element_mask     Method .  element_mask(lhs, rhs)  rhs elmentwise mask lhs with broadcast  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.empty     Method .  empty(shape :: Tuple, ctx :: Context)\nempty(shape :: Tuple)\nempty(dim1, dim2, ...)  Allocate memory for an uninitialized  NDArray  with specific shape of type Float32.  source  #  MXNet.mx.empty     Method .  empty(DType, shape :: Tuple, ctx :: Context)\nempty(DType, shape :: Tuple)\nempty(DType, dim1, dim2, ...)  Allocate memory for an uninitialized  NDArray  with a specified type.  source  #  MXNet.mx.expand_dims     Method .  expand_dims(src, axis)  Expand the shape of array by inserting a new axis.  Arguments   src::NDArray : Source input to the function  axis::int (non-negative), required : Position (amongst axes) where new axis is to be inserted.   source  #  MXNet.mx.fill_element_0index     Method .  fill_element_0index(lhs, mhs, rhs)  Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.  Arguments   lhs::NDArray : Left operand to the function.  mhs::NDArray : Middle operand to the function.  rhs::NDArray : Right operand to the function.   source  #  MXNet.mx.flip     Method .  flip(src, axis)  Flip the input matrix along axis and return a new one  Arguments   src::NDArray : Source input to the function  axis::int, required : The dimension to flip   source  #  MXNet.mx.is_shared     Method .  is_shared(j_arr, arr)  Test whether  j_arr  is sharing data with  arr .  Arguments:   Array j_arr: the Julia Array.  NDArray arr: the  NDArray .   source  #  MXNet.mx.load     Method .  load(filename, ::Type{NDArray})  Load NDArrays from binary file.  Arguments:   filename::String : the path of the file to load. It could be S3 or HDFS address.   Returns either  Dict{Symbol, NDArray}  or  Vector{NDArray} .  filename  can point to  s3  or  hdfs  resources if the  libmxnet  is built with the corresponding components enabled. Examples: *  s3://my-bucket/path/my-s3-ndarray  *  hdfs://my-bucket/path/my-hdfs-ndarray  *  /path-to/my-local-ndarray  source  #  MXNet.mx.max_axis     Method .  max_axis(src, axis, keepdims)  (Depreciated! Use max instead!) Take max of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.min_axis     Method .  min_axis(src, axis, keepdims)  (Depreciated! Use min instead!) Take min of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.mul_to!     Method .  mul_to!(dst :: NDArray, arg :: Union{Real, NDArray})  Elementwise multiplication into  dst  of either a scalar or an  NDArray  of the same shape. Inplace updating.  source  #  MXNet.mx.ones     Method .  ones(shape :: Tuple, ctx :: Context)\nones(shape :: Tuple)\nones(dim1, dim2, ...)  Create an  NDArray  with specific shape and initialize with 1.  source  #  MXNet.mx.ones     Method .  ones(DType, shape :: Tuple, ctx :: Context)\nones(DType, shape :: Tuple)\nones(DType, dim1, dim2, ...)  Create an  NDArray  with specific shape   type, and initialize with 1.  source  #  MXNet.mx.rsqrt     Method .  rsqrt(src)  Take rsqrt of the src  Arguments   src::NDArray : Source input to the function   source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, data)  Save NDarrays to binary file. Filename could be S3 or HDFS address, if  libmxnet  is built with corresponding support (see  load ).   filename::String : path to the binary file to write to.  data : data to save to file. Data can be a NDArray , a  Vector{NDArray} , or a  Dict{Base.Symbol, NDArray} .   source  #  MXNet.mx.slice_axis     Method .  slice_axis(src, axis, begin, end)  Slice the input along certain axis and return a sliced array.  Arguments   src::NDArray : Source input to the function  axis::int, required : The axis to be sliced  begin::int, required : The beginning index to be sliced  end::int, required : The end index to be sliced   source  #  MXNet.mx.smooth_l1     Method .  smooth_l1(src)  Calculate Smooth L1 Loss(lhs, scalar)  Arguments   src::NDArray : Source input to the function   source  #  MXNet.mx.softmax_cross_entropy     Method .  softmax_cross_entropy(lhs, rhs)  Calculate cross_entropy(lhs, one_hot(rhs))  Arguments   lhs::NDArray : Left operand  to the function  rhs::NDArray : Right operand to the function   source  #  MXNet.mx.square     Method .  square(src)  Take square of the src  Arguments   src::NDArray : Source input to the function   source  #  MXNet.mx.sub_from!     Method .  sub_from!(dst :: NDArray, args :: Union{Real, NDArray}...)  Subtract a bunch of arguments from  dst . Inplace updating.  source  #  MXNet.mx.sum_axis     Method .  sum_axis(src, axis, keepdims)  (Depreciated! Use sum instead!) Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::NDArray : Source input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.   source  #  MXNet.mx.try_get_shared     Method .  try_get_shared(arr)  Try to create a Julia array by sharing the data with the underlying  NDArray .  Arguments:   arr::NDArray : the array to be shared.    Note  The returned array does not guarantee to share data with the underlying  NDArray . In particular, data sharing is possible only when the  NDArray  lives on CPU.   source  #  MXNet.mx.zeros     Method .  zeros(shape :: Tuple, ctx :: Context)\nzeros(shape :: Tuple)\nzeros(dim1, dim2, ...)  Create zero-ed  NDArray  with specific shape.  source  #  MXNet.mx.zeros     Method .  zeros(DType, shape :: Tuple, ctx :: Context)\nzeros(DType, shape :: Tuple)\nzeros(DType, dim1, dim2, ...)  Create zero-ed  NDArray  with specific shape and type  source  #  MXNet.mx.@inplace     Macro .  @inplace  Julia does not support re-definiton of  +=  operator (like  __iadd__  in python), When one write  a += b , it gets translated to  a = a+b .  a+b  will allocate new memory for the results, and the newly allocated  NDArray  object is then assigned back to a, while the original contents in a is discarded. This is very inefficient when we want to do inplace update.  This macro is a simple utility to implement this behavior. Write    @mx.inplace a += b  will translate into    mx.add_to!(a, b)  which will do inplace adding of the contents of  b  into  a .  source  #  MXNet.mx.@nd_as_jl     Macro .  Manipulating as Julia Arrays  @nd_as_jl(captures..., statement)  A convenient macro that allows to operate  NDArray  as Julia Arrays. For example,    x = mx.zeros(3,4)\n  y = mx.ones(3,4)\n  z = mx.zeros((3,4), mx.gpu())\n\n  @mx.nd_as_jl ro=(x,y) rw=z begin\n    # now x, y, z are just ordinary Julia Arrays\n    z[:,1] = y[:,2]\n    z[:,2] = 5\n  end  Under the hood, the macro convert all the declared captures from  NDArray  into Julia Arrays, by using  try_get_shared . And automatically commit the modifications back into the  NDArray  that is declared as  rw . This is useful for fast prototyping and when implement non-critical computations, such as  AbstractEvalMetric .   Note    Multiple  rw  and / or  ro  capture declaration could be made.  The macro does  not  check to make sure that  ro  captures are not modified. If the original  NDArray  lives in CPU memory, then it is very likely the corresponding Julia Array shares data with the  NDArray , so modifying the Julia Array will also modify the underlying  NDArray .  More importantly, since the  NDArray  is asynchronized, we will wait for  writing  for  rw  variables but wait only for  reading  in  ro  variables. If we write into those  ro  variables,  and  if the memory is shared, racing condition might happen, and the behavior is undefined.  When an  NDArray  is declared to be captured as  rw , its contents is always sync back in the end.  The execution results of the expanded macro is always  nothing .  The statements are wrapped in a  let , thus locally introduced new variables will not be available after the statements. So you will need to declare the variables before calling the macro if needed.   source", 
            "title": "NDArray API"
        }, 
        {
            "location": "/api/symbolic-node/", 
            "text": "Symbolic API\n\n\n#\n\n\nMXNet.mx.SymbolicNode\n \n \nType\n.\n\n\nSymbolicNode\n\n\n\n\nSymbolicNode is the basic building block of the symbolic graph in MXNet.jl.\n\n\n(self :: SymbolicNode)(args :: SymbolicNode...)\n(self :: SymbolicNode)(; kwargs...)\n\n\n\n\nMake a new node by composing \nself\n with \nargs\n. Or the arguments can be specified using keyword arguments.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.dot\n \n \nMethod\n.\n\n\nCalculate dot product of two matrices or two vectors\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.abs\n \n \nMethod\n.\n\n\nTake absolute value of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.ceil\n \n \nMethod\n.\n\n\nTake ceil value of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.copy\n \n \nMethod\n.\n\n\ncopy(self :: SymbolicNode)\n\n\n\n\nMake a copy of a SymbolicNode. The same as making a deep copy.\n\n\nsource\n\n\n#\n\n\nBase.cos\n \n \nMethod\n.\n\n\nTake cos of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.deepcopy\n \n \nMethod\n.\n\n\ndeepcopy(self :: SymbolicNode)\n\n\n\n\nMake a deep copy of a SymbolicNode.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nMethod\n.\n\n\nTake exp of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.floor\n \n \nMethod\n.\n\n\nTake floor value of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})\n\n\n\n\nGet a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of \nlist_outputs\n.\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nMethod\n.\n\n\nTake log of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.round\n \n \nMethod\n.\n\n\nTake round value of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.sign\n \n \nMethod\n.\n\n\nTake sign value of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.sin\n \n \nMethod\n.\n\n\nTake sin of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.sqrt\n \n \nMethod\n.\n\n\nTake sqrt of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\nTake sum of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\nTranspose the input matrix and return a new one\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxes::Shape(tuple), optional, default=()\n: Target axis order. By default the axes will be inverted.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Activation\n \n \nMethod\n.\n\n\nApply activation function to input.Softmax Activation is only available with CUDNN on GPUand will be computed at each location across channel if input is 4D.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required\n: Activation function to be applied.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BatchNorm\n \n \nMethod\n.\n\n\nApply batch normalization to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to batch normalization\n\n\neps::float, optional, default=0.001\n: Epsilon to prevent div 0\n\n\nmomentum::float, optional, default=0.9\n: Momentum for moving average\n\n\nfix_gamma::boolean, optional, default=True\n: Fix gamma while training\n\n\nuse_global_stats::boolean, optional, default=False\n: Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.BlockGrad\n \n \nMethod\n.\n\n\nGet output from a symbol and pass 0 gradient back\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Cast\n \n \nMethod\n.\n\n\nCast array to a different data type.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to cast function.\n\n\ndtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required\n: Target data type.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Concat\n \n \nMethod\n.\n\n\nPerform an feature concat on channel dim (defaut is 1) over all\n\n\nThis function support variable length positional \nSymbolicNode\n inputs.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode[]\n: List of tensors to concatenate\n\n\nnum_args::int, required\n: Number of inputs to be concated.\n\n\ndim::int, optional, default='1'\n: the dimension to be concated.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Convolution\n \n \nMethod\n.\n\n\nApply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: convolution kernel size: (y, x) or (d, y, x)\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: convolution stride: (y, x) or (d, y, x)\n\n\ndilate::Shape(tuple), optional, default=(1,1)\n: convolution dilate: (y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for convolution: (y, x) or (d, y, x)\n\n\nnum_filter::int (non-negative), required\n: convolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: Number of groups partition. This option is not supported by CuDNN, you can use SliceChannel to num_group,apply convolution and concat instead to achieve the same need.\n\n\nworkspace::long (non-negative), optional, default=1024\n: Tmp workspace for convolution (MB).\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\ncudnn_tune::{'fastest', 'limited_workspace', 'off'},optional, default='off'\n: Whether to find convolution algo by running performance test.Leads to higher startup time but may give better speed.auto tune is turned off by default.Set environment varialbe MXNET_CUDNN_AUTOTUNE_DEFAULT=1 to turn on by default.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Correlation\n \n \nMethod\n.\n\n\nApply correlation to inputs\n\n\nArguments\n\n\n\n\ndata1::SymbolicNode\n: Input data1 to the correlation.\n\n\ndata2::SymbolicNode\n: Input data2 to the correlation.\n\n\nkernel_size::int (non-negative), optional, default=1\n: kernel size for Correlation must be an odd number\n\n\nmax_displacement::int (non-negative), optional, default=1\n: Max displacement of Correlation\n\n\nstride1::int (non-negative), optional, default=1\n: stride1 quantize data1 globally\n\n\nstride2::int (non-negative), optional, default=1\n: stride2 quantize data2 within the neighborhood centered around data1\n\n\npad_size::int (non-negative), optional, default=0\n: pad for Correlation\n\n\nis_multiply::boolean, optional, default=True\n: operation type is either multiplication or subduction\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Crop\n \n \nMethod\n.\n\n\nCrop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used\n\n\nThis function support variable length positional \nSymbolicNode\n inputs.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode or SymbolicNode[]\n: Tensor or List of Tensors, the second input will be used as crop_like shape reference\n\n\nnum_args::int, required\n: Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here\n\n\noffset::Shape(tuple), optional, default=(0,0)\n: crop offset coordinate: (y, x)\n\n\nh_w::Shape(tuple), optional, default=(0,0)\n: crop height and weight: (h, w)\n\n\ncenter_crop::boolean, optional, default=False\n: If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Custom\n \n \nMethod\n.\n\n\nCustom operator implemented in frontend.\n\n\nArguments\n\n\n\n\nop_type::string\n: Type of custom operator. Must be registered first.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Deconvolution\n \n \nMethod\n.\n\n\nApply deconvolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the DeconvolutionOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nkernel::Shape(tuple), required\n: deconvolution kernel size: (y, x)\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: deconvolution stride: (y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically\n\n\nadj::Shape(tuple), optional, default=(0,0)\n: adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape with targe shape : (y, x)\n\n\nnum_filter::int (non-negative), required\n: deconvolution filter(channel) number\n\n\nnum_group::int (non-negative), optional, default=1\n: number of groups partition\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nno_bias::boolean, optional, default=True\n: Whether to disable bias parameter.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Dropout\n \n \nMethod\n.\n\n\nApply dropout to input\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to dropout.\n\n\np::float, optional, default=0.5\n: Fraction of the input that gets dropped out at training time\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ElementWiseSum\n \n \nMethod\n.\n\n\nPerform an elementwise sum over all the inputs.\n\n\nThis function support variable length positional \nSymbolicNode\n inputs.\n\n\nArguments\n\n\n\n\nnum_args::int, required\n: Number of inputs to be summed.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Embedding\n \n \nMethod\n.\n\n\nGet embedding for one-hot input. A n-dimensional input tensor will be trainsformed into a (n+1)-dimensional tensor, where a new dimension is added for the embedding results.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the EmbeddingOp.\n\n\nweight::SymbolicNode\n: Enbedding weight matrix.\n\n\ninput_dim::int, required\n: input dim of one-hot encoding\n\n\noutput_dim::int, required\n: output dim of embedding\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Flatten\n \n \nMethod\n.\n\n\nFlatten input\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to flatten.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.FullyConnected\n \n \nMethod\n.\n\n\nApply matrix multiplication to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the FullyConnectedOp.\n\n\nweight::SymbolicNode\n: Weight matrix.\n\n\nbias::SymbolicNode\n: Bias parameter.\n\n\nnum_hidden::int, required\n: Number of hidden nodes of the output.\n\n\nno_bias::boolean, optional, default=False\n: Whether to disable bias parameter.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Group\n \n \nMethod\n.\n\n\nGroup(nodes :: SymbolicNode...)\n\n\n\n\nCreate a \nSymbolicNode\n by grouping nodes together.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.IdentityAttachKLSparseReg\n \n \nMethod\n.\n\n\nApply a sparse regularization to the output a sigmoid activation function.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\nsparseness_target::float, optional, default=0.1\n: The sparseness target\n\n\npenalty::float, optional, default=0.001\n: The tradeoff parameter for the sparseness penalty\n\n\nmomentum::float, optional, default=0.9\n: The momentum for running average\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.L2Normalization\n \n \nMethod\n.\n\n\nSet the l2 norm of each instance to a constant.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the L2NormalizationOp.\n\n\neps::float, optional, default=1e-10\n: Epsilon to prevent div 0\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LRN\n \n \nMethod\n.\n\n\nApply convolution to input then add a bias.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the ConvolutionOp.\n\n\nalpha::float, optional, default=0.0001\n: value of the alpha variance scaling parameter in the normalization formula\n\n\nbeta::float, optional, default=0.75\n: value of the beta power parameter in the normalization formula\n\n\nknorm::float, optional, default=2\n: value of the k parameter in normalization formula\n\n\nnsize::int (non-negative), required\n: normalization window width in elements.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LeakyReLU\n \n \nMethod\n.\n\n\nApply activation function to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nact_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'\n: Activation function to be applied.\n\n\nslope::float, optional, default=0.25\n: Init slope for the activation. (For leaky and elu only)\n\n\nlower_bound::float, optional, default=0.125\n: Lower bound of random slope. (For rrelu only)\n\n\nupper_bound::float, optional, default=0.334\n: Upper bound of random slope. (For rrelu only)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LinearRegressionOutput\n \n \nMethod\n.\n\n\nUse linear regression for final output, this is used on final output of a net.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.LogisticRegressionOutput\n \n \nMethod\n.\n\n\nUse Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MAERegressionOutput\n \n \nMethod\n.\n\n\nUse mean absolute error regression for final output, this is used on final output of a net.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to function.\n\n\nlabel::SymbolicNode\n: Input label to function.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.MakeLoss\n \n \nMethod\n.\n\n\nGet output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data.\n\n\ngrad_scale::float, optional, default=1\n: gradient scale as a supplement to unary and binary operators\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Pooling\n \n \nMethod\n.\n\n\nPerform spatial pooling on inputs.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator.\n\n\nglobal_pool::boolean, optional, default=False\n: Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape\n\n\nkernel::Shape(tuple), required\n: pooling kernel size: (y, x) or (d, y, x)\n\n\npool_type::{'avg', 'max', 'sum'}, required\n: Pooling type to be applied.\n\n\nstride::Shape(tuple), optional, default=(1,1)\n: stride: for pooling (y, x) or (d, y, x)\n\n\npad::Shape(tuple), optional, default=(0,0)\n: pad for pooling: (y, x) or (d, y, x)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.RNN\n \n \nMethod\n.\n\n\nApply a recurrent layer to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to RNN\n\n\nparameters::SymbolicNode\n: Vector of all RNN trainable parameters\n\n\nstate::SymbolicNode\n: initial hidden state of the RNN\n\n\nstate_cell::SymbolicNode\n: initial cell state for LSTM networks (only for LSTM)\n\n\nstate_size::int (non-negative), required\n: size of the state for each layer\n\n\nnum_layers::int (non-negative), required\n: number of stacked layers\n\n\nbidirectional::boolean, optional, default=False\n: whether to use bidirectional recurrent layers\n\n\nmode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required\n: the type of RNN to compute\n\n\np::float, optional, default=0\n: Fraction of the input that gets dropped out at training time\n\n\nstate_outputs::boolean, optional, default=False\n: Whether to have the states as symbol outputs.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.ROIPooling\n \n \nMethod\n.\n\n\nPerforms region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the pooling operator, a 4D Feature maps\n\n\nrois::SymbolicNode\n: Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data\n\n\npooled_size::Shape(tuple), required\n: fix pooled size: (h, w)\n\n\nspatial_scale::float, required\n: Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Reshape\n \n \nMethod\n.\n\n\nReshape input to target shape\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to reshape.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims\n\n\nkeep_highest::boolean, optional, default=False\n: (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input\n\n\nshape::, optional, default=()\n: Target new shape. If the dim is same, set it to 0. If the dim is set to be -1, it will be inferred from the rest of dims. One and only one dim can be -1\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SVMOutput\n \n \nMethod\n.\n\n\nSupport Vector Machine based transformation on input, backprop L2-SVM\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to svm.\n\n\nlabel::SymbolicNode\n: Label data.\n\n\nmargin::float, optional, default=1\n: Scale the DType(param_.margin) for activation size\n\n\nregularization_coefficient::float, optional, default=1\n: Scale the coefficient responsible for balacing coefficient size and error tradeoff\n\n\nuse_linear::boolean, optional, default=False\n: If set true, uses L1-SVM objective function. Default uses L2-SVM objective\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SliceChannel\n \n \nMethod\n.\n\n\nSlice input equally along specified axis\n\n\nArguments\n\n\n\n\nnum_outputs::int, required\n: Number of outputs to be sliced.\n\n\naxis::int, optional, default='1'\n: Dimension along which to slice.\n\n\nsqueeze_axis::boolean, optional, default=False\n: If true AND the sliced dimension becomes 1, squeeze that dimension.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode[]\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Softmax\n \n \nMethod\n.\n\n\nDEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxActivation\n \n \nMethod\n.\n\n\nApply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to activation function.\n\n\nmode::{'channel', 'instance'},optional, default='instance'\n: Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SoftmaxOutput\n \n \nMethod\n.\n\n\nPerform a softmax transformation on input, backprop with logloss.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to softmax.\n\n\nlabel::SymbolicNode\n: Label data, can also be probability value with same shape as data\n\n\ngrad_scale::float, optional, default=1\n: Scale the gradient by a float factor\n\n\nignore_label::float, optional, default=-1\n: the label value will be ignored during backward (only works if use_ignore is set to be true).\n\n\nmulti_output::boolean, optional, default=False\n: If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n\nx_1\n...*x_n output, each has k classes\n\n\nuse_ignore::boolean, optional, default=False\n: If set to true, the ignore_label value will not contribute to the backward gradient\n\n\nnormalization::{'batch', 'null', 'valid'},optional, default='null'\n: If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SpatialTransformer\n \n \nMethod\n.\n\n\nApply spatial transformer to input feature map.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the SpatialTransformerOp.\n\n\nloc::SymbolicNode\n: localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.\n\n\ntarget_shape::Shape(tuple), optional, default=(0,0)\n: output shape(h, w) of spatial transformer: (y, x)\n\n\ntransform_type::{'affine'}, required\n: transformation type\n\n\nsampler_type::{'bilinear'}, required\n: sampling type\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.SwapAxis\n \n \nMethod\n.\n\n\nApply swapaxis to input.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode\n: Input data to the SwapAxisOp.\n\n\ndim1::int (non-negative), optional, default=0\n: the first axis to be swapped.\n\n\ndim2::int (non-negative), optional, default=0\n: the second axis to be swapped.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.UpSampling\n \n \nMethod\n.\n\n\nPerform nearest neighboor/bilinear up sampling to inputs\n\n\nThis function support variable length positional \nSymbolicNode\n inputs.\n\n\nArguments\n\n\n\n\ndata::SymbolicNode[]\n: Array of tensors to upsample\n\n\nscale::int (non-negative), required\n: Up sampling scale\n\n\nnum_filter::int (non-negative), optional, default=0\n: Input filter. Only used by bilinear sample_type.\n\n\nsample_type::{'bilinear', 'nearest'}, required\n: upsampling method\n\n\nmulti_input_mode::{'concat', 'sum'},optional, default='concat'\n: How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n\n\nnum_args::int, required\n: Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale\nh_0,scale\nw_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.\n\n\nworkspace::long (non-negative), optional, default=512\n: Tmp workspace for deconvolution (MB)\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.Variable\n \n \nMethod\n.\n\n\nVariable(name :: Union{Symbol, AbstractString})\n\n\n\n\nCreate a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.\n\n\nArguments\n\n\n\n\nDict{Symbol, AbstractString} attrs: The attributes associated with this \nVariable\n.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx._CrossDeviceCopy\n \n \nMethod\n.\n\n\nSpecial op to copy data cross device\n\n\nArguments\n\n\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Div\n \n \nMethod\n.\n\n\nMultiply lhs by rhs\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._DivScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Maximum\n \n \nMethod\n.\n\n\nElementwise max of lhs by rhs\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MaximumScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minimum\n \n \nMethod\n.\n\n\nElementwise min of lhs by rhs\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinimumScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Minus\n \n \nMethod\n.\n\n\nMinus lhs and rhs\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MinusScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Mul\n \n \nMethod\n.\n\n\nMultiply lhs and rhs\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._MulScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._NDArray\n \n \nMethod\n.\n\n\nStub for implementing an operator implemented in native frontend language with ndarray.\n\n\nArguments\n\n\n\n\ninfo::, required\n:\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Native\n \n \nMethod\n.\n\n\nStub for implementing an operator implemented in native frontend language.\n\n\nArguments\n\n\n\n\ninfo::, required\n:\n\n\nneed_top_grad::boolean, optional, default=True\n: Whether this layer needs out grad for backward. Should be false for loss layers.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Plus\n \n \nMethod\n.\n\n\nAdd lhs and rhs\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PlusScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._Power\n \n \nMethod\n.\n\n\nElementwise power(lhs, rhs)\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._PowerScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RDivScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RMinusScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx._RPowerScalar\n \n \nMethod\n.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.batch_dot\n \n \nMethod\n.\n\n\nCalculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) \u2013\n (batch, M, N)\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_axis\n \n \nMethod\n.\n\n\nBroadcast data in the given axis to the given size. The original size of the broadcasting axis must be 1.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: The axes to perform the broadcasting.\n\n\nsize::Shape(tuple), optional, default=()\n: Target sizes of the broadcasting axes.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_div\n \n \nMethod\n.\n\n\nlhs divide rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_minus\n \n \nMethod\n.\n\n\nlhs minus rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_mul\n \n \nMethod\n.\n\n\nlhs multiple rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_plus\n \n \nMethod\n.\n\n\nlhs add rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_power\n \n \nMethod\n.\n\n\nlhs power rhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.broadcast_to\n \n \nMethod\n.\n\n\nBroadcast data to the target shape. The original size of the broadcasting axis must be 1.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nshape::Shape(tuple), optional, default=()\n: The shape of the desired array. We can set the dim to zero if it's same as the original. E.g \nA = broadcast_to(B, shape=(10, 0, 0))\n has the same meaning as \nA = broadcast_axis(B, axis=0, size=10)\n.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.element_mask\n \n \nMethod\n.\n\n\nrhs elmentwise mask lhs with broadcast\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.expand_dims\n \n \nMethod\n.\n\n\nExpand the shape of array by inserting a new axis.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::int (non-negative), required\n: Position (amongst axes) where new axis is to be inserted.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.from_json\n \n \nMethod\n.\n\n\nfrom_json(repr :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a \nSymbolicNode\n from a JSON string representation.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_attr\n \n \nMethod\n.\n\n\nget_attr(self :: SymbolicNode, key :: Symbol)\n\n\n\n\nGet attribute attached to this \nSymbolicNode\n belonging to key.\n\n\nReturns the value belonging to key as a \nNullable\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.get_internals\n \n \nMethod\n.\n\n\nget_internals(self :: SymbolicNode)\n\n\n\n\nGet a new grouped \nSymbolicNode\n whose output contains all the internal outputs of this \nSymbolicNode\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.grad\n \n \nMethod\n.\n\n\ngrad(self :: SymbolicNode, wrt :: Vector{SymbolicNode})\n\n\n\n\nGet the autodiff gradient of the current \nSymbolicNode\n. This function can only be used if the current symbol is a loss function.\n\n\nArguments:\n\n\n\n\nself::SymbolicNode\n: current node.\n\n\nwrt::Vector{Symbol}\n: the names of the arguments to the gradient.\n\n\n\n\nReturns a gradient symbol of the corresponding gradient.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_shape\n \n \nMethod\n.\n\n\ninfer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)\n\n\n\n\nDo shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by \nlist_arguments\n. Alternatively, the shape information could be specified via keyword arguments.\n\n\nReturns a 3-tuple containing shapes of all the arguments, shapes of all the outputs and shapes of all the auxiliary variables. If shape inference failed due to incomplete or incompatible inputs, the return value will be \n(nothing, nothing, nothing)\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.infer_type\n \n \nMethod\n.\n\n\ninfer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)\n\n\n\n\nDo type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by \nlist_arguments\n. Alternatively, the type information could be specified via keyword arguments.\n\n\nReturns a 3-tuple containing types of all the arguments, types of all the outputs and types of all the auxiliary variables. If type inference failed due to incomplete or incompatible inputs, the return value will be \n(nothing, nothing, nothing)\n.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_all_attr\n \n \nMethod\n.\n\n\nlist_all_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from the symbol graph.\n\n\nReturns a dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_arguments\n \n \nMethod\n.\n\n\nlist_arguments(self :: SymbolicNode)\n\n\n\n\nList all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a \nFullyConnected\n node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.\n\n\nReturns a list of symbols indicating the names of the arguments.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_attr\n \n \nMethod\n.\n\n\nlist_attr(self :: SymbolicNode)\n\n\n\n\nGet all attributes from a symbol.\n\n\nReturns a dictionary of attributes.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_auxiliary_states\n \n \nMethod\n.\n\n\nlist_auxiliary_states(self :: SymbolicNode)\n\n\n\n\nList all auxiliary states in the symbool.\n\n\nAuxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.\n\n\nReturns a list of symbols indicating the names of the auxiliary states.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.list_outputs\n \n \nMethod\n.\n\n\nlist_outputs(self :: SymbolicNode)\n\n\n\n\nList all the outputs of this node.\n\n\nReturns a list of symbols indicating the names of the outputs.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.load\n \n \nMethod\n.\n\n\nload(filename :: AbstractString, ::Type{SymbolicNode})\n\n\n\n\nLoad a \nSymbolicNode\n from a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.normal\n \n \nMethod\n.\n\n\nSample a normal distribution\n\n\nArguments\n\n\n\n\nloc::float, optional, default=0\n: Mean of the distribution.\n\n\nscale::float, optional, default=1\n: Standard deviation of the distribution.\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.rsqrt\n \n \nMethod\n.\n\n\nTake rsqrt of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.save\n \n \nMethod\n.\n\n\nsave(filename :: AbstractString, node :: SymbolicNode)\n\n\n\n\nSave a \nSymbolicNode\n to a JSON file.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.set_attr\n \n \nMethod\n.\n\n\nset_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)\n\n\n\n\nSet the attribute key to value for this \nSymbolicNode\n.\n\n\n\n\nNote\n\n\nIt is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the \nSymbolicNode\n. Changing the attributes of a \nSymbolicNode\n that is already been used somewhere else might cause unexpected behavior and inconsistency.\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.slice_axis\n \n \nMethod\n.\n\n\nSlice the input along certain axis and return a sliced array.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::int, required\n: The axis to be sliced\n\n\nbegin::int, required\n: The beginning index to be sliced\n\n\nend::int, required\n: The end index to be sliced\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.smooth_l1\n \n \nMethod\n.\n\n\nCalculate Smooth L1 Loss(lhs, scalar)\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.softmax_cross_entropy\n \n \nMethod\n.\n\n\nCalculate cross_entropy(lhs, one_hot(rhs))\n\n\nArguments\n\n\n\n\nlhs::SymbolicNode\n: Left symbolic input to the function\n\n\nrhs::SymbolicNode\n: Right symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.square\n \n \nMethod\n.\n\n\nTake square of the src\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.sum_axis\n \n \nMethod\n.\n\n\n(Depreciated! Use sum instead!) Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.\n\n\nArguments\n\n\n\n\nsrc::SymbolicNode\n: Left symbolic input to the function\n\n\naxis::Shape(tuple), optional, default=()\n: Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.\n\n\nkeepdims::boolean, optional, default=False\n: Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.to_json\n \n \nMethod\n.\n\n\nto_json(self :: SymbolicNode)\n\n\n\n\nConvert a \nSymbolicNode\n into a JSON string.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.uniform\n \n \nMethod\n.\n\n\nSample a uniform distribution\n\n\nArguments\n\n\n\n\nlow::float, optional, default=0\n: The lower bound of distribution\n\n\nhigh::float, optional, default=1\n: The upper bound of distribution\n\n\nshape::Shape(tuple), required\n: The shape of the output\n\n\nname::Symbol\n: The name of the \nSymbolicNode\n. (e.g. \n:my_symbol\n), optional.\n\n\nattrs::Dict{Symbol, AbstractString}\n: The attributes associated with this \nSymbolicNode\n.\n\n\n\n\nReturns ``.\n\n\nsource", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/symbolic-node/#symbolic-api", 
            "text": "#  MXNet.mx.SymbolicNode     Type .  SymbolicNode  SymbolicNode is the basic building block of the symbolic graph in MXNet.jl.  (self :: SymbolicNode)(args :: SymbolicNode...)\n(self :: SymbolicNode)(; kwargs...)  Make a new node by composing  self  with  args . Or the arguments can be specified using keyword arguments.  source  #  Base.LinAlg.dot     Method .  Calculate dot product of two matrices or two vectors  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.abs     Method .  Take absolute value of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.ceil     Method .  Take ceil value of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.copy     Method .  copy(self :: SymbolicNode)  Make a copy of a SymbolicNode. The same as making a deep copy.  source  #  Base.cos     Method .  Take cos of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.deepcopy     Method .  deepcopy(self :: SymbolicNode)  Make a deep copy of a SymbolicNode.  source  #  Base.exp     Method .  Take exp of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.floor     Method .  Take floor value of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.getindex     Method .  getindex(self :: SymbolicNode, idx :: Union{Int, Base.Symbol, AbstractString})  Get a node representing the specified output of this node. The index could be a symbol or string indicating the name of the output, or a 1-based integer indicating the index, as in the list of  list_outputs .  source  #  Base.log     Method .  Take log of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.round     Method .  Take round value of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.sign     Method .  Take sign value of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.sin     Method .  Take sin of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.sqrt     Method .  Take sqrt of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.sum     Method .  Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::SymbolicNode : Left symbolic input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  Base.transpose     Method .  Transpose the input matrix and return a new one  Arguments   src::SymbolicNode : Left symbolic input to the function  axes::Shape(tuple), optional, default=() : Target axis order. By default the axes will be inverted.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.Activation     Method .  Apply activation function to input.Softmax Activation is only available with CUDNN on GPUand will be computed at each location across channel if input is 4D.  Arguments   data::SymbolicNode : Input data to activation function.  act_type::{'relu', 'sigmoid', 'softrelu', 'tanh'}, required : Activation function to be applied.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.BatchNorm     Method .  Apply batch normalization to input.  Arguments   data::SymbolicNode : Input data to batch normalization  eps::float, optional, default=0.001 : Epsilon to prevent div 0  momentum::float, optional, default=0.9 : Momentum for moving average  fix_gamma::boolean, optional, default=True : Fix gamma while training  use_global_stats::boolean, optional, default=False : Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.BlockGrad     Method .  Get output from a symbol and pass 0 gradient back  Arguments   data::SymbolicNode : Input data.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Cast     Method .  Cast array to a different data type.  Arguments   data::SymbolicNode : Input data to cast function.  dtype::{'float16', 'float32', 'float64', 'int32', 'uint8'}, required : Target data type.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Concat     Method .  Perform an feature concat on channel dim (defaut is 1) over all  This function support variable length positional  SymbolicNode  inputs.  Arguments   data::SymbolicNode[] : List of tensors to concatenate  num_args::int, required : Number of inputs to be concated.  dim::int, optional, default='1' : the dimension to be concated.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Convolution     Method .  Apply convolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the ConvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : convolution kernel size: (y, x) or (d, y, x)  stride::Shape(tuple), optional, default=(1,1) : convolution stride: (y, x) or (d, y, x)  dilate::Shape(tuple), optional, default=(1,1) : convolution dilate: (y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for convolution: (y, x) or (d, y, x)  num_filter::int (non-negative), required : convolution filter(channel) number  num_group::int (non-negative), optional, default=1 : Number of groups partition. This option is not supported by CuDNN, you can use SliceChannel to num_group,apply convolution and concat instead to achieve the same need.  workspace::long (non-negative), optional, default=1024 : Tmp workspace for convolution (MB).  no_bias::boolean, optional, default=False : Whether to disable bias parameter.  cudnn_tune::{'fastest', 'limited_workspace', 'off'},optional, default='off' : Whether to find convolution algo by running performance test.Leads to higher startup time but may give better speed.auto tune is turned off by default.Set environment varialbe MXNET_CUDNN_AUTOTUNE_DEFAULT=1 to turn on by default.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Correlation     Method .  Apply correlation to inputs  Arguments   data1::SymbolicNode : Input data1 to the correlation.  data2::SymbolicNode : Input data2 to the correlation.  kernel_size::int (non-negative), optional, default=1 : kernel size for Correlation must be an odd number  max_displacement::int (non-negative), optional, default=1 : Max displacement of Correlation  stride1::int (non-negative), optional, default=1 : stride1 quantize data1 globally  stride2::int (non-negative), optional, default=1 : stride2 quantize data2 within the neighborhood centered around data1  pad_size::int (non-negative), optional, default=0 : pad for Correlation  is_multiply::boolean, optional, default=True : operation type is either multiplication or subduction  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Crop     Method .  Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used  This function support variable length positional  SymbolicNode  inputs.  Arguments   data::SymbolicNode or SymbolicNode[] : Tensor or List of Tensors, the second input will be used as crop_like shape reference  num_args::int, required : Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here  offset::Shape(tuple), optional, default=(0,0) : crop offset coordinate: (y, x)  h_w::Shape(tuple), optional, default=(0,0) : crop height and weight: (h, w)  center_crop::boolean, optional, default=False : If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Custom     Method .  Custom operator implemented in frontend.  Arguments   op_type::string : Type of custom operator. Must be registered first.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Deconvolution     Method .  Apply deconvolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the DeconvolutionOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  kernel::Shape(tuple), required : deconvolution kernel size: (y, x)  stride::Shape(tuple), optional, default=(1,1) : deconvolution stride: (y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically  adj::Shape(tuple), optional, default=(0,0) : adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically  target_shape::Shape(tuple), optional, default=(0,0) : output shape with targe shape : (y, x)  num_filter::int (non-negative), required : deconvolution filter(channel) number  num_group::int (non-negative), optional, default=1 : number of groups partition  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  no_bias::boolean, optional, default=True : Whether to disable bias parameter.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Dropout     Method .  Apply dropout to input  Arguments   data::SymbolicNode : Input data to dropout.  p::float, optional, default=0.5 : Fraction of the input that gets dropped out at training time  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.ElementWiseSum     Method .  Perform an elementwise sum over all the inputs.  This function support variable length positional  SymbolicNode  inputs.  Arguments   num_args::int, required : Number of inputs to be summed.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Embedding     Method .  Get embedding for one-hot input. A n-dimensional input tensor will be trainsformed into a (n+1)-dimensional tensor, where a new dimension is added for the embedding results.  Arguments   data::SymbolicNode : Input data to the EmbeddingOp.  weight::SymbolicNode : Enbedding weight matrix.  input_dim::int, required : input dim of one-hot encoding  output_dim::int, required : output dim of embedding  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Flatten     Method .  Flatten input  Arguments   data::SymbolicNode : Input data to flatten.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.FullyConnected     Method .  Apply matrix multiplication to input then add a bias.  Arguments   data::SymbolicNode : Input data to the FullyConnectedOp.  weight::SymbolicNode : Weight matrix.  bias::SymbolicNode : Bias parameter.  num_hidden::int, required : Number of hidden nodes of the output.  no_bias::boolean, optional, default=False : Whether to disable bias parameter.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Group     Method .  Group(nodes :: SymbolicNode...)  Create a  SymbolicNode  by grouping nodes together.  source  #  MXNet.mx.IdentityAttachKLSparseReg     Method .  Apply a sparse regularization to the output a sigmoid activation function.  Arguments   data::SymbolicNode : Input data.  sparseness_target::float, optional, default=0.1 : The sparseness target  penalty::float, optional, default=0.001 : The tradeoff parameter for the sparseness penalty  momentum::float, optional, default=0.9 : The momentum for running average  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.L2Normalization     Method .  Set the l2 norm of each instance to a constant.  Arguments   data::SymbolicNode : Input data to the L2NormalizationOp.  eps::float, optional, default=1e-10 : Epsilon to prevent div 0  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.LRN     Method .  Apply convolution to input then add a bias.  Arguments   data::SymbolicNode : Input data to the ConvolutionOp.  alpha::float, optional, default=0.0001 : value of the alpha variance scaling parameter in the normalization formula  beta::float, optional, default=0.75 : value of the beta power parameter in the normalization formula  knorm::float, optional, default=2 : value of the k parameter in normalization formula  nsize::int (non-negative), required : normalization window width in elements.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.LeakyReLU     Method .  Apply activation function to input.  Arguments   data::SymbolicNode : Input data to activation function.  act_type::{'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky' : Activation function to be applied.  slope::float, optional, default=0.25 : Init slope for the activation. (For leaky and elu only)  lower_bound::float, optional, default=0.125 : Lower bound of random slope. (For rrelu only)  upper_bound::float, optional, default=0.334 : Upper bound of random slope. (For rrelu only)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.LinearRegressionOutput     Method .  Use linear regression for final output, this is used on final output of a net.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.LogisticRegressionOutput     Method .  Use Logistic regression for final output, this is used on final output of a net. Logistic regression is suitable for binary classification or probability prediction tasks.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.MAERegressionOutput     Method .  Use mean absolute error regression for final output, this is used on final output of a net.  Arguments   data::SymbolicNode : Input data to function.  label::SymbolicNode : Input label to function.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.MakeLoss     Method .  Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency  Arguments   data::SymbolicNode : Input data.  grad_scale::float, optional, default=1 : gradient scale as a supplement to unary and binary operators  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Pooling     Method .  Perform spatial pooling on inputs.  Arguments   data::SymbolicNode : Input data to the pooling operator.  global_pool::boolean, optional, default=False : Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape  kernel::Shape(tuple), required : pooling kernel size: (y, x) or (d, y, x)  pool_type::{'avg', 'max', 'sum'}, required : Pooling type to be applied.  stride::Shape(tuple), optional, default=(1,1) : stride: for pooling (y, x) or (d, y, x)  pad::Shape(tuple), optional, default=(0,0) : pad for pooling: (y, x) or (d, y, x)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.RNN     Method .  Apply a recurrent layer to input.  Arguments   data::SymbolicNode : Input data to RNN  parameters::SymbolicNode : Vector of all RNN trainable parameters  state::SymbolicNode : initial hidden state of the RNN  state_cell::SymbolicNode : initial cell state for LSTM networks (only for LSTM)  state_size::int (non-negative), required : size of the state for each layer  num_layers::int (non-negative), required : number of stacked layers  bidirectional::boolean, optional, default=False : whether to use bidirectional recurrent layers  mode::{'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required : the type of RNN to compute  p::float, optional, default=0 : Fraction of the input that gets dropped out at training time  state_outputs::boolean, optional, default=False : Whether to have the states as symbol outputs.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.ROIPooling     Method .  Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling  Arguments   data::SymbolicNode : Input data to the pooling operator, a 4D Feature maps  rois::SymbolicNode : Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data  pooled_size::Shape(tuple), required : fix pooled size: (h, w)  spatial_scale::float, required : Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Reshape     Method .  Reshape input to target shape  Arguments   data::SymbolicNode : Input data to reshape.  target_shape::Shape(tuple), optional, default=(0,0) : (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims  keep_highest::boolean, optional, default=False : (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input  shape::, optional, default=() : Target new shape. If the dim is same, set it to 0. If the dim is set to be -1, it will be inferred from the rest of dims. One and only one dim can be -1  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.SVMOutput     Method .  Support Vector Machine based transformation on input, backprop L2-SVM  Arguments   data::SymbolicNode : Input data to svm.  label::SymbolicNode : Label data.  margin::float, optional, default=1 : Scale the DType(param_.margin) for activation size  regularization_coefficient::float, optional, default=1 : Scale the coefficient responsible for balacing coefficient size and error tradeoff  use_linear::boolean, optional, default=False : If set true, uses L1-SVM objective function. Default uses L2-SVM objective  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.SliceChannel     Method .  Slice input equally along specified axis  Arguments   num_outputs::int, required : Number of outputs to be sliced.  axis::int, optional, default='1' : Dimension along which to slice.  squeeze_axis::boolean, optional, default=False : If true AND the sliced dimension becomes 1, squeeze that dimension.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode[] .  source  #  MXNet.mx.Softmax     Method .  DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput  Arguments   data::SymbolicNode : Input data to softmax.  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.SoftmaxActivation     Method .  Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.  Arguments   data::SymbolicNode : Input data to activation function.  mode::{'channel', 'instance'},optional, default='instance' : Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.SoftmaxOutput     Method .  Perform a softmax transformation on input, backprop with logloss.  Arguments   data::SymbolicNode : Input data to softmax.  label::SymbolicNode : Label data, can also be probability value with same shape as data  grad_scale::float, optional, default=1 : Scale the gradient by a float factor  ignore_label::float, optional, default=-1 : the label value will be ignored during backward (only works if use_ignore is set to be true).  multi_output::boolean, optional, default=False : If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n x_1 ...*x_n output, each has k classes  use_ignore::boolean, optional, default=False : If set to true, the ignore_label value will not contribute to the backward gradient  normalization::{'batch', 'null', 'valid'},optional, default='null' : If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.SpatialTransformer     Method .  Apply spatial transformer to input feature map.  Arguments   data::SymbolicNode : Input data to the SpatialTransformerOp.  loc::SymbolicNode : localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.  target_shape::Shape(tuple), optional, default=(0,0) : output shape(h, w) of spatial transformer: (y, x)  transform_type::{'affine'}, required : transformation type  sampler_type::{'bilinear'}, required : sampling type  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.SwapAxis     Method .  Apply swapaxis to input.  Arguments   data::SymbolicNode : Input data to the SwapAxisOp.  dim1::int (non-negative), optional, default=0 : the first axis to be swapped.  dim2::int (non-negative), optional, default=0 : the second axis to be swapped.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.UpSampling     Method .  Perform nearest neighboor/bilinear up sampling to inputs  This function support variable length positional  SymbolicNode  inputs.  Arguments   data::SymbolicNode[] : Array of tensors to upsample  scale::int (non-negative), required : Up sampling scale  num_filter::int (non-negative), optional, default=0 : Input filter. Only used by bilinear sample_type.  sample_type::{'bilinear', 'nearest'}, required : upsampling method  multi_input_mode::{'concat', 'sum'},optional, default='concat' : How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.  num_args::int, required : Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale h_0,scale w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.  workspace::long (non-negative), optional, default=512 : Tmp workspace for deconvolution (MB)  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx.Variable     Method .  Variable(name :: Union{Symbol, AbstractString})  Create a symbolic variable with the given name. This is typically used as a placeholder. For example, the data node, acting as the starting point of a network architecture.  Arguments   Dict{Symbol, AbstractString} attrs: The attributes associated with this  Variable .   source  #  MXNet.mx._CrossDeviceCopy     Method .  Special op to copy data cross device  Arguments   name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx._Div     Method .  Multiply lhs by rhs  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._DivScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._Maximum     Method .  Elementwise max of lhs by rhs  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._MaximumScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._Minimum     Method .  Elementwise min of lhs by rhs  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._MinimumScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._Minus     Method .  Minus lhs and rhs  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._MinusScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._Mul     Method .  Multiply lhs and rhs  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._MulScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._NDArray     Method .  Stub for implementing an operator implemented in native frontend language with ndarray.  Arguments   info::, required :  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx._Native     Method .  Stub for implementing an operator implemented in native frontend language.  Arguments   info::, required :  need_top_grad::boolean, optional, default=True : Whether this layer needs out grad for backward. Should be false for loss layers.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns  SymbolicNode .  source  #  MXNet.mx._Plus     Method .  Add lhs and rhs  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._PlusScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._Power     Method .  Elementwise power(lhs, rhs)  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._PowerScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._RDivScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._RMinusScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx._RPowerScalar     Method .  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.batch_dot     Method .  Calculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) \u2013  (batch, M, N)  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_axis     Method .  Broadcast data in the given axis to the given size. The original size of the broadcasting axis must be 1.  Arguments   src::SymbolicNode : Left symbolic input to the function  axis::Shape(tuple), optional, default=() : The axes to perform the broadcasting.  size::Shape(tuple), optional, default=() : Target sizes of the broadcasting axes.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_div     Method .  lhs divide rhs with broadcast  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_minus     Method .  lhs minus rhs with broadcast  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_mul     Method .  lhs multiple rhs with broadcast  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_plus     Method .  lhs add rhs with broadcast  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_power     Method .  lhs power rhs with broadcast  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.broadcast_to     Method .  Broadcast data to the target shape. The original size of the broadcasting axis must be 1.  Arguments   src::SymbolicNode : Left symbolic input to the function  shape::Shape(tuple), optional, default=() : The shape of the desired array. We can set the dim to zero if it's same as the original. E.g  A = broadcast_to(B, shape=(10, 0, 0))  has the same meaning as  A = broadcast_axis(B, axis=0, size=10) .  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.element_mask     Method .  rhs elmentwise mask lhs with broadcast  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.expand_dims     Method .  Expand the shape of array by inserting a new axis.  Arguments   src::SymbolicNode : Left symbolic input to the function  axis::int (non-negative), required : Position (amongst axes) where new axis is to be inserted.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.from_json     Method .  from_json(repr :: AbstractString, ::Type{SymbolicNode})  Load a  SymbolicNode  from a JSON string representation.  source  #  MXNet.mx.get_attr     Method .  get_attr(self :: SymbolicNode, key :: Symbol)  Get attribute attached to this  SymbolicNode  belonging to key.  Returns the value belonging to key as a  Nullable .  source  #  MXNet.mx.get_internals     Method .  get_internals(self :: SymbolicNode)  Get a new grouped  SymbolicNode  whose output contains all the internal outputs of this  SymbolicNode .  source  #  MXNet.mx.grad     Method .  grad(self :: SymbolicNode, wrt :: Vector{SymbolicNode})  Get the autodiff gradient of the current  SymbolicNode . This function can only be used if the current symbol is a loss function.  Arguments:   self::SymbolicNode : current node.  wrt::Vector{Symbol} : the names of the arguments to the gradient.   Returns a gradient symbol of the corresponding gradient.  source  #  MXNet.mx.infer_shape     Method .  infer_shape(self :: SymbolicNode, args...)\ninfer_shape(self :: SymbolicNode; kwargs...)  Do shape inference according to the input shapes. The input shapes could be provided as a list of shapes, which should specify the shapes of inputs in the same order as the arguments returned by  list_arguments . Alternatively, the shape information could be specified via keyword arguments.  Returns a 3-tuple containing shapes of all the arguments, shapes of all the outputs and shapes of all the auxiliary variables. If shape inference failed due to incomplete or incompatible inputs, the return value will be  (nothing, nothing, nothing) .  source  #  MXNet.mx.infer_type     Method .  infer_type(self :: SymbolicNode; kwargs...)\ninfer_type(self :: SymbolicNode, args...)  Do type inference according to the input types. The input types could be provided as a list of types, which should specify the types of inputs in the same order as the arguments returned by  list_arguments . Alternatively, the type information could be specified via keyword arguments.  Returns a 3-tuple containing types of all the arguments, types of all the outputs and types of all the auxiliary variables. If type inference failed due to incomplete or incompatible inputs, the return value will be  (nothing, nothing, nothing) .  source  #  MXNet.mx.list_all_attr     Method .  list_all_attr(self :: SymbolicNode)  Get all attributes from the symbol graph.  Returns a dictionary of attributes.  source  #  MXNet.mx.list_arguments     Method .  list_arguments(self :: SymbolicNode)  List all the arguments of this node. The argument for a node contains both the inputs and parameters. For example, a  FullyConnected  node will have both data and weights in its arguments. A composed node (e.g. a MLP) will list all the arguments for intermediate nodes.  Returns a list of symbols indicating the names of the arguments.  source  #  MXNet.mx.list_attr     Method .  list_attr(self :: SymbolicNode)  Get all attributes from a symbol.  Returns a dictionary of attributes.  source  #  MXNet.mx.list_auxiliary_states     Method .  list_auxiliary_states(self :: SymbolicNode)  List all auxiliary states in the symbool.  Auxiliary states are special states of symbols that do not corresponds to an argument, and do not have gradient. But still be useful for the specific operations. A common example of auxiliary state is the moving_mean and moving_variance in BatchNorm. Most operators do not have Auxiliary states.  Returns a list of symbols indicating the names of the auxiliary states.  source  #  MXNet.mx.list_outputs     Method .  list_outputs(self :: SymbolicNode)  List all the outputs of this node.  Returns a list of symbols indicating the names of the outputs.  source  #  MXNet.mx.load     Method .  load(filename :: AbstractString, ::Type{SymbolicNode})  Load a  SymbolicNode  from a JSON file.  source  #  MXNet.mx.normal     Method .  Sample a normal distribution  Arguments   loc::float, optional, default=0 : Mean of the distribution.  scale::float, optional, default=1 : Standard deviation of the distribution.  shape::Shape(tuple), required : The shape of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.rsqrt     Method .  Take rsqrt of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.save     Method .  save(filename :: AbstractString, node :: SymbolicNode)  Save a  SymbolicNode  to a JSON file.  source  #  MXNet.mx.set_attr     Method .  set_attr(self:: SymbolicNode, key :: Symbol, value :: AbstractString)  Set the attribute key to value for this  SymbolicNode .   Note  It is encouraged not to call this function directly, unless you know exactly what you are doing. The recommended way of setting attributes is when creating the  SymbolicNode . Changing the attributes of a  SymbolicNode  that is already been used somewhere else might cause unexpected behavior and inconsistency.   source  #  MXNet.mx.slice_axis     Method .  Slice the input along certain axis and return a sliced array.  Arguments   src::SymbolicNode : Left symbolic input to the function  axis::int, required : The axis to be sliced  begin::int, required : The beginning index to be sliced  end::int, required : The end index to be sliced  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.smooth_l1     Method .  Calculate Smooth L1 Loss(lhs, scalar)  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.softmax_cross_entropy     Method .  Calculate cross_entropy(lhs, one_hot(rhs))  Arguments   lhs::SymbolicNode : Left symbolic input to the function  rhs::SymbolicNode : Right symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.square     Method .  Take square of the src  Arguments   src::SymbolicNode : Left symbolic input to the function  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.sum_axis     Method .  (Depreciated! Use sum instead!) Take sum of the src in the given axis and returns a NDArray. Follows numpy semantics.  Arguments   src::SymbolicNode : Left symbolic input to the function  axis::Shape(tuple), optional, default=() : Same as Numpy. The axes to perform the reduction.If left empty, a global reduction will be performed.  keepdims::boolean, optional, default=False : Same as Numpy. If keepdims is set to true, the axis which is reduced is left in the result as dimension with size one.  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source  #  MXNet.mx.to_json     Method .  to_json(self :: SymbolicNode)  Convert a  SymbolicNode  into a JSON string.  source  #  MXNet.mx.uniform     Method .  Sample a uniform distribution  Arguments   low::float, optional, default=0 : The lower bound of distribution  high::float, optional, default=1 : The upper bound of distribution  shape::Shape(tuple), required : The shape of the output  name::Symbol : The name of the  SymbolicNode . (e.g.  :my_symbol ), optional.  attrs::Dict{Symbol, AbstractString} : The attributes associated with this  SymbolicNode .   Returns ``.  source", 
            "title": "Symbolic API"
        }, 
        {
            "location": "/api/nn-factory/", 
            "text": "Neural Network Factory\n\n\nNeural network factory provide convenient helper functions to define common neural networks.\n\n\n#\n\n\nMXNet.mx.MLP\n \n \nMethod\n.\n\n\nMLP(input, spec; hidden_activation = :relu, prefix)\n\n\n\n\nConstruct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.\n\n\nArguments:\n\n\n\n\ninput::SymbolicNode\n: the input to the mlp.\n\n\nspec\n: the mlp specification, a list of hidden dimensions. For example,         \n[128, (512, :sigmoid), 10]\n. The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).\n\n\nhidden_activation::Symbol\n: keyword argument, default \n:relu\n, indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the \nspec\n argument. Also activation is not         applied to the last, i.e. the prediction layer. See \nActivation\n for a         list of supported activation types.\n\n\nprefix\n: keyword argument, default \ngensym()\n, used as the prefix to         name the constructed layers.\n\n\n\n\nReturns the constructed MLP.\n\n\nsource", 
            "title": "Neural Networks Factory"
        }, 
        {
            "location": "/api/nn-factory/#neural-network-factory", 
            "text": "Neural network factory provide convenient helper functions to define common neural networks.  #  MXNet.mx.MLP     Method .  MLP(input, spec; hidden_activation = :relu, prefix)  Construct a multi-layer perceptron. A MLP is a multi-layer neural network with fully connected layers.  Arguments:   input::SymbolicNode : the input to the mlp.  spec : the mlp specification, a list of hidden dimensions. For example,          [128, (512, :sigmoid), 10] . The number in the list indicate the         number of hidden units in each layer. A tuple could be used to specify         the activation of each layer. Otherwise, the default activation will         be used (except for the last layer).  hidden_activation::Symbol : keyword argument, default  :relu , indicating         the default activation for hidden layers. The specification here could be overwritten         by layer-wise specification in the  spec  argument. Also activation is not         applied to the last, i.e. the prediction layer. See  Activation  for a         list of supported activation types.  prefix : keyword argument, default  gensym() , used as the prefix to         name the constructed layers.   Returns the constructed MLP.  source", 
            "title": "Neural Network Factory"
        }, 
        {
            "location": "/api/executor/", 
            "text": "Executor\n\n\n#\n\n\nMXNet.mx.Executor\n \n \nType\n.\n\n\nExecutor\n\n\n\n\nAn executor is a realization of a symbolic architecture defined by a \nSymbolicNode\n. The actual forward and backward computation specified by the network architecture can be carried out with an executor.\n\n\nsource\n\n\n#\n\n\nMXNet.mx.GRAD_REQ\n \n \nType\n.\n\n\nbind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)\n\n\n\n\nCreate an \nExecutor\n by binding a \nSymbolicNode\n to concrete \nNDArray\n.\n\n\nArguments\n\n\n\n\nsym::SymbolicNode\n: the network architecture describing the computation graph.\n\n\nctx::Context\n: the context on which the computation should run.\n\n\nargs\n: either a list of \nNDArray\n or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels. See \nlist_arguments\n         and \ninfer_shape\n.\n\n\nargs_grad\n:\n\n\naux_states\n:\n\n\ngrad_req\n:\n\n\n\n\nsource\n\n\n#\n\n\nMXNet.mx.debug_str\n \n \nMethod\n.\n\n\nGet a debug string about internal execution plan.\n\n\nCan be used to get an estimated about the memory cost.\n\n\n  net = ... # Symbol\n  dProvider = ... # DataProvider\n  exec = mx.simple_bind(net, mx.cpu(), data=size(dProvider.data_batch[1]))\n  dbg_str = mx.debug_str(exec)\n  println(split(ref, ['\\n'])[end-2])\n\n\n\n\nsource", 
            "title": "Executor"
        }, 
        {
            "location": "/api/executor/#executor", 
            "text": "#  MXNet.mx.Executor     Type .  Executor  An executor is a realization of a symbolic architecture defined by a  SymbolicNode . The actual forward and backward computation specified by the network architecture can be carried out with an executor.  source  #  MXNet.mx.GRAD_REQ     Type .  bind(sym, ctx, args; args_grad=Dict(), aux_states=Dict(), grad_req=GRAD_WRITE)  Create an  Executor  by binding a  SymbolicNode  to concrete  NDArray .  Arguments   sym::SymbolicNode : the network architecture describing the computation graph.  ctx::Context : the context on which the computation should run.  args : either a list of  NDArray  or a dictionary of name-array pairs. Concrete         arrays for all the inputs in the network architecture. The inputs typically include         network parameters (weights, bias, filters, etc.), data and labels. See  list_arguments          and  infer_shape .  args_grad :  aux_states :  grad_req :   source  #  MXNet.mx.debug_str     Method .  Get a debug string about internal execution plan.  Can be used to get an estimated about the memory cost.    net = ... # Symbol\n  dProvider = ... # DataProvider\n  exec = mx.simple_bind(net, mx.cpu(), data=size(dProvider.data_batch[1]))\n  dbg_str = mx.debug_str(exec)\n  println(split(ref, ['\\n'])[end-2])  source", 
            "title": "Executor"
        }, 
        {
            "location": "/api/visualize/", 
            "text": "Network Visualization\n\n\n#\n\n\nMXNet.mx.to_graphviz\n \n \nMethod\n.\n\n\nto_graphviz(network)\n\n\n\n\n\n\nnetwork::SymbolicNode\n: the network to visualize.\n\n\ntitle::AbstractString:\n keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.\n\n\ninput_shapes\n: keyword argument, default \nnothing\n. If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.\n\n\n\n\nReturns the graph description in GraphViz \ndot\n language.\n\n\nsource", 
            "title": "Network Visualization"
        }, 
        {
            "location": "/api/visualize/#network-visualization", 
            "text": "#  MXNet.mx.to_graphviz     Method .  to_graphviz(network)   network::SymbolicNode : the network to visualize.  title::AbstractString:  keyword argument, default \"Network Visualization\",         the title of the GraphViz graph.  input_shapes : keyword argument, default  nothing . If provided,         will run shape inference and plot with the shape information. Should         be either a dictionary of name-shape mapping or an array of shapes.   Returns the graph description in GraphViz  dot  language.  source", 
            "title": "Network Visualization"
        }
    ]
}